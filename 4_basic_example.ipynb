{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and visualizing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0   1            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1   2            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2   3            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3   4            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4   5            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the dataset \n",
    "data_set = pd.read_csv(\"Iris.csv\")\n",
    "\n",
    "#Let's have a look at the dataset\n",
    "data_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Examples and Labels and string conversion to numerics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Species = {'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica':2}\n",
    "data_set.Species = [Species[item] for item in data_set.Species]\n",
    "\n",
    "X = data_set.iloc[:, 0:4] #predictors\n",
    "y = data_set.iloc[:, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "X = normalize(X)\n",
    "y = np.array(y)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (135, 4)\n",
      "Y_train shape:  (135,)\n",
      "X_test shape:  (15, 4)\n",
      "Y_test shape:  (15,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.10)\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"Y_test shape: \", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dataloader to convert numpy arrays to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loading data into pytorch model\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "trainloader = DataLoader(TensorDataset(torch.from_numpy(X_train), torch.from_numpy(Y_train)), \n",
    "                         batch_size=135, shuffle=True)\n",
    "\n",
    "testloader = DataLoader(TensorDataset(torch.from_numpy(X_test), torch.from_numpy(Y_test)), \n",
    "                        batch_size=135, shuffle=True)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": trainloader,\n",
    "    \"validation\": testloader\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This class will define our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using  init we will define number of nodes in the particular layer\n",
    "#### forward() defines the functionality of each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 227) ### 4 predictors\n",
    "        self.fc2 = nn.Linear(227, 94) ### 227 neurons in hidden layer 1, 94 in 3\n",
    "        self.fc3 = nn.Linear(94, 75) ### 75 neurons in hidden layer 3\n",
    "        self.fc4 = nn.Linear(75, 3) ### 3 classes\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x)) #activation function for 1\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(F.relu(self.fc3(x)))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model declaration, Type of loss and optimizer.\n",
    "\n",
    "#### Adam optimizer is being used to optimize our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier()\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This block print a summary of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (fc1): Linear(in_features=4, out_features=227, bias=True)\n",
       "  (fc2): Linear(in_features=227, out_features=94, bias=True)\n",
       "  (fc3): Linear(in_features=94, out_features=75, bias=True)\n",
       "  (fc4): Linear(in_features=75, out_features=3, bias=True)\n",
       "  (dropout): Dropout(p=0.2)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, inputs):\n",
    "    output = model(inputs)\n",
    "    return output.data.numpy().argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procedures for forward and backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2000], Iter [1] Loss: 1.1368 Training Accuracy: 0.18519\n",
      "Epoch [2/2000], Iter [1] Loss: 1.1357 Training Accuracy: 0.18519\n",
      "Epoch [3/2000], Iter [1] Loss: 1.1318 Training Accuracy: 0.18519\n",
      "Epoch [4/2000], Iter [1] Loss: 1.1323 Training Accuracy: 0.18519\n",
      "Epoch [5/2000], Iter [1] Loss: 1.1310 Training Accuracy: 0.18519\n",
      "Epoch [6/2000], Iter [1] Loss: 1.1288 Training Accuracy: 0.17778\n",
      "Epoch [7/2000], Iter [1] Loss: 1.1287 Training Accuracy: 0.18519\n",
      "Epoch [8/2000], Iter [1] Loss: 1.1248 Training Accuracy: 0.18519\n",
      "Epoch [9/2000], Iter [1] Loss: 1.1237 Training Accuracy: 0.17778\n",
      "Epoch [10/2000], Iter [1] Loss: 1.1259 Training Accuracy: 0.19259\n",
      "Epoch [11/2000], Iter [1] Loss: 1.1214 Training Accuracy: 0.18519\n",
      "Epoch [12/2000], Iter [1] Loss: 1.1203 Training Accuracy: 0.19259\n",
      "Epoch [13/2000], Iter [1] Loss: 1.1207 Training Accuracy: 0.18519\n",
      "Epoch [14/2000], Iter [1] Loss: 1.1193 Training Accuracy: 0.17037\n",
      "Epoch [15/2000], Iter [1] Loss: 1.1196 Training Accuracy: 0.20000\n",
      "Epoch [16/2000], Iter [1] Loss: 1.1177 Training Accuracy: 0.20000\n",
      "Epoch [17/2000], Iter [1] Loss: 1.1164 Training Accuracy: 0.18519\n",
      "Epoch [18/2000], Iter [1] Loss: 1.1163 Training Accuracy: 0.19259\n",
      "Epoch [19/2000], Iter [1] Loss: 1.1126 Training Accuracy: 0.20741\n",
      "Epoch [20/2000], Iter [1] Loss: 1.1147 Training Accuracy: 0.22963\n",
      "Epoch [21/2000], Iter [1] Loss: 1.1111 Training Accuracy: 0.22963\n",
      "Epoch [22/2000], Iter [1] Loss: 1.1114 Training Accuracy: 0.22963\n",
      "Epoch [23/2000], Iter [1] Loss: 1.1119 Training Accuracy: 0.24444\n",
      "Epoch [24/2000], Iter [1] Loss: 1.1104 Training Accuracy: 0.26667\n",
      "Epoch [25/2000], Iter [1] Loss: 1.1102 Training Accuracy: 0.28148\n",
      "Epoch [26/2000], Iter [1] Loss: 1.1066 Training Accuracy: 0.28148\n",
      "Epoch [27/2000], Iter [1] Loss: 1.1073 Training Accuracy: 0.21481\n",
      "Epoch [28/2000], Iter [1] Loss: 1.1059 Training Accuracy: 0.27407\n",
      "Epoch [29/2000], Iter [1] Loss: 1.1075 Training Accuracy: 0.26667\n",
      "Epoch [30/2000], Iter [1] Loss: 1.1048 Training Accuracy: 0.31852\n",
      "Epoch [31/2000], Iter [1] Loss: 1.1067 Training Accuracy: 0.33333\n",
      "Epoch [32/2000], Iter [1] Loss: 1.1031 Training Accuracy: 0.29630\n",
      "Epoch [33/2000], Iter [1] Loss: 1.1007 Training Accuracy: 0.28148\n",
      "Epoch [34/2000], Iter [1] Loss: 1.0969 Training Accuracy: 0.34074\n",
      "Epoch [35/2000], Iter [1] Loss: 1.0985 Training Accuracy: 0.32593\n",
      "Epoch [36/2000], Iter [1] Loss: 1.0967 Training Accuracy: 0.30370\n",
      "Epoch [37/2000], Iter [1] Loss: 1.1003 Training Accuracy: 0.31111\n",
      "Epoch [38/2000], Iter [1] Loss: 1.0973 Training Accuracy: 0.29630\n",
      "Epoch [39/2000], Iter [1] Loss: 1.0983 Training Accuracy: 0.34074\n",
      "Epoch [40/2000], Iter [1] Loss: 1.0979 Training Accuracy: 0.36296\n",
      "Epoch [41/2000], Iter [1] Loss: 1.0931 Training Accuracy: 0.36296\n",
      "Epoch [42/2000], Iter [1] Loss: 1.0924 Training Accuracy: 0.37037\n",
      "Epoch [43/2000], Iter [1] Loss: 1.0910 Training Accuracy: 0.42963\n",
      "Epoch [44/2000], Iter [1] Loss: 1.0925 Training Accuracy: 0.39259\n",
      "Epoch [45/2000], Iter [1] Loss: 1.0880 Training Accuracy: 0.43704\n",
      "Epoch [46/2000], Iter [1] Loss: 1.0902 Training Accuracy: 0.38519\n",
      "Epoch [47/2000], Iter [1] Loss: 1.0875 Training Accuracy: 0.50370\n",
      "Epoch [48/2000], Iter [1] Loss: 1.0890 Training Accuracy: 0.51111\n",
      "Epoch [49/2000], Iter [1] Loss: 1.0895 Training Accuracy: 0.49630\n",
      "Epoch [50/2000], Iter [1] Loss: 1.0886 Training Accuracy: 0.51111\n",
      "Epoch [51/2000], Iter [1] Loss: 1.0856 Training Accuracy: 0.48148\n",
      "Epoch [52/2000], Iter [1] Loss: 1.0852 Training Accuracy: 0.53333\n",
      "Epoch [53/2000], Iter [1] Loss: 1.0833 Training Accuracy: 0.45926\n",
      "Epoch [54/2000], Iter [1] Loss: 1.0864 Training Accuracy: 0.49630\n",
      "Epoch [55/2000], Iter [1] Loss: 1.0812 Training Accuracy: 0.52593\n",
      "Epoch [56/2000], Iter [1] Loss: 1.0801 Training Accuracy: 0.45185\n",
      "Epoch [57/2000], Iter [1] Loss: 1.0802 Training Accuracy: 0.50370\n",
      "Epoch [58/2000], Iter [1] Loss: 1.0771 Training Accuracy: 0.55556\n",
      "Epoch [59/2000], Iter [1] Loss: 1.0771 Training Accuracy: 0.52593\n",
      "Epoch [60/2000], Iter [1] Loss: 1.0784 Training Accuracy: 0.54815\n",
      "Epoch [61/2000], Iter [1] Loss: 1.0780 Training Accuracy: 0.52593\n",
      "Epoch [62/2000], Iter [1] Loss: 1.0781 Training Accuracy: 0.57037\n",
      "Epoch [63/2000], Iter [1] Loss: 1.0751 Training Accuracy: 0.54074\n",
      "Epoch [64/2000], Iter [1] Loss: 1.0707 Training Accuracy: 0.45185\n",
      "Epoch [65/2000], Iter [1] Loss: 1.0726 Training Accuracy: 0.50370\n",
      "Epoch [66/2000], Iter [1] Loss: 1.0715 Training Accuracy: 0.59259\n",
      "Epoch [67/2000], Iter [1] Loss: 1.0685 Training Accuracy: 0.51111\n",
      "Epoch [68/2000], Iter [1] Loss: 1.0663 Training Accuracy: 0.51852\n",
      "Epoch [69/2000], Iter [1] Loss: 1.0683 Training Accuracy: 0.59259\n",
      "Epoch [70/2000], Iter [1] Loss: 1.0676 Training Accuracy: 0.53333\n",
      "Epoch [71/2000], Iter [1] Loss: 1.0659 Training Accuracy: 0.54815\n",
      "Epoch [72/2000], Iter [1] Loss: 1.0651 Training Accuracy: 0.57037\n",
      "Epoch [73/2000], Iter [1] Loss: 1.0657 Training Accuracy: 0.53333\n",
      "Epoch [74/2000], Iter [1] Loss: 1.0639 Training Accuracy: 0.57778\n",
      "Epoch [75/2000], Iter [1] Loss: 1.0656 Training Accuracy: 0.57037\n",
      "Epoch [76/2000], Iter [1] Loss: 1.0597 Training Accuracy: 0.61481\n",
      "Epoch [77/2000], Iter [1] Loss: 1.0625 Training Accuracy: 0.50370\n",
      "Epoch [78/2000], Iter [1] Loss: 1.0619 Training Accuracy: 0.54815\n",
      "Epoch [79/2000], Iter [1] Loss: 1.0582 Training Accuracy: 0.50370\n",
      "Epoch [80/2000], Iter [1] Loss: 1.0571 Training Accuracy: 0.56296\n",
      "Epoch [81/2000], Iter [1] Loss: 1.0545 Training Accuracy: 0.54815\n",
      "Epoch [82/2000], Iter [1] Loss: 1.0564 Training Accuracy: 0.57037\n",
      "Epoch [83/2000], Iter [1] Loss: 1.0623 Training Accuracy: 0.56296\n",
      "Epoch [84/2000], Iter [1] Loss: 1.0529 Training Accuracy: 0.55556\n",
      "Epoch [85/2000], Iter [1] Loss: 1.0518 Training Accuracy: 0.56296\n",
      "Epoch [86/2000], Iter [1] Loss: 1.0501 Training Accuracy: 0.57037\n",
      "Epoch [87/2000], Iter [1] Loss: 1.0506 Training Accuracy: 0.57037\n",
      "Epoch [88/2000], Iter [1] Loss: 1.0530 Training Accuracy: 0.54074\n",
      "Epoch [89/2000], Iter [1] Loss: 1.0447 Training Accuracy: 0.54074\n",
      "Epoch [90/2000], Iter [1] Loss: 1.0471 Training Accuracy: 0.54074\n",
      "Epoch [91/2000], Iter [1] Loss: 1.0500 Training Accuracy: 0.55556\n",
      "Epoch [92/2000], Iter [1] Loss: 1.0406 Training Accuracy: 0.54074\n",
      "Epoch [93/2000], Iter [1] Loss: 1.0396 Training Accuracy: 0.53333\n",
      "Epoch [94/2000], Iter [1] Loss: 1.0438 Training Accuracy: 0.50370\n",
      "Epoch [95/2000], Iter [1] Loss: 1.0444 Training Accuracy: 0.57778\n",
      "Epoch [96/2000], Iter [1] Loss: 1.0450 Training Accuracy: 0.57778\n",
      "Epoch [97/2000], Iter [1] Loss: 1.0389 Training Accuracy: 0.55556\n",
      "Epoch [98/2000], Iter [1] Loss: 1.0417 Training Accuracy: 0.59259\n",
      "Epoch [99/2000], Iter [1] Loss: 1.0381 Training Accuracy: 0.57778\n",
      "Epoch [100/2000], Iter [1] Loss: 1.0370 Training Accuracy: 0.54815\n",
      "Epoch [101/2000], Iter [1] Loss: 1.0335 Training Accuracy: 0.57037\n",
      "Epoch [102/2000], Iter [1] Loss: 1.0319 Training Accuracy: 0.56296\n",
      "Epoch [103/2000], Iter [1] Loss: 1.0326 Training Accuracy: 0.51111\n",
      "Epoch [104/2000], Iter [1] Loss: 1.0334 Training Accuracy: 0.53333\n",
      "Epoch [105/2000], Iter [1] Loss: 1.0282 Training Accuracy: 0.52593\n",
      "Epoch [106/2000], Iter [1] Loss: 1.0379 Training Accuracy: 0.52593\n",
      "Epoch [107/2000], Iter [1] Loss: 1.0264 Training Accuracy: 0.54074\n",
      "Epoch [108/2000], Iter [1] Loss: 1.0314 Training Accuracy: 0.54815\n",
      "Epoch [109/2000], Iter [1] Loss: 1.0277 Training Accuracy: 0.55556\n",
      "Epoch [110/2000], Iter [1] Loss: 1.0192 Training Accuracy: 0.55556\n",
      "Epoch [111/2000], Iter [1] Loss: 1.0228 Training Accuracy: 0.54074\n",
      "Epoch [112/2000], Iter [1] Loss: 1.0223 Training Accuracy: 0.57037\n",
      "Epoch [113/2000], Iter [1] Loss: 1.0234 Training Accuracy: 0.55556\n",
      "Epoch [114/2000], Iter [1] Loss: 1.0247 Training Accuracy: 0.54815\n",
      "Epoch [115/2000], Iter [1] Loss: 1.0122 Training Accuracy: 0.58519\n",
      "Epoch [116/2000], Iter [1] Loss: 1.0239 Training Accuracy: 0.56296\n",
      "Epoch [117/2000], Iter [1] Loss: 1.0164 Training Accuracy: 0.54815\n",
      "Epoch [118/2000], Iter [1] Loss: 1.0173 Training Accuracy: 0.58519\n",
      "Epoch [119/2000], Iter [1] Loss: 1.0212 Training Accuracy: 0.54074\n",
      "Epoch [120/2000], Iter [1] Loss: 1.0179 Training Accuracy: 0.55556\n",
      "Epoch [121/2000], Iter [1] Loss: 1.0245 Training Accuracy: 0.58519\n",
      "Epoch [122/2000], Iter [1] Loss: 1.0110 Training Accuracy: 0.60000\n",
      "Epoch [123/2000], Iter [1] Loss: 1.0115 Training Accuracy: 0.54074\n",
      "Epoch [124/2000], Iter [1] Loss: 1.0120 Training Accuracy: 0.55556\n",
      "Epoch [125/2000], Iter [1] Loss: 1.0106 Training Accuracy: 0.57778\n",
      "Epoch [126/2000], Iter [1] Loss: 1.0044 Training Accuracy: 0.57778\n",
      "Epoch [127/2000], Iter [1] Loss: 1.0151 Training Accuracy: 0.54815\n",
      "Epoch [128/2000], Iter [1] Loss: 1.0172 Training Accuracy: 0.57037\n",
      "Epoch [129/2000], Iter [1] Loss: 1.0035 Training Accuracy: 0.57037\n",
      "Epoch [130/2000], Iter [1] Loss: 0.9995 Training Accuracy: 0.55556\n",
      "Epoch [131/2000], Iter [1] Loss: 1.0021 Training Accuracy: 0.57037\n",
      "Epoch [132/2000], Iter [1] Loss: 1.0012 Training Accuracy: 0.57778\n",
      "Epoch [133/2000], Iter [1] Loss: 0.9901 Training Accuracy: 0.53333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [134/2000], Iter [1] Loss: 1.0035 Training Accuracy: 0.55556\n",
      "Epoch [135/2000], Iter [1] Loss: 1.0070 Training Accuracy: 0.54074\n",
      "Epoch [136/2000], Iter [1] Loss: 0.9883 Training Accuracy: 0.57778\n",
      "Epoch [137/2000], Iter [1] Loss: 0.9900 Training Accuracy: 0.54815\n",
      "Epoch [138/2000], Iter [1] Loss: 0.9966 Training Accuracy: 0.57037\n",
      "Epoch [139/2000], Iter [1] Loss: 0.9896 Training Accuracy: 0.54815\n",
      "Epoch [140/2000], Iter [1] Loss: 0.9924 Training Accuracy: 0.55556\n",
      "Epoch [141/2000], Iter [1] Loss: 0.9889 Training Accuracy: 0.54074\n",
      "Epoch [142/2000], Iter [1] Loss: 0.9904 Training Accuracy: 0.54074\n",
      "Epoch [143/2000], Iter [1] Loss: 0.9816 Training Accuracy: 0.54074\n",
      "Epoch [144/2000], Iter [1] Loss: 0.9955 Training Accuracy: 0.54815\n",
      "Epoch [145/2000], Iter [1] Loss: 0.9961 Training Accuracy: 0.56296\n",
      "Epoch [146/2000], Iter [1] Loss: 0.9821 Training Accuracy: 0.57778\n",
      "Epoch [147/2000], Iter [1] Loss: 0.9920 Training Accuracy: 0.54815\n",
      "Epoch [148/2000], Iter [1] Loss: 0.9844 Training Accuracy: 0.57037\n",
      "Epoch [149/2000], Iter [1] Loss: 0.9851 Training Accuracy: 0.58519\n",
      "Epoch [150/2000], Iter [1] Loss: 0.9941 Training Accuracy: 0.57037\n",
      "Epoch [151/2000], Iter [1] Loss: 0.9857 Training Accuracy: 0.55556\n",
      "Epoch [152/2000], Iter [1] Loss: 0.9777 Training Accuracy: 0.57778\n",
      "Epoch [153/2000], Iter [1] Loss: 0.9661 Training Accuracy: 0.55556\n",
      "Epoch [154/2000], Iter [1] Loss: 0.9791 Training Accuracy: 0.57037\n",
      "Epoch [155/2000], Iter [1] Loss: 0.9769 Training Accuracy: 0.57037\n",
      "Epoch [156/2000], Iter [1] Loss: 0.9875 Training Accuracy: 0.57037\n",
      "Epoch [157/2000], Iter [1] Loss: 0.9698 Training Accuracy: 0.57037\n",
      "Epoch [158/2000], Iter [1] Loss: 0.9761 Training Accuracy: 0.57037\n",
      "Epoch [159/2000], Iter [1] Loss: 0.9794 Training Accuracy: 0.55556\n",
      "Epoch [160/2000], Iter [1] Loss: 0.9773 Training Accuracy: 0.56296\n",
      "Epoch [161/2000], Iter [1] Loss: 0.9733 Training Accuracy: 0.57778\n",
      "Epoch [162/2000], Iter [1] Loss: 0.9694 Training Accuracy: 0.58519\n",
      "Epoch [163/2000], Iter [1] Loss: 0.9730 Training Accuracy: 0.57037\n",
      "Epoch [164/2000], Iter [1] Loss: 0.9706 Training Accuracy: 0.57778\n",
      "Epoch [165/2000], Iter [1] Loss: 0.9606 Training Accuracy: 0.58519\n",
      "Epoch [166/2000], Iter [1] Loss: 0.9743 Training Accuracy: 0.57778\n",
      "Epoch [167/2000], Iter [1] Loss: 0.9662 Training Accuracy: 0.57778\n",
      "Epoch [168/2000], Iter [1] Loss: 0.9794 Training Accuracy: 0.60000\n",
      "Epoch [169/2000], Iter [1] Loss: 0.9613 Training Accuracy: 0.57037\n",
      "Epoch [170/2000], Iter [1] Loss: 0.9647 Training Accuracy: 0.54815\n",
      "Epoch [171/2000], Iter [1] Loss: 0.9574 Training Accuracy: 0.57037\n",
      "Epoch [172/2000], Iter [1] Loss: 0.9539 Training Accuracy: 0.59259\n",
      "Epoch [173/2000], Iter [1] Loss: 0.9569 Training Accuracy: 0.59259\n",
      "Epoch [174/2000], Iter [1] Loss: 0.9504 Training Accuracy: 0.64444\n",
      "Epoch [175/2000], Iter [1] Loss: 0.9571 Training Accuracy: 0.59259\n",
      "Epoch [176/2000], Iter [1] Loss: 0.9581 Training Accuracy: 0.61481\n",
      "Epoch [177/2000], Iter [1] Loss: 0.9478 Training Accuracy: 0.59259\n",
      "Epoch [178/2000], Iter [1] Loss: 0.9453 Training Accuracy: 0.58519\n",
      "Epoch [179/2000], Iter [1] Loss: 0.9547 Training Accuracy: 0.59259\n",
      "Epoch [180/2000], Iter [1] Loss: 0.9547 Training Accuracy: 0.58519\n",
      "Epoch [181/2000], Iter [1] Loss: 0.9477 Training Accuracy: 0.61481\n",
      "Epoch [182/2000], Iter [1] Loss: 0.9390 Training Accuracy: 0.61481\n",
      "Epoch [183/2000], Iter [1] Loss: 0.9508 Training Accuracy: 0.61481\n",
      "Epoch [184/2000], Iter [1] Loss: 0.9492 Training Accuracy: 0.60741\n",
      "Epoch [185/2000], Iter [1] Loss: 0.9361 Training Accuracy: 0.61481\n",
      "Epoch [186/2000], Iter [1] Loss: 0.9372 Training Accuracy: 0.60741\n",
      "Epoch [187/2000], Iter [1] Loss: 0.9425 Training Accuracy: 0.62222\n",
      "Epoch [188/2000], Iter [1] Loss: 0.9287 Training Accuracy: 0.61481\n",
      "Epoch [189/2000], Iter [1] Loss: 0.9441 Training Accuracy: 0.60000\n",
      "Epoch [190/2000], Iter [1] Loss: 0.9386 Training Accuracy: 0.60741\n",
      "Epoch [191/2000], Iter [1] Loss: 0.9360 Training Accuracy: 0.60000\n",
      "Epoch [192/2000], Iter [1] Loss: 0.9404 Training Accuracy: 0.61481\n",
      "Epoch [193/2000], Iter [1] Loss: 0.9372 Training Accuracy: 0.61481\n",
      "Epoch [194/2000], Iter [1] Loss: 0.9374 Training Accuracy: 0.62222\n",
      "Epoch [195/2000], Iter [1] Loss: 0.9294 Training Accuracy: 0.59259\n",
      "Epoch [196/2000], Iter [1] Loss: 0.9332 Training Accuracy: 0.60000\n",
      "Epoch [197/2000], Iter [1] Loss: 0.9250 Training Accuracy: 0.61481\n",
      "Epoch [198/2000], Iter [1] Loss: 0.9306 Training Accuracy: 0.61481\n",
      "Epoch [199/2000], Iter [1] Loss: 0.9197 Training Accuracy: 0.60741\n",
      "Epoch [200/2000], Iter [1] Loss: 0.9245 Training Accuracy: 0.61481\n",
      "Epoch [201/2000], Iter [1] Loss: 0.9117 Training Accuracy: 0.61481\n",
      "Epoch [202/2000], Iter [1] Loss: 0.9251 Training Accuracy: 0.61481\n",
      "Epoch [203/2000], Iter [1] Loss: 0.9074 Training Accuracy: 0.61481\n",
      "Epoch [204/2000], Iter [1] Loss: 0.9120 Training Accuracy: 0.60741\n",
      "Epoch [205/2000], Iter [1] Loss: 0.9250 Training Accuracy: 0.61481\n",
      "Epoch [206/2000], Iter [1] Loss: 0.9031 Training Accuracy: 0.63704\n",
      "Epoch [207/2000], Iter [1] Loss: 0.9147 Training Accuracy: 0.62963\n",
      "Epoch [208/2000], Iter [1] Loss: 0.9009 Training Accuracy: 0.62963\n",
      "Epoch [209/2000], Iter [1] Loss: 0.9048 Training Accuracy: 0.64444\n",
      "Epoch [210/2000], Iter [1] Loss: 0.9116 Training Accuracy: 0.63704\n",
      "Epoch [211/2000], Iter [1] Loss: 0.9036 Training Accuracy: 0.61481\n",
      "Epoch [212/2000], Iter [1] Loss: 0.9052 Training Accuracy: 0.62222\n",
      "Epoch [213/2000], Iter [1] Loss: 0.8993 Training Accuracy: 0.64444\n",
      "Epoch [214/2000], Iter [1] Loss: 0.8997 Training Accuracy: 0.63704\n",
      "Epoch [215/2000], Iter [1] Loss: 0.9029 Training Accuracy: 0.63704\n",
      "Epoch [216/2000], Iter [1] Loss: 0.8942 Training Accuracy: 0.61481\n",
      "Epoch [217/2000], Iter [1] Loss: 0.8865 Training Accuracy: 0.63704\n",
      "Epoch [218/2000], Iter [1] Loss: 0.9129 Training Accuracy: 0.62222\n",
      "Epoch [219/2000], Iter [1] Loss: 0.8870 Training Accuracy: 0.63704\n",
      "Epoch [220/2000], Iter [1] Loss: 0.8799 Training Accuracy: 0.63704\n",
      "Epoch [221/2000], Iter [1] Loss: 0.8803 Training Accuracy: 0.64444\n",
      "Epoch [222/2000], Iter [1] Loss: 0.8912 Training Accuracy: 0.64444\n",
      "Epoch [223/2000], Iter [1] Loss: 0.8918 Training Accuracy: 0.63704\n",
      "Epoch [224/2000], Iter [1] Loss: 0.8813 Training Accuracy: 0.62222\n",
      "Epoch [225/2000], Iter [1] Loss: 0.8924 Training Accuracy: 0.64444\n",
      "Epoch [226/2000], Iter [1] Loss: 0.8903 Training Accuracy: 0.62963\n",
      "Epoch [227/2000], Iter [1] Loss: 0.8753 Training Accuracy: 0.64444\n",
      "Epoch [228/2000], Iter [1] Loss: 0.8685 Training Accuracy: 0.65926\n",
      "Epoch [229/2000], Iter [1] Loss: 0.8736 Training Accuracy: 0.64444\n",
      "Epoch [230/2000], Iter [1] Loss: 0.8711 Training Accuracy: 0.64444\n",
      "Epoch [231/2000], Iter [1] Loss: 0.8716 Training Accuracy: 0.65926\n",
      "Epoch [232/2000], Iter [1] Loss: 0.8782 Training Accuracy: 0.64444\n",
      "Epoch [233/2000], Iter [1] Loss: 0.8775 Training Accuracy: 0.65185\n",
      "Epoch [234/2000], Iter [1] Loss: 0.8754 Training Accuracy: 0.65185\n",
      "Epoch [235/2000], Iter [1] Loss: 0.8689 Training Accuracy: 0.63704\n",
      "Epoch [236/2000], Iter [1] Loss: 0.8733 Training Accuracy: 0.63704\n",
      "Epoch [237/2000], Iter [1] Loss: 0.8718 Training Accuracy: 0.64444\n",
      "Epoch [238/2000], Iter [1] Loss: 0.8549 Training Accuracy: 0.65185\n",
      "Epoch [239/2000], Iter [1] Loss: 0.8618 Training Accuracy: 0.64444\n",
      "Epoch [240/2000], Iter [1] Loss: 0.8482 Training Accuracy: 0.64444\n",
      "Epoch [241/2000], Iter [1] Loss: 0.8652 Training Accuracy: 0.65185\n",
      "Epoch [242/2000], Iter [1] Loss: 0.8606 Training Accuracy: 0.66667\n",
      "Epoch [243/2000], Iter [1] Loss: 0.8658 Training Accuracy: 0.65185\n",
      "Epoch [244/2000], Iter [1] Loss: 0.8698 Training Accuracy: 0.63704\n",
      "Epoch [245/2000], Iter [1] Loss: 0.8557 Training Accuracy: 0.65185\n",
      "Epoch [246/2000], Iter [1] Loss: 0.8502 Training Accuracy: 0.63704\n",
      "Epoch [247/2000], Iter [1] Loss: 0.8578 Training Accuracy: 0.63704\n",
      "Epoch [248/2000], Iter [1] Loss: 0.8427 Training Accuracy: 0.67407\n",
      "Epoch [249/2000], Iter [1] Loss: 0.8580 Training Accuracy: 0.63704\n",
      "Epoch [250/2000], Iter [1] Loss: 0.8463 Training Accuracy: 0.64444\n",
      "Epoch [251/2000], Iter [1] Loss: 0.8448 Training Accuracy: 0.66667\n",
      "Epoch [252/2000], Iter [1] Loss: 0.8378 Training Accuracy: 0.64444\n",
      "Epoch [253/2000], Iter [1] Loss: 0.8429 Training Accuracy: 0.66667\n",
      "Epoch [254/2000], Iter [1] Loss: 0.8363 Training Accuracy: 0.66667\n",
      "Epoch [255/2000], Iter [1] Loss: 0.8410 Training Accuracy: 0.65926\n",
      "Epoch [256/2000], Iter [1] Loss: 0.8414 Training Accuracy: 0.66667\n",
      "Epoch [257/2000], Iter [1] Loss: 0.8281 Training Accuracy: 0.66667\n",
      "Epoch [258/2000], Iter [1] Loss: 0.8286 Training Accuracy: 0.66667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [259/2000], Iter [1] Loss: 0.8408 Training Accuracy: 0.65185\n",
      "Epoch [260/2000], Iter [1] Loss: 0.8201 Training Accuracy: 0.65185\n",
      "Epoch [261/2000], Iter [1] Loss: 0.8187 Training Accuracy: 0.65926\n",
      "Epoch [262/2000], Iter [1] Loss: 0.8190 Training Accuracy: 0.66667\n",
      "Epoch [263/2000], Iter [1] Loss: 0.8341 Training Accuracy: 0.65926\n",
      "Epoch [264/2000], Iter [1] Loss: 0.8149 Training Accuracy: 0.65926\n",
      "Epoch [265/2000], Iter [1] Loss: 0.8150 Training Accuracy: 0.67407\n",
      "Epoch [266/2000], Iter [1] Loss: 0.8311 Training Accuracy: 0.67407\n",
      "Epoch [267/2000], Iter [1] Loss: 0.8249 Training Accuracy: 0.68148\n",
      "Epoch [268/2000], Iter [1] Loss: 0.8160 Training Accuracy: 0.66667\n",
      "Epoch [269/2000], Iter [1] Loss: 0.8089 Training Accuracy: 0.67407\n",
      "Epoch [270/2000], Iter [1] Loss: 0.8068 Training Accuracy: 0.69630\n",
      "Epoch [271/2000], Iter [1] Loss: 0.8109 Training Accuracy: 0.67407\n",
      "Epoch [272/2000], Iter [1] Loss: 0.8098 Training Accuracy: 0.66667\n",
      "Epoch [273/2000], Iter [1] Loss: 0.8215 Training Accuracy: 0.68889\n",
      "Epoch [274/2000], Iter [1] Loss: 0.8178 Training Accuracy: 0.67407\n",
      "Epoch [275/2000], Iter [1] Loss: 0.8082 Training Accuracy: 0.68889\n",
      "Epoch [276/2000], Iter [1] Loss: 0.8016 Training Accuracy: 0.69630\n",
      "Epoch [277/2000], Iter [1] Loss: 0.8106 Training Accuracy: 0.66667\n",
      "Epoch [278/2000], Iter [1] Loss: 0.8096 Training Accuracy: 0.68148\n",
      "Epoch [279/2000], Iter [1] Loss: 0.8029 Training Accuracy: 0.68148\n",
      "Epoch [280/2000], Iter [1] Loss: 0.8108 Training Accuracy: 0.68148\n",
      "Epoch [281/2000], Iter [1] Loss: 0.8018 Training Accuracy: 0.68889\n",
      "Epoch [282/2000], Iter [1] Loss: 0.8028 Training Accuracy: 0.68889\n",
      "Epoch [283/2000], Iter [1] Loss: 0.7966 Training Accuracy: 0.68889\n",
      "Epoch [284/2000], Iter [1] Loss: 0.8095 Training Accuracy: 0.69630\n",
      "Epoch [285/2000], Iter [1] Loss: 0.7797 Training Accuracy: 0.66667\n",
      "Epoch [286/2000], Iter [1] Loss: 0.7912 Training Accuracy: 0.65926\n",
      "Epoch [287/2000], Iter [1] Loss: 0.7861 Training Accuracy: 0.68889\n",
      "Epoch [288/2000], Iter [1] Loss: 0.7863 Training Accuracy: 0.67407\n",
      "Epoch [289/2000], Iter [1] Loss: 0.7806 Training Accuracy: 0.68148\n",
      "Epoch [290/2000], Iter [1] Loss: 0.7803 Training Accuracy: 0.67407\n",
      "Epoch [291/2000], Iter [1] Loss: 0.7779 Training Accuracy: 0.67407\n",
      "Epoch [292/2000], Iter [1] Loss: 0.7818 Training Accuracy: 0.67407\n",
      "Epoch [293/2000], Iter [1] Loss: 0.7784 Training Accuracy: 0.69630\n",
      "Epoch [294/2000], Iter [1] Loss: 0.7818 Training Accuracy: 0.68889\n",
      "Epoch [295/2000], Iter [1] Loss: 0.7806 Training Accuracy: 0.68148\n",
      "Epoch [296/2000], Iter [1] Loss: 0.7808 Training Accuracy: 0.67407\n",
      "Epoch [297/2000], Iter [1] Loss: 0.7827 Training Accuracy: 0.68889\n",
      "Epoch [298/2000], Iter [1] Loss: 0.7824 Training Accuracy: 0.68889\n",
      "Epoch [299/2000], Iter [1] Loss: 0.7715 Training Accuracy: 0.68148\n",
      "Epoch [300/2000], Iter [1] Loss: 0.7693 Training Accuracy: 0.68148\n",
      "Epoch [301/2000], Iter [1] Loss: 0.7632 Training Accuracy: 0.68889\n",
      "Epoch [302/2000], Iter [1] Loss: 0.7759 Training Accuracy: 0.68889\n",
      "Epoch [303/2000], Iter [1] Loss: 0.7794 Training Accuracy: 0.69630\n",
      "Epoch [304/2000], Iter [1] Loss: 0.7675 Training Accuracy: 0.71111\n",
      "Epoch [305/2000], Iter [1] Loss: 0.7716 Training Accuracy: 0.69630\n",
      "Epoch [306/2000], Iter [1] Loss: 0.7710 Training Accuracy: 0.69630\n",
      "Epoch [307/2000], Iter [1] Loss: 0.7559 Training Accuracy: 0.69630\n",
      "Epoch [308/2000], Iter [1] Loss: 0.7558 Training Accuracy: 0.70370\n",
      "Epoch [309/2000], Iter [1] Loss: 0.7493 Training Accuracy: 0.71111\n",
      "Epoch [310/2000], Iter [1] Loss: 0.7717 Training Accuracy: 0.71111\n",
      "Epoch [311/2000], Iter [1] Loss: 0.7422 Training Accuracy: 0.70370\n",
      "Epoch [312/2000], Iter [1] Loss: 0.7539 Training Accuracy: 0.69630\n",
      "Epoch [313/2000], Iter [1] Loss: 0.7556 Training Accuracy: 0.71111\n",
      "Epoch [314/2000], Iter [1] Loss: 0.7597 Training Accuracy: 0.71111\n",
      "Epoch [315/2000], Iter [1] Loss: 0.7530 Training Accuracy: 0.71852\n",
      "Epoch [316/2000], Iter [1] Loss: 0.7561 Training Accuracy: 0.68889\n",
      "Epoch [317/2000], Iter [1] Loss: 0.7547 Training Accuracy: 0.71111\n",
      "Epoch [318/2000], Iter [1] Loss: 0.7552 Training Accuracy: 0.70370\n",
      "Epoch [319/2000], Iter [1] Loss: 0.7441 Training Accuracy: 0.68889\n",
      "Epoch [320/2000], Iter [1] Loss: 0.7374 Training Accuracy: 0.69630\n",
      "Epoch [321/2000], Iter [1] Loss: 0.7334 Training Accuracy: 0.70370\n",
      "Epoch [322/2000], Iter [1] Loss: 0.7374 Training Accuracy: 0.69630\n",
      "Epoch [323/2000], Iter [1] Loss: 0.7368 Training Accuracy: 0.68889\n",
      "Epoch [324/2000], Iter [1] Loss: 0.7448 Training Accuracy: 0.70370\n",
      "Epoch [325/2000], Iter [1] Loss: 0.7567 Training Accuracy: 0.69630\n",
      "Epoch [326/2000], Iter [1] Loss: 0.7548 Training Accuracy: 0.70370\n",
      "Epoch [327/2000], Iter [1] Loss: 0.7362 Training Accuracy: 0.69630\n",
      "Epoch [328/2000], Iter [1] Loss: 0.7359 Training Accuracy: 0.71111\n",
      "Epoch [329/2000], Iter [1] Loss: 0.7496 Training Accuracy: 0.71111\n",
      "Epoch [330/2000], Iter [1] Loss: 0.7454 Training Accuracy: 0.70370\n",
      "Epoch [331/2000], Iter [1] Loss: 0.7344 Training Accuracy: 0.70370\n",
      "Epoch [332/2000], Iter [1] Loss: 0.7340 Training Accuracy: 0.70370\n",
      "Epoch [333/2000], Iter [1] Loss: 0.7368 Training Accuracy: 0.72593\n",
      "Epoch [334/2000], Iter [1] Loss: 0.7229 Training Accuracy: 0.71111\n",
      "Epoch [335/2000], Iter [1] Loss: 0.7373 Training Accuracy: 0.70370\n",
      "Epoch [336/2000], Iter [1] Loss: 0.7269 Training Accuracy: 0.71111\n",
      "Epoch [337/2000], Iter [1] Loss: 0.7215 Training Accuracy: 0.70370\n",
      "Epoch [338/2000], Iter [1] Loss: 0.7159 Training Accuracy: 0.71111\n",
      "Epoch [339/2000], Iter [1] Loss: 0.7253 Training Accuracy: 0.72593\n",
      "Epoch [340/2000], Iter [1] Loss: 0.7204 Training Accuracy: 0.72593\n",
      "Epoch [341/2000], Iter [1] Loss: 0.7197 Training Accuracy: 0.70370\n",
      "Epoch [342/2000], Iter [1] Loss: 0.7235 Training Accuracy: 0.71111\n",
      "Epoch [343/2000], Iter [1] Loss: 0.7055 Training Accuracy: 0.71852\n",
      "Epoch [344/2000], Iter [1] Loss: 0.7185 Training Accuracy: 0.71111\n",
      "Epoch [345/2000], Iter [1] Loss: 0.7094 Training Accuracy: 0.72593\n",
      "Epoch [346/2000], Iter [1] Loss: 0.7091 Training Accuracy: 0.68889\n",
      "Epoch [347/2000], Iter [1] Loss: 0.7148 Training Accuracy: 0.71852\n",
      "Epoch [348/2000], Iter [1] Loss: 0.7159 Training Accuracy: 0.71111\n",
      "Epoch [349/2000], Iter [1] Loss: 0.7232 Training Accuracy: 0.71111\n",
      "Epoch [350/2000], Iter [1] Loss: 0.6985 Training Accuracy: 0.72593\n",
      "Epoch [351/2000], Iter [1] Loss: 0.7128 Training Accuracy: 0.71852\n",
      "Epoch [352/2000], Iter [1] Loss: 0.7094 Training Accuracy: 0.71111\n",
      "Epoch [353/2000], Iter [1] Loss: 0.7064 Training Accuracy: 0.72593\n",
      "Epoch [354/2000], Iter [1] Loss: 0.6996 Training Accuracy: 0.71111\n",
      "Epoch [355/2000], Iter [1] Loss: 0.7043 Training Accuracy: 0.73333\n",
      "Epoch [356/2000], Iter [1] Loss: 0.7129 Training Accuracy: 0.70370\n",
      "Epoch [357/2000], Iter [1] Loss: 0.7072 Training Accuracy: 0.73333\n",
      "Epoch [358/2000], Iter [1] Loss: 0.7025 Training Accuracy: 0.71111\n",
      "Epoch [359/2000], Iter [1] Loss: 0.7046 Training Accuracy: 0.72593\n",
      "Epoch [360/2000], Iter [1] Loss: 0.7123 Training Accuracy: 0.72593\n",
      "Epoch [361/2000], Iter [1] Loss: 0.7061 Training Accuracy: 0.71111\n",
      "Epoch [362/2000], Iter [1] Loss: 0.7066 Training Accuracy: 0.71852\n",
      "Epoch [363/2000], Iter [1] Loss: 0.7035 Training Accuracy: 0.71111\n",
      "Epoch [364/2000], Iter [1] Loss: 0.7021 Training Accuracy: 0.73333\n",
      "Epoch [365/2000], Iter [1] Loss: 0.6905 Training Accuracy: 0.71852\n",
      "Epoch [366/2000], Iter [1] Loss: 0.6985 Training Accuracy: 0.74074\n",
      "Epoch [367/2000], Iter [1] Loss: 0.7036 Training Accuracy: 0.71111\n",
      "Epoch [368/2000], Iter [1] Loss: 0.6882 Training Accuracy: 0.72593\n",
      "Epoch [369/2000], Iter [1] Loss: 0.6929 Training Accuracy: 0.71852\n",
      "Epoch [370/2000], Iter [1] Loss: 0.6955 Training Accuracy: 0.72593\n",
      "Epoch [371/2000], Iter [1] Loss: 0.6905 Training Accuracy: 0.71852\n",
      "Epoch [372/2000], Iter [1] Loss: 0.7070 Training Accuracy: 0.72593\n",
      "Epoch [373/2000], Iter [1] Loss: 0.6900 Training Accuracy: 0.73333\n",
      "Epoch [374/2000], Iter [1] Loss: 0.6899 Training Accuracy: 0.74074\n",
      "Epoch [375/2000], Iter [1] Loss: 0.6708 Training Accuracy: 0.74074\n",
      "Epoch [376/2000], Iter [1] Loss: 0.6692 Training Accuracy: 0.74074\n",
      "Epoch [377/2000], Iter [1] Loss: 0.6917 Training Accuracy: 0.72593\n",
      "Epoch [378/2000], Iter [1] Loss: 0.6797 Training Accuracy: 0.73333\n",
      "Epoch [379/2000], Iter [1] Loss: 0.6852 Training Accuracy: 0.72593\n",
      "Epoch [380/2000], Iter [1] Loss: 0.6763 Training Accuracy: 0.71111\n",
      "Epoch [381/2000], Iter [1] Loss: 0.6648 Training Accuracy: 0.70370\n",
      "Epoch [382/2000], Iter [1] Loss: 0.6733 Training Accuracy: 0.71111\n",
      "Epoch [383/2000], Iter [1] Loss: 0.6790 Training Accuracy: 0.71852\n",
      "Epoch [384/2000], Iter [1] Loss: 0.6796 Training Accuracy: 0.72593\n",
      "Epoch [385/2000], Iter [1] Loss: 0.6675 Training Accuracy: 0.74074\n",
      "Epoch [386/2000], Iter [1] Loss: 0.6703 Training Accuracy: 0.73333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [387/2000], Iter [1] Loss: 0.6755 Training Accuracy: 0.72593\n",
      "Epoch [388/2000], Iter [1] Loss: 0.6776 Training Accuracy: 0.74815\n",
      "Epoch [389/2000], Iter [1] Loss: 0.6824 Training Accuracy: 0.71111\n",
      "Epoch [390/2000], Iter [1] Loss: 0.6668 Training Accuracy: 0.74815\n",
      "Epoch [391/2000], Iter [1] Loss: 0.6631 Training Accuracy: 0.72593\n",
      "Epoch [392/2000], Iter [1] Loss: 0.6802 Training Accuracy: 0.74074\n",
      "Epoch [393/2000], Iter [1] Loss: 0.6630 Training Accuracy: 0.71111\n",
      "Epoch [394/2000], Iter [1] Loss: 0.6481 Training Accuracy: 0.72593\n",
      "Epoch [395/2000], Iter [1] Loss: 0.6628 Training Accuracy: 0.72593\n",
      "Epoch [396/2000], Iter [1] Loss: 0.6547 Training Accuracy: 0.72593\n",
      "Epoch [397/2000], Iter [1] Loss: 0.6771 Training Accuracy: 0.74074\n",
      "Epoch [398/2000], Iter [1] Loss: 0.6572 Training Accuracy: 0.73333\n",
      "Epoch [399/2000], Iter [1] Loss: 0.6389 Training Accuracy: 0.74074\n",
      "Epoch [400/2000], Iter [1] Loss: 0.6619 Training Accuracy: 0.73333\n",
      "Epoch [401/2000], Iter [1] Loss: 0.6618 Training Accuracy: 0.72593\n",
      "Epoch [402/2000], Iter [1] Loss: 0.6483 Training Accuracy: 0.73333\n",
      "Epoch [403/2000], Iter [1] Loss: 0.6456 Training Accuracy: 0.74074\n",
      "Epoch [404/2000], Iter [1] Loss: 0.6517 Training Accuracy: 0.74074\n",
      "Epoch [405/2000], Iter [1] Loss: 0.6538 Training Accuracy: 0.72593\n",
      "Epoch [406/2000], Iter [1] Loss: 0.6591 Training Accuracy: 0.74815\n",
      "Epoch [407/2000], Iter [1] Loss: 0.6622 Training Accuracy: 0.74074\n",
      "Epoch [408/2000], Iter [1] Loss: 0.6524 Training Accuracy: 0.71852\n",
      "Epoch [409/2000], Iter [1] Loss: 0.6473 Training Accuracy: 0.72593\n",
      "Epoch [410/2000], Iter [1] Loss: 0.6507 Training Accuracy: 0.74074\n",
      "Epoch [411/2000], Iter [1] Loss: 0.6421 Training Accuracy: 0.74074\n",
      "Epoch [412/2000], Iter [1] Loss: 0.6616 Training Accuracy: 0.73333\n",
      "Epoch [413/2000], Iter [1] Loss: 0.6489 Training Accuracy: 0.74815\n",
      "Epoch [414/2000], Iter [1] Loss: 0.6412 Training Accuracy: 0.74074\n",
      "Epoch [415/2000], Iter [1] Loss: 0.6389 Training Accuracy: 0.73333\n",
      "Epoch [416/2000], Iter [1] Loss: 0.6570 Training Accuracy: 0.71852\n",
      "Epoch [417/2000], Iter [1] Loss: 0.6547 Training Accuracy: 0.73333\n",
      "Epoch [418/2000], Iter [1] Loss: 0.6317 Training Accuracy: 0.73333\n",
      "Epoch [419/2000], Iter [1] Loss: 0.6398 Training Accuracy: 0.74815\n",
      "Epoch [420/2000], Iter [1] Loss: 0.6437 Training Accuracy: 0.74815\n",
      "Epoch [421/2000], Iter [1] Loss: 0.6303 Training Accuracy: 0.73333\n",
      "Epoch [422/2000], Iter [1] Loss: 0.6400 Training Accuracy: 0.74074\n",
      "Epoch [423/2000], Iter [1] Loss: 0.6340 Training Accuracy: 0.74074\n",
      "Epoch [424/2000], Iter [1] Loss: 0.6418 Training Accuracy: 0.72593\n",
      "Epoch [425/2000], Iter [1] Loss: 0.6269 Training Accuracy: 0.73333\n",
      "Epoch [426/2000], Iter [1] Loss: 0.6396 Training Accuracy: 0.74074\n",
      "Epoch [427/2000], Iter [1] Loss: 0.6226 Training Accuracy: 0.74074\n",
      "Epoch [428/2000], Iter [1] Loss: 0.6372 Training Accuracy: 0.74815\n",
      "Epoch [429/2000], Iter [1] Loss: 0.6280 Training Accuracy: 0.74815\n",
      "Epoch [430/2000], Iter [1] Loss: 0.6232 Training Accuracy: 0.75556\n",
      "Epoch [431/2000], Iter [1] Loss: 0.6276 Training Accuracy: 0.74815\n",
      "Epoch [432/2000], Iter [1] Loss: 0.6283 Training Accuracy: 0.73333\n",
      "Epoch [433/2000], Iter [1] Loss: 0.6310 Training Accuracy: 0.74815\n",
      "Epoch [434/2000], Iter [1] Loss: 0.6378 Training Accuracy: 0.74815\n",
      "Epoch [435/2000], Iter [1] Loss: 0.6270 Training Accuracy: 0.74815\n",
      "Epoch [436/2000], Iter [1] Loss: 0.6269 Training Accuracy: 0.73333\n",
      "Epoch [437/2000], Iter [1] Loss: 0.6300 Training Accuracy: 0.74074\n",
      "Epoch [438/2000], Iter [1] Loss: 0.6099 Training Accuracy: 0.75556\n",
      "Epoch [439/2000], Iter [1] Loss: 0.6319 Training Accuracy: 0.75556\n",
      "Epoch [440/2000], Iter [1] Loss: 0.6123 Training Accuracy: 0.75556\n",
      "Epoch [441/2000], Iter [1] Loss: 0.6373 Training Accuracy: 0.74074\n",
      "Epoch [442/2000], Iter [1] Loss: 0.6055 Training Accuracy: 0.74815\n",
      "Epoch [443/2000], Iter [1] Loss: 0.6206 Training Accuracy: 0.75556\n",
      "Epoch [444/2000], Iter [1] Loss: 0.6344 Training Accuracy: 0.73333\n",
      "Epoch [445/2000], Iter [1] Loss: 0.6187 Training Accuracy: 0.75556\n",
      "Epoch [446/2000], Iter [1] Loss: 0.6091 Training Accuracy: 0.74815\n",
      "Epoch [447/2000], Iter [1] Loss: 0.6229 Training Accuracy: 0.74815\n",
      "Epoch [448/2000], Iter [1] Loss: 0.6070 Training Accuracy: 0.74815\n",
      "Epoch [449/2000], Iter [1] Loss: 0.6351 Training Accuracy: 0.75556\n",
      "Epoch [450/2000], Iter [1] Loss: 0.6082 Training Accuracy: 0.74815\n",
      "Epoch [451/2000], Iter [1] Loss: 0.6178 Training Accuracy: 0.75556\n",
      "Epoch [452/2000], Iter [1] Loss: 0.6255 Training Accuracy: 0.74815\n",
      "Epoch [453/2000], Iter [1] Loss: 0.6132 Training Accuracy: 0.74815\n",
      "Epoch [454/2000], Iter [1] Loss: 0.6409 Training Accuracy: 0.76296\n",
      "Epoch [455/2000], Iter [1] Loss: 0.6197 Training Accuracy: 0.74815\n",
      "Epoch [456/2000], Iter [1] Loss: 0.6180 Training Accuracy: 0.76296\n",
      "Epoch [457/2000], Iter [1] Loss: 0.6058 Training Accuracy: 0.74815\n",
      "Epoch [458/2000], Iter [1] Loss: 0.6100 Training Accuracy: 0.74815\n",
      "Epoch [459/2000], Iter [1] Loss: 0.6017 Training Accuracy: 0.74074\n",
      "Epoch [460/2000], Iter [1] Loss: 0.6174 Training Accuracy: 0.76296\n",
      "Epoch [461/2000], Iter [1] Loss: 0.5922 Training Accuracy: 0.76296\n",
      "Epoch [462/2000], Iter [1] Loss: 0.6051 Training Accuracy: 0.74074\n",
      "Epoch [463/2000], Iter [1] Loss: 0.6203 Training Accuracy: 0.74074\n",
      "Epoch [464/2000], Iter [1] Loss: 0.6247 Training Accuracy: 0.77037\n",
      "Epoch [465/2000], Iter [1] Loss: 0.6294 Training Accuracy: 0.75556\n",
      "Epoch [466/2000], Iter [1] Loss: 0.5951 Training Accuracy: 0.75556\n",
      "Epoch [467/2000], Iter [1] Loss: 0.6026 Training Accuracy: 0.75556\n",
      "Epoch [468/2000], Iter [1] Loss: 0.6117 Training Accuracy: 0.75556\n",
      "Epoch [469/2000], Iter [1] Loss: 0.6162 Training Accuracy: 0.76296\n",
      "Epoch [470/2000], Iter [1] Loss: 0.6125 Training Accuracy: 0.75556\n",
      "Epoch [471/2000], Iter [1] Loss: 0.6165 Training Accuracy: 0.75556\n",
      "Epoch [472/2000], Iter [1] Loss: 0.5956 Training Accuracy: 0.74074\n",
      "Epoch [473/2000], Iter [1] Loss: 0.5872 Training Accuracy: 0.76296\n",
      "Epoch [474/2000], Iter [1] Loss: 0.6042 Training Accuracy: 0.77037\n",
      "Epoch [475/2000], Iter [1] Loss: 0.5959 Training Accuracy: 0.73333\n",
      "Epoch [476/2000], Iter [1] Loss: 0.5946 Training Accuracy: 0.76296\n",
      "Epoch [477/2000], Iter [1] Loss: 0.5902 Training Accuracy: 0.75556\n",
      "Epoch [478/2000], Iter [1] Loss: 0.6087 Training Accuracy: 0.75556\n",
      "Epoch [479/2000], Iter [1] Loss: 0.5928 Training Accuracy: 0.75556\n",
      "Epoch [480/2000], Iter [1] Loss: 0.5973 Training Accuracy: 0.74815\n",
      "Epoch [481/2000], Iter [1] Loss: 0.6023 Training Accuracy: 0.74815\n",
      "Epoch [482/2000], Iter [1] Loss: 0.5766 Training Accuracy: 0.76296\n",
      "Epoch [483/2000], Iter [1] Loss: 0.5808 Training Accuracy: 0.75556\n",
      "Epoch [484/2000], Iter [1] Loss: 0.5906 Training Accuracy: 0.74815\n",
      "Epoch [485/2000], Iter [1] Loss: 0.5934 Training Accuracy: 0.74815\n",
      "Epoch [486/2000], Iter [1] Loss: 0.5886 Training Accuracy: 0.76296\n",
      "Epoch [487/2000], Iter [1] Loss: 0.5893 Training Accuracy: 0.74074\n",
      "Epoch [488/2000], Iter [1] Loss: 0.5852 Training Accuracy: 0.74815\n",
      "Epoch [489/2000], Iter [1] Loss: 0.5983 Training Accuracy: 0.76296\n",
      "Epoch [490/2000], Iter [1] Loss: 0.5834 Training Accuracy: 0.76296\n",
      "Epoch [491/2000], Iter [1] Loss: 0.5911 Training Accuracy: 0.75556\n",
      "Epoch [492/2000], Iter [1] Loss: 0.5824 Training Accuracy: 0.74815\n",
      "Epoch [493/2000], Iter [1] Loss: 0.5798 Training Accuracy: 0.76296\n",
      "Epoch [494/2000], Iter [1] Loss: 0.5809 Training Accuracy: 0.75556\n",
      "Epoch [495/2000], Iter [1] Loss: 0.5745 Training Accuracy: 0.76296\n",
      "Epoch [496/2000], Iter [1] Loss: 0.5777 Training Accuracy: 0.74815\n",
      "Epoch [497/2000], Iter [1] Loss: 0.5825 Training Accuracy: 0.75556\n",
      "Epoch [498/2000], Iter [1] Loss: 0.5760 Training Accuracy: 0.74815\n",
      "Epoch [499/2000], Iter [1] Loss: 0.5822 Training Accuracy: 0.76296\n",
      "Epoch [500/2000], Iter [1] Loss: 0.5908 Training Accuracy: 0.75556\n",
      "Epoch [501/2000], Iter [1] Loss: 0.5655 Training Accuracy: 0.74815\n",
      "Epoch [502/2000], Iter [1] Loss: 0.5879 Training Accuracy: 0.76296\n",
      "Epoch [503/2000], Iter [1] Loss: 0.5853 Training Accuracy: 0.76296\n",
      "Epoch [504/2000], Iter [1] Loss: 0.5853 Training Accuracy: 0.74815\n",
      "Epoch [505/2000], Iter [1] Loss: 0.5646 Training Accuracy: 0.76296\n",
      "Epoch [506/2000], Iter [1] Loss: 0.5741 Training Accuracy: 0.78519\n",
      "Epoch [507/2000], Iter [1] Loss: 0.5917 Training Accuracy: 0.77037\n",
      "Epoch [508/2000], Iter [1] Loss: 0.5828 Training Accuracy: 0.76296\n",
      "Epoch [509/2000], Iter [1] Loss: 0.5854 Training Accuracy: 0.75556\n",
      "Epoch [510/2000], Iter [1] Loss: 0.5727 Training Accuracy: 0.74815\n",
      "Epoch [511/2000], Iter [1] Loss: 0.5727 Training Accuracy: 0.74815\n",
      "Epoch [512/2000], Iter [1] Loss: 0.5859 Training Accuracy: 0.77778\n",
      "Epoch [513/2000], Iter [1] Loss: 0.5820 Training Accuracy: 0.76296\n",
      "Epoch [514/2000], Iter [1] Loss: 0.5768 Training Accuracy: 0.76296\n",
      "Epoch [515/2000], Iter [1] Loss: 0.5575 Training Accuracy: 0.76296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [516/2000], Iter [1] Loss: 0.5772 Training Accuracy: 0.75556\n",
      "Epoch [517/2000], Iter [1] Loss: 0.5639 Training Accuracy: 0.76296\n",
      "Epoch [518/2000], Iter [1] Loss: 0.5708 Training Accuracy: 0.74815\n",
      "Epoch [519/2000], Iter [1] Loss: 0.5697 Training Accuracy: 0.76296\n",
      "Epoch [520/2000], Iter [1] Loss: 0.5637 Training Accuracy: 0.75556\n",
      "Epoch [521/2000], Iter [1] Loss: 0.5652 Training Accuracy: 0.77037\n",
      "Epoch [522/2000], Iter [1] Loss: 0.5596 Training Accuracy: 0.75556\n",
      "Epoch [523/2000], Iter [1] Loss: 0.5509 Training Accuracy: 0.75556\n",
      "Epoch [524/2000], Iter [1] Loss: 0.5774 Training Accuracy: 0.74074\n",
      "Epoch [525/2000], Iter [1] Loss: 0.5631 Training Accuracy: 0.74074\n",
      "Epoch [526/2000], Iter [1] Loss: 0.5631 Training Accuracy: 0.77037\n",
      "Epoch [527/2000], Iter [1] Loss: 0.5627 Training Accuracy: 0.75556\n",
      "Epoch [528/2000], Iter [1] Loss: 0.5565 Training Accuracy: 0.74074\n",
      "Epoch [529/2000], Iter [1] Loss: 0.5558 Training Accuracy: 0.77037\n",
      "Epoch [530/2000], Iter [1] Loss: 0.5782 Training Accuracy: 0.74815\n",
      "Epoch [531/2000], Iter [1] Loss: 0.5795 Training Accuracy: 0.77037\n",
      "Epoch [532/2000], Iter [1] Loss: 0.5715 Training Accuracy: 0.77037\n",
      "Epoch [533/2000], Iter [1] Loss: 0.5477 Training Accuracy: 0.75556\n",
      "Epoch [534/2000], Iter [1] Loss: 0.5467 Training Accuracy: 0.74815\n",
      "Epoch [535/2000], Iter [1] Loss: 0.5601 Training Accuracy: 0.76296\n",
      "Epoch [536/2000], Iter [1] Loss: 0.5586 Training Accuracy: 0.77037\n",
      "Epoch [537/2000], Iter [1] Loss: 0.5584 Training Accuracy: 0.76296\n",
      "Epoch [538/2000], Iter [1] Loss: 0.5670 Training Accuracy: 0.76296\n",
      "Epoch [539/2000], Iter [1] Loss: 0.5615 Training Accuracy: 0.77037\n",
      "Epoch [540/2000], Iter [1] Loss: 0.5580 Training Accuracy: 0.75556\n",
      "Epoch [541/2000], Iter [1] Loss: 0.5556 Training Accuracy: 0.77037\n",
      "Epoch [542/2000], Iter [1] Loss: 0.5521 Training Accuracy: 0.77037\n",
      "Epoch [543/2000], Iter [1] Loss: 0.5476 Training Accuracy: 0.76296\n",
      "Epoch [544/2000], Iter [1] Loss: 0.5491 Training Accuracy: 0.75556\n",
      "Epoch [545/2000], Iter [1] Loss: 0.5443 Training Accuracy: 0.76296\n",
      "Epoch [546/2000], Iter [1] Loss: 0.5544 Training Accuracy: 0.77778\n",
      "Epoch [547/2000], Iter [1] Loss: 0.5513 Training Accuracy: 0.75556\n",
      "Epoch [548/2000], Iter [1] Loss: 0.5447 Training Accuracy: 0.75556\n",
      "Epoch [549/2000], Iter [1] Loss: 0.5594 Training Accuracy: 0.76296\n",
      "Epoch [550/2000], Iter [1] Loss: 0.5526 Training Accuracy: 0.74815\n",
      "Epoch [551/2000], Iter [1] Loss: 0.5471 Training Accuracy: 0.75556\n",
      "Epoch [552/2000], Iter [1] Loss: 0.5582 Training Accuracy: 0.74074\n",
      "Epoch [553/2000], Iter [1] Loss: 0.5618 Training Accuracy: 0.75556\n",
      "Epoch [554/2000], Iter [1] Loss: 0.5570 Training Accuracy: 0.77778\n",
      "Epoch [555/2000], Iter [1] Loss: 0.5606 Training Accuracy: 0.77037\n",
      "Epoch [556/2000], Iter [1] Loss: 0.5410 Training Accuracy: 0.77037\n",
      "Epoch [557/2000], Iter [1] Loss: 0.5550 Training Accuracy: 0.76296\n",
      "Epoch [558/2000], Iter [1] Loss: 0.5333 Training Accuracy: 0.77037\n",
      "Epoch [559/2000], Iter [1] Loss: 0.5454 Training Accuracy: 0.78519\n",
      "Epoch [560/2000], Iter [1] Loss: 0.5544 Training Accuracy: 0.77037\n",
      "Epoch [561/2000], Iter [1] Loss: 0.5479 Training Accuracy: 0.79259\n",
      "Epoch [562/2000], Iter [1] Loss: 0.5476 Training Accuracy: 0.77778\n",
      "Epoch [563/2000], Iter [1] Loss: 0.5524 Training Accuracy: 0.77037\n",
      "Epoch [564/2000], Iter [1] Loss: 0.5405 Training Accuracy: 0.76296\n",
      "Epoch [565/2000], Iter [1] Loss: 0.5397 Training Accuracy: 0.76296\n",
      "Epoch [566/2000], Iter [1] Loss: 0.5283 Training Accuracy: 0.77037\n",
      "Epoch [567/2000], Iter [1] Loss: 0.5433 Training Accuracy: 0.76296\n",
      "Epoch [568/2000], Iter [1] Loss: 0.5426 Training Accuracy: 0.77037\n",
      "Epoch [569/2000], Iter [1] Loss: 0.5378 Training Accuracy: 0.76296\n",
      "Epoch [570/2000], Iter [1] Loss: 0.5379 Training Accuracy: 0.76296\n",
      "Epoch [571/2000], Iter [1] Loss: 0.5383 Training Accuracy: 0.78519\n",
      "Epoch [572/2000], Iter [1] Loss: 0.5331 Training Accuracy: 0.78519\n",
      "Epoch [573/2000], Iter [1] Loss: 0.5419 Training Accuracy: 0.77778\n",
      "Epoch [574/2000], Iter [1] Loss: 0.5477 Training Accuracy: 0.76296\n",
      "Epoch [575/2000], Iter [1] Loss: 0.5278 Training Accuracy: 0.74815\n",
      "Epoch [576/2000], Iter [1] Loss: 0.5546 Training Accuracy: 0.77037\n",
      "Epoch [577/2000], Iter [1] Loss: 0.5322 Training Accuracy: 0.77778\n",
      "Epoch [578/2000], Iter [1] Loss: 0.5327 Training Accuracy: 0.79259\n",
      "Epoch [579/2000], Iter [1] Loss: 0.5360 Training Accuracy: 0.77037\n",
      "Epoch [580/2000], Iter [1] Loss: 0.5397 Training Accuracy: 0.75556\n",
      "Epoch [581/2000], Iter [1] Loss: 0.5484 Training Accuracy: 0.77037\n",
      "Epoch [582/2000], Iter [1] Loss: 0.5362 Training Accuracy: 0.75556\n",
      "Epoch [583/2000], Iter [1] Loss: 0.5332 Training Accuracy: 0.77037\n",
      "Epoch [584/2000], Iter [1] Loss: 0.5281 Training Accuracy: 0.75556\n",
      "Epoch [585/2000], Iter [1] Loss: 0.5233 Training Accuracy: 0.76296\n",
      "Epoch [586/2000], Iter [1] Loss: 0.5407 Training Accuracy: 0.74815\n",
      "Epoch [587/2000], Iter [1] Loss: 0.5226 Training Accuracy: 0.77778\n",
      "Epoch [588/2000], Iter [1] Loss: 0.5347 Training Accuracy: 0.74815\n",
      "Epoch [589/2000], Iter [1] Loss: 0.5229 Training Accuracy: 0.75556\n",
      "Epoch [590/2000], Iter [1] Loss: 0.5394 Training Accuracy: 0.78519\n",
      "Epoch [591/2000], Iter [1] Loss: 0.5190 Training Accuracy: 0.77037\n",
      "Epoch [592/2000], Iter [1] Loss: 0.5281 Training Accuracy: 0.77778\n",
      "Epoch [593/2000], Iter [1] Loss: 0.5256 Training Accuracy: 0.77037\n",
      "Epoch [594/2000], Iter [1] Loss: 0.5328 Training Accuracy: 0.77778\n",
      "Epoch [595/2000], Iter [1] Loss: 0.5361 Training Accuracy: 0.77037\n",
      "Epoch [596/2000], Iter [1] Loss: 0.5262 Training Accuracy: 0.76296\n",
      "Epoch [597/2000], Iter [1] Loss: 0.5283 Training Accuracy: 0.76296\n",
      "Epoch [598/2000], Iter [1] Loss: 0.5147 Training Accuracy: 0.77778\n",
      "Epoch [599/2000], Iter [1] Loss: 0.5324 Training Accuracy: 0.77037\n",
      "Epoch [600/2000], Iter [1] Loss: 0.5289 Training Accuracy: 0.79259\n",
      "Epoch [601/2000], Iter [1] Loss: 0.5312 Training Accuracy: 0.76296\n",
      "Epoch [602/2000], Iter [1] Loss: 0.5269 Training Accuracy: 0.74815\n",
      "Epoch [603/2000], Iter [1] Loss: 0.5215 Training Accuracy: 0.76296\n",
      "Epoch [604/2000], Iter [1] Loss: 0.5213 Training Accuracy: 0.77778\n",
      "Epoch [605/2000], Iter [1] Loss: 0.5192 Training Accuracy: 0.77037\n",
      "Epoch [606/2000], Iter [1] Loss: 0.5244 Training Accuracy: 0.78519\n",
      "Epoch [607/2000], Iter [1] Loss: 0.5231 Training Accuracy: 0.74815\n",
      "Epoch [608/2000], Iter [1] Loss: 0.5239 Training Accuracy: 0.76296\n",
      "Epoch [609/2000], Iter [1] Loss: 0.5252 Training Accuracy: 0.76296\n",
      "Epoch [610/2000], Iter [1] Loss: 0.5206 Training Accuracy: 0.77778\n",
      "Epoch [611/2000], Iter [1] Loss: 0.5201 Training Accuracy: 0.77778\n",
      "Epoch [612/2000], Iter [1] Loss: 0.5137 Training Accuracy: 0.76296\n",
      "Epoch [613/2000], Iter [1] Loss: 0.5042 Training Accuracy: 0.79259\n",
      "Epoch [614/2000], Iter [1] Loss: 0.5261 Training Accuracy: 0.78519\n",
      "Epoch [615/2000], Iter [1] Loss: 0.5217 Training Accuracy: 0.77037\n",
      "Epoch [616/2000], Iter [1] Loss: 0.5105 Training Accuracy: 0.77037\n",
      "Epoch [617/2000], Iter [1] Loss: 0.5077 Training Accuracy: 0.77778\n",
      "Epoch [618/2000], Iter [1] Loss: 0.5207 Training Accuracy: 0.78519\n",
      "Epoch [619/2000], Iter [1] Loss: 0.5103 Training Accuracy: 0.77778\n",
      "Epoch [620/2000], Iter [1] Loss: 0.5195 Training Accuracy: 0.77778\n",
      "Epoch [621/2000], Iter [1] Loss: 0.5144 Training Accuracy: 0.76296\n",
      "Epoch [622/2000], Iter [1] Loss: 0.5216 Training Accuracy: 0.77037\n",
      "Epoch [623/2000], Iter [1] Loss: 0.5145 Training Accuracy: 0.75556\n",
      "Epoch [624/2000], Iter [1] Loss: 0.5164 Training Accuracy: 0.77037\n",
      "Epoch [625/2000], Iter [1] Loss: 0.5295 Training Accuracy: 0.75556\n",
      "Epoch [626/2000], Iter [1] Loss: 0.5139 Training Accuracy: 0.77778\n",
      "Epoch [627/2000], Iter [1] Loss: 0.5134 Training Accuracy: 0.76296\n",
      "Epoch [628/2000], Iter [1] Loss: 0.5221 Training Accuracy: 0.77778\n",
      "Epoch [629/2000], Iter [1] Loss: 0.5139 Training Accuracy: 0.77778\n",
      "Epoch [630/2000], Iter [1] Loss: 0.5139 Training Accuracy: 0.77778\n",
      "Epoch [631/2000], Iter [1] Loss: 0.5057 Training Accuracy: 0.77778\n",
      "Epoch [632/2000], Iter [1] Loss: 0.5222 Training Accuracy: 0.78519\n",
      "Epoch [633/2000], Iter [1] Loss: 0.5105 Training Accuracy: 0.78519\n",
      "Epoch [634/2000], Iter [1] Loss: 0.5096 Training Accuracy: 0.77778\n",
      "Epoch [635/2000], Iter [1] Loss: 0.5040 Training Accuracy: 0.77778\n",
      "Epoch [636/2000], Iter [1] Loss: 0.5075 Training Accuracy: 0.78519\n",
      "Epoch [637/2000], Iter [1] Loss: 0.5047 Training Accuracy: 0.78519\n",
      "Epoch [638/2000], Iter [1] Loss: 0.5240 Training Accuracy: 0.78519\n",
      "Epoch [639/2000], Iter [1] Loss: 0.5067 Training Accuracy: 0.77778\n",
      "Epoch [640/2000], Iter [1] Loss: 0.5132 Training Accuracy: 0.78519\n",
      "Epoch [641/2000], Iter [1] Loss: 0.5128 Training Accuracy: 0.77037\n",
      "Epoch [642/2000], Iter [1] Loss: 0.5101 Training Accuracy: 0.77037\n",
      "Epoch [643/2000], Iter [1] Loss: 0.5036 Training Accuracy: 0.77037\n",
      "Epoch [644/2000], Iter [1] Loss: 0.5179 Training Accuracy: 0.78519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [645/2000], Iter [1] Loss: 0.5128 Training Accuracy: 0.78519\n",
      "Epoch [646/2000], Iter [1] Loss: 0.5079 Training Accuracy: 0.79259\n",
      "Epoch [647/2000], Iter [1] Loss: 0.5147 Training Accuracy: 0.77778\n",
      "Epoch [648/2000], Iter [1] Loss: 0.4992 Training Accuracy: 0.78519\n",
      "Epoch [649/2000], Iter [1] Loss: 0.5106 Training Accuracy: 0.78519\n",
      "Epoch [650/2000], Iter [1] Loss: 0.4959 Training Accuracy: 0.76296\n",
      "Epoch [651/2000], Iter [1] Loss: 0.5011 Training Accuracy: 0.79259\n",
      "Epoch [652/2000], Iter [1] Loss: 0.4989 Training Accuracy: 0.78519\n",
      "Epoch [653/2000], Iter [1] Loss: 0.5043 Training Accuracy: 0.75556\n",
      "Epoch [654/2000], Iter [1] Loss: 0.5019 Training Accuracy: 0.79259\n",
      "Epoch [655/2000], Iter [1] Loss: 0.5025 Training Accuracy: 0.77778\n",
      "Epoch [656/2000], Iter [1] Loss: 0.5052 Training Accuracy: 0.79259\n",
      "Epoch [657/2000], Iter [1] Loss: 0.5169 Training Accuracy: 0.76296\n",
      "Epoch [658/2000], Iter [1] Loss: 0.4981 Training Accuracy: 0.78519\n",
      "Epoch [659/2000], Iter [1] Loss: 0.5043 Training Accuracy: 0.75556\n",
      "Epoch [660/2000], Iter [1] Loss: 0.4918 Training Accuracy: 0.77037\n",
      "Epoch [661/2000], Iter [1] Loss: 0.4925 Training Accuracy: 0.76296\n",
      "Epoch [662/2000], Iter [1] Loss: 0.4970 Training Accuracy: 0.78519\n",
      "Epoch [663/2000], Iter [1] Loss: 0.4876 Training Accuracy: 0.77037\n",
      "Epoch [664/2000], Iter [1] Loss: 0.4965 Training Accuracy: 0.78519\n",
      "Epoch [665/2000], Iter [1] Loss: 0.5050 Training Accuracy: 0.77037\n",
      "Epoch [666/2000], Iter [1] Loss: 0.5094 Training Accuracy: 0.77778\n",
      "Epoch [667/2000], Iter [1] Loss: 0.4874 Training Accuracy: 0.77778\n",
      "Epoch [668/2000], Iter [1] Loss: 0.4912 Training Accuracy: 0.77778\n",
      "Epoch [669/2000], Iter [1] Loss: 0.4954 Training Accuracy: 0.77037\n",
      "Epoch [670/2000], Iter [1] Loss: 0.4968 Training Accuracy: 0.77037\n",
      "Epoch [671/2000], Iter [1] Loss: 0.5043 Training Accuracy: 0.77778\n",
      "Epoch [672/2000], Iter [1] Loss: 0.4962 Training Accuracy: 0.78519\n",
      "Epoch [673/2000], Iter [1] Loss: 0.4972 Training Accuracy: 0.78519\n",
      "Epoch [674/2000], Iter [1] Loss: 0.4953 Training Accuracy: 0.76296\n",
      "Epoch [675/2000], Iter [1] Loss: 0.5045 Training Accuracy: 0.76296\n",
      "Epoch [676/2000], Iter [1] Loss: 0.4944 Training Accuracy: 0.77778\n",
      "Epoch [677/2000], Iter [1] Loss: 0.4848 Training Accuracy: 0.78519\n",
      "Epoch [678/2000], Iter [1] Loss: 0.4863 Training Accuracy: 0.77037\n",
      "Epoch [679/2000], Iter [1] Loss: 0.5012 Training Accuracy: 0.77037\n",
      "Epoch [680/2000], Iter [1] Loss: 0.4995 Training Accuracy: 0.78519\n",
      "Epoch [681/2000], Iter [1] Loss: 0.4928 Training Accuracy: 0.76296\n",
      "Epoch [682/2000], Iter [1] Loss: 0.4849 Training Accuracy: 0.77778\n",
      "Epoch [683/2000], Iter [1] Loss: 0.4949 Training Accuracy: 0.77778\n",
      "Epoch [684/2000], Iter [1] Loss: 0.4906 Training Accuracy: 0.77778\n",
      "Epoch [685/2000], Iter [1] Loss: 0.5132 Training Accuracy: 0.77037\n",
      "Epoch [686/2000], Iter [1] Loss: 0.4986 Training Accuracy: 0.77778\n",
      "Epoch [687/2000], Iter [1] Loss: 0.4842 Training Accuracy: 0.77778\n",
      "Epoch [688/2000], Iter [1] Loss: 0.4876 Training Accuracy: 0.79259\n",
      "Epoch [689/2000], Iter [1] Loss: 0.4951 Training Accuracy: 0.77037\n",
      "Epoch [690/2000], Iter [1] Loss: 0.4894 Training Accuracy: 0.78519\n",
      "Epoch [691/2000], Iter [1] Loss: 0.4835 Training Accuracy: 0.77037\n",
      "Epoch [692/2000], Iter [1] Loss: 0.4823 Training Accuracy: 0.77778\n",
      "Epoch [693/2000], Iter [1] Loss: 0.4916 Training Accuracy: 0.77037\n",
      "Epoch [694/2000], Iter [1] Loss: 0.4823 Training Accuracy: 0.77037\n",
      "Epoch [695/2000], Iter [1] Loss: 0.4862 Training Accuracy: 0.79259\n",
      "Epoch [696/2000], Iter [1] Loss: 0.4948 Training Accuracy: 0.77778\n",
      "Epoch [697/2000], Iter [1] Loss: 0.4888 Training Accuracy: 0.78519\n",
      "Epoch [698/2000], Iter [1] Loss: 0.4794 Training Accuracy: 0.80000\n",
      "Epoch [699/2000], Iter [1] Loss: 0.4907 Training Accuracy: 0.78519\n",
      "Epoch [700/2000], Iter [1] Loss: 0.5001 Training Accuracy: 0.77037\n",
      "Epoch [701/2000], Iter [1] Loss: 0.4733 Training Accuracy: 0.77037\n",
      "Epoch [702/2000], Iter [1] Loss: 0.4775 Training Accuracy: 0.79259\n",
      "Epoch [703/2000], Iter [1] Loss: 0.4919 Training Accuracy: 0.78519\n",
      "Epoch [704/2000], Iter [1] Loss: 0.4767 Training Accuracy: 0.77778\n",
      "Epoch [705/2000], Iter [1] Loss: 0.4811 Training Accuracy: 0.78519\n",
      "Epoch [706/2000], Iter [1] Loss: 0.4638 Training Accuracy: 0.76296\n",
      "Epoch [707/2000], Iter [1] Loss: 0.4869 Training Accuracy: 0.77778\n",
      "Epoch [708/2000], Iter [1] Loss: 0.4898 Training Accuracy: 0.77778\n",
      "Epoch [709/2000], Iter [1] Loss: 0.4645 Training Accuracy: 0.80000\n",
      "Epoch [710/2000], Iter [1] Loss: 0.4811 Training Accuracy: 0.79259\n",
      "Epoch [711/2000], Iter [1] Loss: 0.4726 Training Accuracy: 0.77778\n",
      "Epoch [712/2000], Iter [1] Loss: 0.4940 Training Accuracy: 0.78519\n",
      "Epoch [713/2000], Iter [1] Loss: 0.4791 Training Accuracy: 0.77037\n",
      "Epoch [714/2000], Iter [1] Loss: 0.4774 Training Accuracy: 0.77037\n",
      "Epoch [715/2000], Iter [1] Loss: 0.4786 Training Accuracy: 0.78519\n",
      "Epoch [716/2000], Iter [1] Loss: 0.4779 Training Accuracy: 0.77778\n",
      "Epoch [717/2000], Iter [1] Loss: 0.4732 Training Accuracy: 0.77037\n",
      "Epoch [718/2000], Iter [1] Loss: 0.4859 Training Accuracy: 0.76296\n",
      "Epoch [719/2000], Iter [1] Loss: 0.4766 Training Accuracy: 0.78519\n",
      "Epoch [720/2000], Iter [1] Loss: 0.4810 Training Accuracy: 0.77778\n",
      "Epoch [721/2000], Iter [1] Loss: 0.4919 Training Accuracy: 0.77778\n",
      "Epoch [722/2000], Iter [1] Loss: 0.4748 Training Accuracy: 0.79259\n",
      "Epoch [723/2000], Iter [1] Loss: 0.4781 Training Accuracy: 0.77037\n",
      "Epoch [724/2000], Iter [1] Loss: 0.4922 Training Accuracy: 0.80000\n",
      "Epoch [725/2000], Iter [1] Loss: 0.4847 Training Accuracy: 0.77037\n",
      "Epoch [726/2000], Iter [1] Loss: 0.4732 Training Accuracy: 0.77778\n",
      "Epoch [727/2000], Iter [1] Loss: 0.4698 Training Accuracy: 0.79259\n",
      "Epoch [728/2000], Iter [1] Loss: 0.4802 Training Accuracy: 0.77037\n",
      "Epoch [729/2000], Iter [1] Loss: 0.4747 Training Accuracy: 0.77778\n",
      "Epoch [730/2000], Iter [1] Loss: 0.4885 Training Accuracy: 0.78519\n",
      "Epoch [731/2000], Iter [1] Loss: 0.4746 Training Accuracy: 0.78519\n",
      "Epoch [732/2000], Iter [1] Loss: 0.4639 Training Accuracy: 0.79259\n",
      "Epoch [733/2000], Iter [1] Loss: 0.4751 Training Accuracy: 0.77037\n",
      "Epoch [734/2000], Iter [1] Loss: 0.4795 Training Accuracy: 0.77778\n",
      "Epoch [735/2000], Iter [1] Loss: 0.4690 Training Accuracy: 0.77778\n",
      "Epoch [736/2000], Iter [1] Loss: 0.4834 Training Accuracy: 0.78519\n",
      "Epoch [737/2000], Iter [1] Loss: 0.4711 Training Accuracy: 0.77778\n",
      "Epoch [738/2000], Iter [1] Loss: 0.4768 Training Accuracy: 0.77037\n",
      "Epoch [739/2000], Iter [1] Loss: 0.4825 Training Accuracy: 0.79259\n",
      "Epoch [740/2000], Iter [1] Loss: 0.4771 Training Accuracy: 0.78519\n",
      "Epoch [741/2000], Iter [1] Loss: 0.4740 Training Accuracy: 0.77037\n",
      "Epoch [742/2000], Iter [1] Loss: 0.4728 Training Accuracy: 0.79259\n",
      "Epoch [743/2000], Iter [1] Loss: 0.4851 Training Accuracy: 0.76296\n",
      "Epoch [744/2000], Iter [1] Loss: 0.4818 Training Accuracy: 0.79259\n",
      "Epoch [745/2000], Iter [1] Loss: 0.4760 Training Accuracy: 0.77037\n",
      "Epoch [746/2000], Iter [1] Loss: 0.4587 Training Accuracy: 0.80000\n",
      "Epoch [747/2000], Iter [1] Loss: 0.4692 Training Accuracy: 0.78519\n",
      "Epoch [748/2000], Iter [1] Loss: 0.4694 Training Accuracy: 0.78519\n",
      "Epoch [749/2000], Iter [1] Loss: 0.4632 Training Accuracy: 0.77778\n",
      "Epoch [750/2000], Iter [1] Loss: 0.4689 Training Accuracy: 0.77778\n",
      "Epoch [751/2000], Iter [1] Loss: 0.4769 Training Accuracy: 0.77778\n",
      "Epoch [752/2000], Iter [1] Loss: 0.4580 Training Accuracy: 0.78519\n",
      "Epoch [753/2000], Iter [1] Loss: 0.4651 Training Accuracy: 0.80741\n",
      "Epoch [754/2000], Iter [1] Loss: 0.4706 Training Accuracy: 0.78519\n",
      "Epoch [755/2000], Iter [1] Loss: 0.4551 Training Accuracy: 0.78519\n",
      "Epoch [756/2000], Iter [1] Loss: 0.4672 Training Accuracy: 0.78519\n",
      "Epoch [757/2000], Iter [1] Loss: 0.4658 Training Accuracy: 0.79259\n",
      "Epoch [758/2000], Iter [1] Loss: 0.4720 Training Accuracy: 0.77778\n",
      "Epoch [759/2000], Iter [1] Loss: 0.4652 Training Accuracy: 0.77778\n",
      "Epoch [760/2000], Iter [1] Loss: 0.4716 Training Accuracy: 0.77037\n",
      "Epoch [761/2000], Iter [1] Loss: 0.4758 Training Accuracy: 0.77037\n",
      "Epoch [762/2000], Iter [1] Loss: 0.4756 Training Accuracy: 0.77037\n",
      "Epoch [763/2000], Iter [1] Loss: 0.4470 Training Accuracy: 0.76296\n",
      "Epoch [764/2000], Iter [1] Loss: 0.4861 Training Accuracy: 0.79259\n",
      "Epoch [765/2000], Iter [1] Loss: 0.4717 Training Accuracy: 0.78519\n",
      "Epoch [766/2000], Iter [1] Loss: 0.4545 Training Accuracy: 0.77778\n",
      "Epoch [767/2000], Iter [1] Loss: 0.4466 Training Accuracy: 0.78519\n",
      "Epoch [768/2000], Iter [1] Loss: 0.4577 Training Accuracy: 0.80000\n",
      "Epoch [769/2000], Iter [1] Loss: 0.4654 Training Accuracy: 0.77778\n",
      "Epoch [770/2000], Iter [1] Loss: 0.4513 Training Accuracy: 0.77778\n",
      "Epoch [771/2000], Iter [1] Loss: 0.4534 Training Accuracy: 0.78519\n",
      "Epoch [772/2000], Iter [1] Loss: 0.4604 Training Accuracy: 0.77037\n",
      "Epoch [773/2000], Iter [1] Loss: 0.4635 Training Accuracy: 0.77778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [774/2000], Iter [1] Loss: 0.4748 Training Accuracy: 0.79259\n",
      "Epoch [775/2000], Iter [1] Loss: 0.4542 Training Accuracy: 0.77778\n",
      "Epoch [776/2000], Iter [1] Loss: 0.4587 Training Accuracy: 0.77778\n",
      "Epoch [777/2000], Iter [1] Loss: 0.4518 Training Accuracy: 0.77037\n",
      "Epoch [778/2000], Iter [1] Loss: 0.4691 Training Accuracy: 0.77778\n",
      "Epoch [779/2000], Iter [1] Loss: 0.4815 Training Accuracy: 0.78519\n",
      "Epoch [780/2000], Iter [1] Loss: 0.4723 Training Accuracy: 0.78519\n",
      "Epoch [781/2000], Iter [1] Loss: 0.4478 Training Accuracy: 0.80000\n",
      "Epoch [782/2000], Iter [1] Loss: 0.4551 Training Accuracy: 0.78519\n",
      "Epoch [783/2000], Iter [1] Loss: 0.4607 Training Accuracy: 0.77037\n",
      "Epoch [784/2000], Iter [1] Loss: 0.4543 Training Accuracy: 0.78519\n",
      "Epoch [785/2000], Iter [1] Loss: 0.4511 Training Accuracy: 0.79259\n",
      "Epoch [786/2000], Iter [1] Loss: 0.4541 Training Accuracy: 0.79259\n",
      "Epoch [787/2000], Iter [1] Loss: 0.4555 Training Accuracy: 0.78519\n",
      "Epoch [788/2000], Iter [1] Loss: 0.4595 Training Accuracy: 0.78519\n",
      "Epoch [789/2000], Iter [1] Loss: 0.4748 Training Accuracy: 0.79259\n",
      "Epoch [790/2000], Iter [1] Loss: 0.4725 Training Accuracy: 0.78519\n",
      "Epoch [791/2000], Iter [1] Loss: 0.4753 Training Accuracy: 0.78519\n",
      "Epoch [792/2000], Iter [1] Loss: 0.4809 Training Accuracy: 0.77778\n",
      "Epoch [793/2000], Iter [1] Loss: 0.4738 Training Accuracy: 0.77037\n",
      "Epoch [794/2000], Iter [1] Loss: 0.4485 Training Accuracy: 0.77778\n",
      "Epoch [795/2000], Iter [1] Loss: 0.4504 Training Accuracy: 0.78519\n",
      "Epoch [796/2000], Iter [1] Loss: 0.4600 Training Accuracy: 0.80000\n",
      "Epoch [797/2000], Iter [1] Loss: 0.4593 Training Accuracy: 0.77037\n",
      "Epoch [798/2000], Iter [1] Loss: 0.4574 Training Accuracy: 0.77778\n",
      "Epoch [799/2000], Iter [1] Loss: 0.4615 Training Accuracy: 0.79259\n",
      "Epoch [800/2000], Iter [1] Loss: 0.4529 Training Accuracy: 0.78519\n",
      "Epoch [801/2000], Iter [1] Loss: 0.4528 Training Accuracy: 0.77778\n",
      "Epoch [802/2000], Iter [1] Loss: 0.4593 Training Accuracy: 0.80000\n",
      "Epoch [803/2000], Iter [1] Loss: 0.4641 Training Accuracy: 0.78519\n",
      "Epoch [804/2000], Iter [1] Loss: 0.4539 Training Accuracy: 0.79259\n",
      "Epoch [805/2000], Iter [1] Loss: 0.4702 Training Accuracy: 0.77778\n",
      "Epoch [806/2000], Iter [1] Loss: 0.4646 Training Accuracy: 0.78519\n",
      "Epoch [807/2000], Iter [1] Loss: 0.4466 Training Accuracy: 0.79259\n",
      "Epoch [808/2000], Iter [1] Loss: 0.4456 Training Accuracy: 0.77778\n",
      "Epoch [809/2000], Iter [1] Loss: 0.4488 Training Accuracy: 0.78519\n",
      "Epoch [810/2000], Iter [1] Loss: 0.4686 Training Accuracy: 0.78519\n",
      "Epoch [811/2000], Iter [1] Loss: 0.4539 Training Accuracy: 0.80000\n",
      "Epoch [812/2000], Iter [1] Loss: 0.4565 Training Accuracy: 0.80000\n",
      "Epoch [813/2000], Iter [1] Loss: 0.4460 Training Accuracy: 0.77778\n",
      "Epoch [814/2000], Iter [1] Loss: 0.4535 Training Accuracy: 0.79259\n",
      "Epoch [815/2000], Iter [1] Loss: 0.4512 Training Accuracy: 0.78519\n",
      "Epoch [816/2000], Iter [1] Loss: 0.4486 Training Accuracy: 0.77778\n",
      "Epoch [817/2000], Iter [1] Loss: 0.4525 Training Accuracy: 0.79259\n",
      "Epoch [818/2000], Iter [1] Loss: 0.4378 Training Accuracy: 0.78519\n",
      "Epoch [819/2000], Iter [1] Loss: 0.4539 Training Accuracy: 0.77778\n",
      "Epoch [820/2000], Iter [1] Loss: 0.4443 Training Accuracy: 0.79259\n",
      "Epoch [821/2000], Iter [1] Loss: 0.4710 Training Accuracy: 0.80000\n",
      "Epoch [822/2000], Iter [1] Loss: 0.4401 Training Accuracy: 0.78519\n",
      "Epoch [823/2000], Iter [1] Loss: 0.4574 Training Accuracy: 0.78519\n",
      "Epoch [824/2000], Iter [1] Loss: 0.4458 Training Accuracy: 0.77778\n",
      "Epoch [825/2000], Iter [1] Loss: 0.4431 Training Accuracy: 0.77778\n",
      "Epoch [826/2000], Iter [1] Loss: 0.4536 Training Accuracy: 0.78519\n",
      "Epoch [827/2000], Iter [1] Loss: 0.4398 Training Accuracy: 0.77778\n",
      "Epoch [828/2000], Iter [1] Loss: 0.4497 Training Accuracy: 0.78519\n",
      "Epoch [829/2000], Iter [1] Loss: 0.4419 Training Accuracy: 0.80000\n",
      "Epoch [830/2000], Iter [1] Loss: 0.4403 Training Accuracy: 0.77778\n",
      "Epoch [831/2000], Iter [1] Loss: 0.4645 Training Accuracy: 0.80000\n",
      "Epoch [832/2000], Iter [1] Loss: 0.4392 Training Accuracy: 0.77037\n",
      "Epoch [833/2000], Iter [1] Loss: 0.4549 Training Accuracy: 0.79259\n",
      "Epoch [834/2000], Iter [1] Loss: 0.4449 Training Accuracy: 0.78519\n",
      "Epoch [835/2000], Iter [1] Loss: 0.4555 Training Accuracy: 0.80000\n",
      "Epoch [836/2000], Iter [1] Loss: 0.4574 Training Accuracy: 0.80741\n",
      "Epoch [837/2000], Iter [1] Loss: 0.4526 Training Accuracy: 0.79259\n",
      "Epoch [838/2000], Iter [1] Loss: 0.4416 Training Accuracy: 0.77778\n",
      "Epoch [839/2000], Iter [1] Loss: 0.4539 Training Accuracy: 0.79259\n",
      "Epoch [840/2000], Iter [1] Loss: 0.4370 Training Accuracy: 0.78519\n",
      "Epoch [841/2000], Iter [1] Loss: 0.4402 Training Accuracy: 0.78519\n",
      "Epoch [842/2000], Iter [1] Loss: 0.4494 Training Accuracy: 0.79259\n",
      "Epoch [843/2000], Iter [1] Loss: 0.4543 Training Accuracy: 0.80000\n",
      "Epoch [844/2000], Iter [1] Loss: 0.4409 Training Accuracy: 0.80000\n",
      "Epoch [845/2000], Iter [1] Loss: 0.4508 Training Accuracy: 0.77037\n",
      "Epoch [846/2000], Iter [1] Loss: 0.4500 Training Accuracy: 0.81481\n",
      "Epoch [847/2000], Iter [1] Loss: 0.4344 Training Accuracy: 0.79259\n",
      "Epoch [848/2000], Iter [1] Loss: 0.4374 Training Accuracy: 0.79259\n",
      "Epoch [849/2000], Iter [1] Loss: 0.4433 Training Accuracy: 0.80000\n",
      "Epoch [850/2000], Iter [1] Loss: 0.4422 Training Accuracy: 0.78519\n",
      "Epoch [851/2000], Iter [1] Loss: 0.4430 Training Accuracy: 0.77037\n",
      "Epoch [852/2000], Iter [1] Loss: 0.4359 Training Accuracy: 0.80000\n",
      "Epoch [853/2000], Iter [1] Loss: 0.4158 Training Accuracy: 0.80000\n",
      "Epoch [854/2000], Iter [1] Loss: 0.4380 Training Accuracy: 0.79259\n",
      "Epoch [855/2000], Iter [1] Loss: 0.4416 Training Accuracy: 0.80000\n",
      "Epoch [856/2000], Iter [1] Loss: 0.4284 Training Accuracy: 0.79259\n",
      "Epoch [857/2000], Iter [1] Loss: 0.4403 Training Accuracy: 0.79259\n",
      "Epoch [858/2000], Iter [1] Loss: 0.4381 Training Accuracy: 0.78519\n",
      "Epoch [859/2000], Iter [1] Loss: 0.4387 Training Accuracy: 0.77778\n",
      "Epoch [860/2000], Iter [1] Loss: 0.4376 Training Accuracy: 0.79259\n",
      "Epoch [861/2000], Iter [1] Loss: 0.4462 Training Accuracy: 0.78519\n",
      "Epoch [862/2000], Iter [1] Loss: 0.4474 Training Accuracy: 0.79259\n",
      "Epoch [863/2000], Iter [1] Loss: 0.4295 Training Accuracy: 0.80000\n",
      "Epoch [864/2000], Iter [1] Loss: 0.4419 Training Accuracy: 0.78519\n",
      "Epoch [865/2000], Iter [1] Loss: 0.4413 Training Accuracy: 0.78519\n",
      "Epoch [866/2000], Iter [1] Loss: 0.4339 Training Accuracy: 0.77778\n",
      "Epoch [867/2000], Iter [1] Loss: 0.4426 Training Accuracy: 0.78519\n",
      "Epoch [868/2000], Iter [1] Loss: 0.4319 Training Accuracy: 0.79259\n",
      "Epoch [869/2000], Iter [1] Loss: 0.4366 Training Accuracy: 0.77778\n",
      "Epoch [870/2000], Iter [1] Loss: 0.4411 Training Accuracy: 0.79259\n",
      "Epoch [871/2000], Iter [1] Loss: 0.4443 Training Accuracy: 0.79259\n",
      "Epoch [872/2000], Iter [1] Loss: 0.4511 Training Accuracy: 0.80000\n",
      "Epoch [873/2000], Iter [1] Loss: 0.4479 Training Accuracy: 0.79259\n",
      "Epoch [874/2000], Iter [1] Loss: 0.4366 Training Accuracy: 0.80741\n",
      "Epoch [875/2000], Iter [1] Loss: 0.4454 Training Accuracy: 0.78519\n",
      "Epoch [876/2000], Iter [1] Loss: 0.4402 Training Accuracy: 0.79259\n",
      "Epoch [877/2000], Iter [1] Loss: 0.4364 Training Accuracy: 0.80741\n",
      "Epoch [878/2000], Iter [1] Loss: 0.4318 Training Accuracy: 0.80000\n",
      "Epoch [879/2000], Iter [1] Loss: 0.4319 Training Accuracy: 0.79259\n",
      "Epoch [880/2000], Iter [1] Loss: 0.4397 Training Accuracy: 0.80000\n",
      "Epoch [881/2000], Iter [1] Loss: 0.4257 Training Accuracy: 0.78519\n",
      "Epoch [882/2000], Iter [1] Loss: 0.4322 Training Accuracy: 0.78519\n",
      "Epoch [883/2000], Iter [1] Loss: 0.4309 Training Accuracy: 0.80741\n",
      "Epoch [884/2000], Iter [1] Loss: 0.4365 Training Accuracy: 0.78519\n",
      "Epoch [885/2000], Iter [1] Loss: 0.4333 Training Accuracy: 0.78519\n",
      "Epoch [886/2000], Iter [1] Loss: 0.4216 Training Accuracy: 0.78519\n",
      "Epoch [887/2000], Iter [1] Loss: 0.4341 Training Accuracy: 0.78519\n",
      "Epoch [888/2000], Iter [1] Loss: 0.4333 Training Accuracy: 0.78519\n",
      "Epoch [889/2000], Iter [1] Loss: 0.4414 Training Accuracy: 0.80000\n",
      "Epoch [890/2000], Iter [1] Loss: 0.4206 Training Accuracy: 0.79259\n",
      "Epoch [891/2000], Iter [1] Loss: 0.4270 Training Accuracy: 0.80000\n",
      "Epoch [892/2000], Iter [1] Loss: 0.4460 Training Accuracy: 0.80000\n",
      "Epoch [893/2000], Iter [1] Loss: 0.4301 Training Accuracy: 0.79259\n",
      "Epoch [894/2000], Iter [1] Loss: 0.4367 Training Accuracy: 0.79259\n",
      "Epoch [895/2000], Iter [1] Loss: 0.4168 Training Accuracy: 0.78519\n",
      "Epoch [896/2000], Iter [1] Loss: 0.4256 Training Accuracy: 0.80000\n",
      "Epoch [897/2000], Iter [1] Loss: 0.4410 Training Accuracy: 0.79259\n",
      "Epoch [898/2000], Iter [1] Loss: 0.4377 Training Accuracy: 0.79259\n",
      "Epoch [899/2000], Iter [1] Loss: 0.4360 Training Accuracy: 0.79259\n",
      "Epoch [900/2000], Iter [1] Loss: 0.4251 Training Accuracy: 0.80000\n",
      "Epoch [901/2000], Iter [1] Loss: 0.4270 Training Accuracy: 0.78519\n",
      "Epoch [902/2000], Iter [1] Loss: 0.4373 Training Accuracy: 0.79259\n",
      "Epoch [903/2000], Iter [1] Loss: 0.4185 Training Accuracy: 0.78519\n",
      "Epoch [904/2000], Iter [1] Loss: 0.4267 Training Accuracy: 0.78519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [905/2000], Iter [1] Loss: 0.4534 Training Accuracy: 0.80000\n",
      "Epoch [906/2000], Iter [1] Loss: 0.4305 Training Accuracy: 0.79259\n",
      "Epoch [907/2000], Iter [1] Loss: 0.4347 Training Accuracy: 0.78519\n",
      "Epoch [908/2000], Iter [1] Loss: 0.4210 Training Accuracy: 0.80000\n",
      "Epoch [909/2000], Iter [1] Loss: 0.4222 Training Accuracy: 0.79259\n",
      "Epoch [910/2000], Iter [1] Loss: 0.4333 Training Accuracy: 0.80000\n",
      "Epoch [911/2000], Iter [1] Loss: 0.4417 Training Accuracy: 0.80741\n",
      "Epoch [912/2000], Iter [1] Loss: 0.4245 Training Accuracy: 0.80741\n",
      "Epoch [913/2000], Iter [1] Loss: 0.4293 Training Accuracy: 0.79259\n",
      "Epoch [914/2000], Iter [1] Loss: 0.4149 Training Accuracy: 0.80000\n",
      "Epoch [915/2000], Iter [1] Loss: 0.4223 Training Accuracy: 0.77778\n",
      "Epoch [916/2000], Iter [1] Loss: 0.4155 Training Accuracy: 0.80741\n",
      "Epoch [917/2000], Iter [1] Loss: 0.4089 Training Accuracy: 0.79259\n",
      "Epoch [918/2000], Iter [1] Loss: 0.4300 Training Accuracy: 0.78519\n",
      "Epoch [919/2000], Iter [1] Loss: 0.4166 Training Accuracy: 0.77778\n",
      "Epoch [920/2000], Iter [1] Loss: 0.4208 Training Accuracy: 0.80000\n",
      "Epoch [921/2000], Iter [1] Loss: 0.4409 Training Accuracy: 0.78519\n",
      "Epoch [922/2000], Iter [1] Loss: 0.4440 Training Accuracy: 0.80741\n",
      "Epoch [923/2000], Iter [1] Loss: 0.4347 Training Accuracy: 0.79259\n",
      "Epoch [924/2000], Iter [1] Loss: 0.4295 Training Accuracy: 0.80000\n",
      "Epoch [925/2000], Iter [1] Loss: 0.4350 Training Accuracy: 0.80741\n",
      "Epoch [926/2000], Iter [1] Loss: 0.4216 Training Accuracy: 0.79259\n",
      "Epoch [927/2000], Iter [1] Loss: 0.4185 Training Accuracy: 0.77778\n",
      "Epoch [928/2000], Iter [1] Loss: 0.4162 Training Accuracy: 0.80000\n",
      "Epoch [929/2000], Iter [1] Loss: 0.4278 Training Accuracy: 0.78519\n",
      "Epoch [930/2000], Iter [1] Loss: 0.4233 Training Accuracy: 0.79259\n",
      "Epoch [931/2000], Iter [1] Loss: 0.4290 Training Accuracy: 0.77778\n",
      "Epoch [932/2000], Iter [1] Loss: 0.4128 Training Accuracy: 0.80741\n",
      "Epoch [933/2000], Iter [1] Loss: 0.4143 Training Accuracy: 0.78519\n",
      "Epoch [934/2000], Iter [1] Loss: 0.4155 Training Accuracy: 0.77778\n",
      "Epoch [935/2000], Iter [1] Loss: 0.4150 Training Accuracy: 0.79259\n",
      "Epoch [936/2000], Iter [1] Loss: 0.4320 Training Accuracy: 0.79259\n",
      "Epoch [937/2000], Iter [1] Loss: 0.4099 Training Accuracy: 0.80000\n",
      "Epoch [938/2000], Iter [1] Loss: 0.4200 Training Accuracy: 0.79259\n",
      "Epoch [939/2000], Iter [1] Loss: 0.4213 Training Accuracy: 0.80000\n",
      "Epoch [940/2000], Iter [1] Loss: 0.4193 Training Accuracy: 0.79259\n",
      "Epoch [941/2000], Iter [1] Loss: 0.4163 Training Accuracy: 0.79259\n",
      "Epoch [942/2000], Iter [1] Loss: 0.4249 Training Accuracy: 0.80741\n",
      "Epoch [943/2000], Iter [1] Loss: 0.4218 Training Accuracy: 0.80000\n",
      "Epoch [944/2000], Iter [1] Loss: 0.4170 Training Accuracy: 0.79259\n",
      "Epoch [945/2000], Iter [1] Loss: 0.4237 Training Accuracy: 0.79259\n",
      "Epoch [946/2000], Iter [1] Loss: 0.4166 Training Accuracy: 0.77778\n",
      "Epoch [947/2000], Iter [1] Loss: 0.4165 Training Accuracy: 0.79259\n",
      "Epoch [948/2000], Iter [1] Loss: 0.4127 Training Accuracy: 0.78519\n",
      "Epoch [949/2000], Iter [1] Loss: 0.4270 Training Accuracy: 0.79259\n",
      "Epoch [950/2000], Iter [1] Loss: 0.4197 Training Accuracy: 0.79259\n",
      "Epoch [951/2000], Iter [1] Loss: 0.4188 Training Accuracy: 0.80000\n",
      "Epoch [952/2000], Iter [1] Loss: 0.4313 Training Accuracy: 0.79259\n",
      "Epoch [953/2000], Iter [1] Loss: 0.4134 Training Accuracy: 0.79259\n",
      "Epoch [954/2000], Iter [1] Loss: 0.4176 Training Accuracy: 0.79259\n",
      "Epoch [955/2000], Iter [1] Loss: 0.4260 Training Accuracy: 0.80000\n",
      "Epoch [956/2000], Iter [1] Loss: 0.4187 Training Accuracy: 0.79259\n",
      "Epoch [957/2000], Iter [1] Loss: 0.4223 Training Accuracy: 0.80741\n",
      "Epoch [958/2000], Iter [1] Loss: 0.4195 Training Accuracy: 0.81481\n",
      "Epoch [959/2000], Iter [1] Loss: 0.4238 Training Accuracy: 0.78519\n",
      "Epoch [960/2000], Iter [1] Loss: 0.4294 Training Accuracy: 0.79259\n",
      "Epoch [961/2000], Iter [1] Loss: 0.4205 Training Accuracy: 0.80741\n",
      "Epoch [962/2000], Iter [1] Loss: 0.4232 Training Accuracy: 0.79259\n",
      "Epoch [963/2000], Iter [1] Loss: 0.4133 Training Accuracy: 0.78519\n",
      "Epoch [964/2000], Iter [1] Loss: 0.4231 Training Accuracy: 0.79259\n",
      "Epoch [965/2000], Iter [1] Loss: 0.4247 Training Accuracy: 0.78519\n",
      "Epoch [966/2000], Iter [1] Loss: 0.4109 Training Accuracy: 0.78519\n",
      "Epoch [967/2000], Iter [1] Loss: 0.4173 Training Accuracy: 0.79259\n",
      "Epoch [968/2000], Iter [1] Loss: 0.4109 Training Accuracy: 0.80000\n",
      "Epoch [969/2000], Iter [1] Loss: 0.4187 Training Accuracy: 0.78519\n",
      "Epoch [970/2000], Iter [1] Loss: 0.4128 Training Accuracy: 0.80741\n",
      "Epoch [971/2000], Iter [1] Loss: 0.4260 Training Accuracy: 0.77778\n",
      "Epoch [972/2000], Iter [1] Loss: 0.4089 Training Accuracy: 0.79259\n",
      "Epoch [973/2000], Iter [1] Loss: 0.4305 Training Accuracy: 0.79259\n",
      "Epoch [974/2000], Iter [1] Loss: 0.4216 Training Accuracy: 0.80000\n",
      "Epoch [975/2000], Iter [1] Loss: 0.4061 Training Accuracy: 0.79259\n",
      "Epoch [976/2000], Iter [1] Loss: 0.4135 Training Accuracy: 0.80000\n",
      "Epoch [977/2000], Iter [1] Loss: 0.4080 Training Accuracy: 0.80000\n",
      "Epoch [978/2000], Iter [1] Loss: 0.4140 Training Accuracy: 0.80741\n",
      "Epoch [979/2000], Iter [1] Loss: 0.4311 Training Accuracy: 0.80741\n",
      "Epoch [980/2000], Iter [1] Loss: 0.4357 Training Accuracy: 0.80000\n",
      "Epoch [981/2000], Iter [1] Loss: 0.4138 Training Accuracy: 0.79259\n",
      "Epoch [982/2000], Iter [1] Loss: 0.4185 Training Accuracy: 0.79259\n",
      "Epoch [983/2000], Iter [1] Loss: 0.4068 Training Accuracy: 0.80000\n",
      "Epoch [984/2000], Iter [1] Loss: 0.4118 Training Accuracy: 0.77778\n",
      "Epoch [985/2000], Iter [1] Loss: 0.4021 Training Accuracy: 0.79259\n",
      "Epoch [986/2000], Iter [1] Loss: 0.4174 Training Accuracy: 0.79259\n",
      "Epoch [987/2000], Iter [1] Loss: 0.4154 Training Accuracy: 0.79259\n",
      "Epoch [988/2000], Iter [1] Loss: 0.4141 Training Accuracy: 0.79259\n",
      "Epoch [989/2000], Iter [1] Loss: 0.4215 Training Accuracy: 0.78519\n",
      "Epoch [990/2000], Iter [1] Loss: 0.4120 Training Accuracy: 0.78519\n",
      "Epoch [991/2000], Iter [1] Loss: 0.4066 Training Accuracy: 0.79259\n",
      "Epoch [992/2000], Iter [1] Loss: 0.4131 Training Accuracy: 0.80741\n",
      "Epoch [993/2000], Iter [1] Loss: 0.4105 Training Accuracy: 0.80741\n",
      "Epoch [994/2000], Iter [1] Loss: 0.4036 Training Accuracy: 0.78519\n",
      "Epoch [995/2000], Iter [1] Loss: 0.4098 Training Accuracy: 0.80000\n",
      "Epoch [996/2000], Iter [1] Loss: 0.4080 Training Accuracy: 0.79259\n",
      "Epoch [997/2000], Iter [1] Loss: 0.4054 Training Accuracy: 0.79259\n",
      "Epoch [998/2000], Iter [1] Loss: 0.4083 Training Accuracy: 0.80000\n",
      "Epoch [999/2000], Iter [1] Loss: 0.4184 Training Accuracy: 0.81481\n",
      "Epoch [1000/2000], Iter [1] Loss: 0.4124 Training Accuracy: 0.80000\n",
      "Epoch [1001/2000], Iter [1] Loss: 0.4097 Training Accuracy: 0.80000\n",
      "Epoch [1002/2000], Iter [1] Loss: 0.4035 Training Accuracy: 0.79259\n",
      "Epoch [1003/2000], Iter [1] Loss: 0.4196 Training Accuracy: 0.79259\n",
      "Epoch [1004/2000], Iter [1] Loss: 0.3976 Training Accuracy: 0.80000\n",
      "Epoch [1005/2000], Iter [1] Loss: 0.4124 Training Accuracy: 0.80000\n",
      "Epoch [1006/2000], Iter [1] Loss: 0.4148 Training Accuracy: 0.78519\n",
      "Epoch [1007/2000], Iter [1] Loss: 0.4112 Training Accuracy: 0.80741\n",
      "Epoch [1008/2000], Iter [1] Loss: 0.4198 Training Accuracy: 0.79259\n",
      "Epoch [1009/2000], Iter [1] Loss: 0.4056 Training Accuracy: 0.81481\n",
      "Epoch [1010/2000], Iter [1] Loss: 0.3972 Training Accuracy: 0.80741\n",
      "Epoch [1011/2000], Iter [1] Loss: 0.3976 Training Accuracy: 0.79259\n",
      "Epoch [1012/2000], Iter [1] Loss: 0.4063 Training Accuracy: 0.79259\n",
      "Epoch [1013/2000], Iter [1] Loss: 0.4022 Training Accuracy: 0.80000\n",
      "Epoch [1014/2000], Iter [1] Loss: 0.4055 Training Accuracy: 0.80000\n",
      "Epoch [1015/2000], Iter [1] Loss: 0.4030 Training Accuracy: 0.80741\n",
      "Epoch [1016/2000], Iter [1] Loss: 0.4111 Training Accuracy: 0.80000\n",
      "Epoch [1017/2000], Iter [1] Loss: 0.4002 Training Accuracy: 0.79259\n",
      "Epoch [1018/2000], Iter [1] Loss: 0.4028 Training Accuracy: 0.77778\n",
      "Epoch [1019/2000], Iter [1] Loss: 0.4000 Training Accuracy: 0.80000\n",
      "Epoch [1020/2000], Iter [1] Loss: 0.4023 Training Accuracy: 0.78519\n",
      "Epoch [1021/2000], Iter [1] Loss: 0.4192 Training Accuracy: 0.80741\n",
      "Epoch [1022/2000], Iter [1] Loss: 0.3954 Training Accuracy: 0.80741\n",
      "Epoch [1023/2000], Iter [1] Loss: 0.4136 Training Accuracy: 0.80741\n",
      "Epoch [1024/2000], Iter [1] Loss: 0.4016 Training Accuracy: 0.80741\n",
      "Epoch [1025/2000], Iter [1] Loss: 0.4039 Training Accuracy: 0.80000\n",
      "Epoch [1026/2000], Iter [1] Loss: 0.4044 Training Accuracy: 0.77778\n",
      "Epoch [1027/2000], Iter [1] Loss: 0.3897 Training Accuracy: 0.80741\n",
      "Epoch [1028/2000], Iter [1] Loss: 0.4110 Training Accuracy: 0.79259\n",
      "Epoch [1029/2000], Iter [1] Loss: 0.4016 Training Accuracy: 0.79259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1030/2000], Iter [1] Loss: 0.4093 Training Accuracy: 0.80000\n",
      "Epoch [1031/2000], Iter [1] Loss: 0.4038 Training Accuracy: 0.80000\n",
      "Epoch [1032/2000], Iter [1] Loss: 0.4078 Training Accuracy: 0.79259\n",
      "Epoch [1033/2000], Iter [1] Loss: 0.4140 Training Accuracy: 0.80741\n",
      "Epoch [1034/2000], Iter [1] Loss: 0.4017 Training Accuracy: 0.81481\n",
      "Epoch [1035/2000], Iter [1] Loss: 0.4059 Training Accuracy: 0.79259\n",
      "Epoch [1036/2000], Iter [1] Loss: 0.3996 Training Accuracy: 0.78519\n",
      "Epoch [1037/2000], Iter [1] Loss: 0.3869 Training Accuracy: 0.80000\n",
      "Epoch [1038/2000], Iter [1] Loss: 0.3982 Training Accuracy: 0.80000\n",
      "Epoch [1039/2000], Iter [1] Loss: 0.3969 Training Accuracy: 0.80000\n",
      "Epoch [1040/2000], Iter [1] Loss: 0.4047 Training Accuracy: 0.80000\n",
      "Epoch [1041/2000], Iter [1] Loss: 0.4041 Training Accuracy: 0.80741\n",
      "Epoch [1042/2000], Iter [1] Loss: 0.4024 Training Accuracy: 0.79259\n",
      "Epoch [1043/2000], Iter [1] Loss: 0.4049 Training Accuracy: 0.80000\n",
      "Epoch [1044/2000], Iter [1] Loss: 0.3896 Training Accuracy: 0.78519\n",
      "Epoch [1045/2000], Iter [1] Loss: 0.4019 Training Accuracy: 0.80741\n",
      "Epoch [1046/2000], Iter [1] Loss: 0.3858 Training Accuracy: 0.80000\n",
      "Epoch [1047/2000], Iter [1] Loss: 0.4016 Training Accuracy: 0.77037\n",
      "Epoch [1048/2000], Iter [1] Loss: 0.4035 Training Accuracy: 0.79259\n",
      "Epoch [1049/2000], Iter [1] Loss: 0.4044 Training Accuracy: 0.79259\n",
      "Epoch [1050/2000], Iter [1] Loss: 0.3988 Training Accuracy: 0.80000\n",
      "Epoch [1051/2000], Iter [1] Loss: 0.3931 Training Accuracy: 0.79259\n",
      "Epoch [1052/2000], Iter [1] Loss: 0.3920 Training Accuracy: 0.80741\n",
      "Epoch [1053/2000], Iter [1] Loss: 0.3864 Training Accuracy: 0.81481\n",
      "Epoch [1054/2000], Iter [1] Loss: 0.3812 Training Accuracy: 0.80741\n",
      "Epoch [1055/2000], Iter [1] Loss: 0.4093 Training Accuracy: 0.80000\n",
      "Epoch [1056/2000], Iter [1] Loss: 0.4079 Training Accuracy: 0.78519\n",
      "Epoch [1057/2000], Iter [1] Loss: 0.3978 Training Accuracy: 0.78519\n",
      "Epoch [1058/2000], Iter [1] Loss: 0.3853 Training Accuracy: 0.78519\n",
      "Epoch [1059/2000], Iter [1] Loss: 0.4077 Training Accuracy: 0.77037\n",
      "Epoch [1060/2000], Iter [1] Loss: 0.3906 Training Accuracy: 0.79259\n",
      "Epoch [1061/2000], Iter [1] Loss: 0.4025 Training Accuracy: 0.80741\n",
      "Epoch [1062/2000], Iter [1] Loss: 0.3974 Training Accuracy: 0.81481\n",
      "Epoch [1063/2000], Iter [1] Loss: 0.4056 Training Accuracy: 0.80000\n",
      "Epoch [1064/2000], Iter [1] Loss: 0.4023 Training Accuracy: 0.81481\n",
      "Epoch [1065/2000], Iter [1] Loss: 0.3959 Training Accuracy: 0.77778\n",
      "Epoch [1066/2000], Iter [1] Loss: 0.3909 Training Accuracy: 0.79259\n",
      "Epoch [1067/2000], Iter [1] Loss: 0.3974 Training Accuracy: 0.79259\n",
      "Epoch [1068/2000], Iter [1] Loss: 0.4015 Training Accuracy: 0.78519\n",
      "Epoch [1069/2000], Iter [1] Loss: 0.3877 Training Accuracy: 0.80741\n",
      "Epoch [1070/2000], Iter [1] Loss: 0.3952 Training Accuracy: 0.80741\n",
      "Epoch [1071/2000], Iter [1] Loss: 0.4085 Training Accuracy: 0.80000\n",
      "Epoch [1072/2000], Iter [1] Loss: 0.3997 Training Accuracy: 0.79259\n",
      "Epoch [1073/2000], Iter [1] Loss: 0.3925 Training Accuracy: 0.81481\n",
      "Epoch [1074/2000], Iter [1] Loss: 0.3900 Training Accuracy: 0.79259\n",
      "Epoch [1075/2000], Iter [1] Loss: 0.4067 Training Accuracy: 0.79259\n",
      "Epoch [1076/2000], Iter [1] Loss: 0.3972 Training Accuracy: 0.81481\n",
      "Epoch [1077/2000], Iter [1] Loss: 0.3994 Training Accuracy: 0.81481\n",
      "Epoch [1078/2000], Iter [1] Loss: 0.3886 Training Accuracy: 0.78519\n",
      "Epoch [1079/2000], Iter [1] Loss: 0.3941 Training Accuracy: 0.79259\n",
      "Epoch [1080/2000], Iter [1] Loss: 0.3844 Training Accuracy: 0.79259\n",
      "Epoch [1081/2000], Iter [1] Loss: 0.3944 Training Accuracy: 0.79259\n",
      "Epoch [1082/2000], Iter [1] Loss: 0.4043 Training Accuracy: 0.80000\n",
      "Epoch [1083/2000], Iter [1] Loss: 0.3830 Training Accuracy: 0.79259\n",
      "Epoch [1084/2000], Iter [1] Loss: 0.4105 Training Accuracy: 0.80741\n",
      "Epoch [1085/2000], Iter [1] Loss: 0.3871 Training Accuracy: 0.77778\n",
      "Epoch [1086/2000], Iter [1] Loss: 0.3925 Training Accuracy: 0.81481\n",
      "Epoch [1087/2000], Iter [1] Loss: 0.3811 Training Accuracy: 0.80000\n",
      "Epoch [1088/2000], Iter [1] Loss: 0.3998 Training Accuracy: 0.80000\n",
      "Epoch [1089/2000], Iter [1] Loss: 0.3925 Training Accuracy: 0.78519\n",
      "Epoch [1090/2000], Iter [1] Loss: 0.3868 Training Accuracy: 0.79259\n",
      "Epoch [1091/2000], Iter [1] Loss: 0.3906 Training Accuracy: 0.80741\n",
      "Epoch [1092/2000], Iter [1] Loss: 0.3946 Training Accuracy: 0.80000\n",
      "Epoch [1093/2000], Iter [1] Loss: 0.3888 Training Accuracy: 0.78519\n",
      "Epoch [1094/2000], Iter [1] Loss: 0.3897 Training Accuracy: 0.80000\n",
      "Epoch [1095/2000], Iter [1] Loss: 0.3965 Training Accuracy: 0.80741\n",
      "Epoch [1096/2000], Iter [1] Loss: 0.3998 Training Accuracy: 0.79259\n",
      "Epoch [1097/2000], Iter [1] Loss: 0.3924 Training Accuracy: 0.79259\n",
      "Epoch [1098/2000], Iter [1] Loss: 0.3834 Training Accuracy: 0.80000\n",
      "Epoch [1099/2000], Iter [1] Loss: 0.4124 Training Accuracy: 0.79259\n",
      "Epoch [1100/2000], Iter [1] Loss: 0.3923 Training Accuracy: 0.80741\n",
      "Epoch [1101/2000], Iter [1] Loss: 0.3885 Training Accuracy: 0.80741\n",
      "Epoch [1102/2000], Iter [1] Loss: 0.3823 Training Accuracy: 0.79259\n",
      "Epoch [1103/2000], Iter [1] Loss: 0.3899 Training Accuracy: 0.79259\n",
      "Epoch [1104/2000], Iter [1] Loss: 0.3923 Training Accuracy: 0.78519\n",
      "Epoch [1105/2000], Iter [1] Loss: 0.3939 Training Accuracy: 0.80741\n",
      "Epoch [1106/2000], Iter [1] Loss: 0.4080 Training Accuracy: 0.80000\n",
      "Epoch [1107/2000], Iter [1] Loss: 0.3862 Training Accuracy: 0.80741\n",
      "Epoch [1108/2000], Iter [1] Loss: 0.3879 Training Accuracy: 0.78519\n",
      "Epoch [1109/2000], Iter [1] Loss: 0.3870 Training Accuracy: 0.81481\n",
      "Epoch [1110/2000], Iter [1] Loss: 0.3824 Training Accuracy: 0.82222\n",
      "Epoch [1111/2000], Iter [1] Loss: 0.4038 Training Accuracy: 0.80741\n",
      "Epoch [1112/2000], Iter [1] Loss: 0.4019 Training Accuracy: 0.80000\n",
      "Epoch [1113/2000], Iter [1] Loss: 0.3710 Training Accuracy: 0.80741\n",
      "Epoch [1114/2000], Iter [1] Loss: 0.3862 Training Accuracy: 0.80741\n",
      "Epoch [1115/2000], Iter [1] Loss: 0.3947 Training Accuracy: 0.80741\n",
      "Epoch [1116/2000], Iter [1] Loss: 0.3839 Training Accuracy: 0.80000\n",
      "Epoch [1117/2000], Iter [1] Loss: 0.3997 Training Accuracy: 0.80000\n",
      "Epoch [1118/2000], Iter [1] Loss: 0.3948 Training Accuracy: 0.80000\n",
      "Epoch [1119/2000], Iter [1] Loss: 0.4016 Training Accuracy: 0.80000\n",
      "Epoch [1120/2000], Iter [1] Loss: 0.3826 Training Accuracy: 0.79259\n",
      "Epoch [1121/2000], Iter [1] Loss: 0.3942 Training Accuracy: 0.80741\n",
      "Epoch [1122/2000], Iter [1] Loss: 0.3849 Training Accuracy: 0.80741\n",
      "Epoch [1123/2000], Iter [1] Loss: 0.3968 Training Accuracy: 0.80000\n",
      "Epoch [1124/2000], Iter [1] Loss: 0.3782 Training Accuracy: 0.80741\n",
      "Epoch [1125/2000], Iter [1] Loss: 0.3868 Training Accuracy: 0.80000\n",
      "Epoch [1126/2000], Iter [1] Loss: 0.3914 Training Accuracy: 0.78519\n",
      "Epoch [1127/2000], Iter [1] Loss: 0.3994 Training Accuracy: 0.79259\n",
      "Epoch [1128/2000], Iter [1] Loss: 0.4022 Training Accuracy: 0.80741\n",
      "Epoch [1129/2000], Iter [1] Loss: 0.3714 Training Accuracy: 0.80000\n",
      "Epoch [1130/2000], Iter [1] Loss: 0.3911 Training Accuracy: 0.81481\n",
      "Epoch [1131/2000], Iter [1] Loss: 0.3914 Training Accuracy: 0.82222\n",
      "Epoch [1132/2000], Iter [1] Loss: 0.3896 Training Accuracy: 0.79259\n",
      "Epoch [1133/2000], Iter [1] Loss: 0.3856 Training Accuracy: 0.82222\n",
      "Epoch [1134/2000], Iter [1] Loss: 0.3873 Training Accuracy: 0.77778\n",
      "Epoch [1135/2000], Iter [1] Loss: 0.3800 Training Accuracy: 0.80741\n",
      "Epoch [1136/2000], Iter [1] Loss: 0.3864 Training Accuracy: 0.79259\n",
      "Epoch [1137/2000], Iter [1] Loss: 0.3928 Training Accuracy: 0.80000\n",
      "Epoch [1138/2000], Iter [1] Loss: 0.3895 Training Accuracy: 0.79259\n",
      "Epoch [1139/2000], Iter [1] Loss: 0.3722 Training Accuracy: 0.79259\n",
      "Epoch [1140/2000], Iter [1] Loss: 0.3868 Training Accuracy: 0.80000\n",
      "Epoch [1141/2000], Iter [1] Loss: 0.3779 Training Accuracy: 0.80000\n",
      "Epoch [1142/2000], Iter [1] Loss: 0.3901 Training Accuracy: 0.81481\n",
      "Epoch [1143/2000], Iter [1] Loss: 0.3737 Training Accuracy: 0.80000\n",
      "Epoch [1144/2000], Iter [1] Loss: 0.3880 Training Accuracy: 0.79259\n",
      "Epoch [1145/2000], Iter [1] Loss: 0.3772 Training Accuracy: 0.82222\n",
      "Epoch [1146/2000], Iter [1] Loss: 0.3700 Training Accuracy: 0.80000\n",
      "Epoch [1147/2000], Iter [1] Loss: 0.3762 Training Accuracy: 0.80000\n",
      "Epoch [1148/2000], Iter [1] Loss: 0.3825 Training Accuracy: 0.80000\n",
      "Epoch [1149/2000], Iter [1] Loss: 0.3850 Training Accuracy: 0.79259\n",
      "Epoch [1150/2000], Iter [1] Loss: 0.3840 Training Accuracy: 0.80000\n",
      "Epoch [1151/2000], Iter [1] Loss: 0.3800 Training Accuracy: 0.78519\n",
      "Epoch [1152/2000], Iter [1] Loss: 0.3760 Training Accuracy: 0.80000\n",
      "Epoch [1153/2000], Iter [1] Loss: 0.3886 Training Accuracy: 0.79259\n",
      "Epoch [1154/2000], Iter [1] Loss: 0.3601 Training Accuracy: 0.79259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1155/2000], Iter [1] Loss: 0.3919 Training Accuracy: 0.80000\n",
      "Epoch [1156/2000], Iter [1] Loss: 0.3841 Training Accuracy: 0.82222\n",
      "Epoch [1157/2000], Iter [1] Loss: 0.3825 Training Accuracy: 0.80741\n",
      "Epoch [1158/2000], Iter [1] Loss: 0.3809 Training Accuracy: 0.80000\n",
      "Epoch [1159/2000], Iter [1] Loss: 0.3816 Training Accuracy: 0.80000\n",
      "Epoch [1160/2000], Iter [1] Loss: 0.4037 Training Accuracy: 0.80741\n",
      "Epoch [1161/2000], Iter [1] Loss: 0.3896 Training Accuracy: 0.80000\n",
      "Epoch [1162/2000], Iter [1] Loss: 0.3723 Training Accuracy: 0.80741\n",
      "Epoch [1163/2000], Iter [1] Loss: 0.3834 Training Accuracy: 0.79259\n",
      "Epoch [1164/2000], Iter [1] Loss: 0.3824 Training Accuracy: 0.81481\n",
      "Epoch [1165/2000], Iter [1] Loss: 0.3747 Training Accuracy: 0.79259\n",
      "Epoch [1166/2000], Iter [1] Loss: 0.3841 Training Accuracy: 0.81481\n",
      "Epoch [1167/2000], Iter [1] Loss: 0.3806 Training Accuracy: 0.81481\n",
      "Epoch [1168/2000], Iter [1] Loss: 0.3755 Training Accuracy: 0.80741\n",
      "Epoch [1169/2000], Iter [1] Loss: 0.3902 Training Accuracy: 0.80741\n",
      "Epoch [1170/2000], Iter [1] Loss: 0.3868 Training Accuracy: 0.80000\n",
      "Epoch [1171/2000], Iter [1] Loss: 0.3902 Training Accuracy: 0.81481\n",
      "Epoch [1172/2000], Iter [1] Loss: 0.3742 Training Accuracy: 0.80000\n",
      "Epoch [1173/2000], Iter [1] Loss: 0.3796 Training Accuracy: 0.82963\n",
      "Epoch [1174/2000], Iter [1] Loss: 0.3832 Training Accuracy: 0.81481\n",
      "Epoch [1175/2000], Iter [1] Loss: 0.3811 Training Accuracy: 0.79259\n",
      "Epoch [1176/2000], Iter [1] Loss: 0.3724 Training Accuracy: 0.80741\n",
      "Epoch [1177/2000], Iter [1] Loss: 0.3674 Training Accuracy: 0.81481\n",
      "Epoch [1178/2000], Iter [1] Loss: 0.3886 Training Accuracy: 0.80741\n",
      "Epoch [1179/2000], Iter [1] Loss: 0.3766 Training Accuracy: 0.80741\n",
      "Epoch [1180/2000], Iter [1] Loss: 0.3669 Training Accuracy: 0.81481\n",
      "Epoch [1181/2000], Iter [1] Loss: 0.3810 Training Accuracy: 0.79259\n",
      "Epoch [1182/2000], Iter [1] Loss: 0.3849 Training Accuracy: 0.79259\n",
      "Epoch [1183/2000], Iter [1] Loss: 0.3815 Training Accuracy: 0.79259\n",
      "Epoch [1184/2000], Iter [1] Loss: 0.3843 Training Accuracy: 0.80000\n",
      "Epoch [1185/2000], Iter [1] Loss: 0.3865 Training Accuracy: 0.80000\n",
      "Epoch [1186/2000], Iter [1] Loss: 0.3773 Training Accuracy: 0.80741\n",
      "Epoch [1187/2000], Iter [1] Loss: 0.3688 Training Accuracy: 0.81481\n",
      "Epoch [1188/2000], Iter [1] Loss: 0.3655 Training Accuracy: 0.80741\n",
      "Epoch [1189/2000], Iter [1] Loss: 0.4041 Training Accuracy: 0.81481\n",
      "Epoch [1190/2000], Iter [1] Loss: 0.3743 Training Accuracy: 0.80000\n",
      "Epoch [1191/2000], Iter [1] Loss: 0.3709 Training Accuracy: 0.80000\n",
      "Epoch [1192/2000], Iter [1] Loss: 0.3855 Training Accuracy: 0.80741\n",
      "Epoch [1193/2000], Iter [1] Loss: 0.3758 Training Accuracy: 0.80741\n",
      "Epoch [1194/2000], Iter [1] Loss: 0.3733 Training Accuracy: 0.82222\n",
      "Epoch [1195/2000], Iter [1] Loss: 0.3708 Training Accuracy: 0.78519\n",
      "Epoch [1196/2000], Iter [1] Loss: 0.3629 Training Accuracy: 0.80000\n",
      "Epoch [1197/2000], Iter [1] Loss: 0.3783 Training Accuracy: 0.79259\n",
      "Epoch [1198/2000], Iter [1] Loss: 0.3805 Training Accuracy: 0.80000\n",
      "Epoch [1199/2000], Iter [1] Loss: 0.3775 Training Accuracy: 0.81481\n",
      "Epoch [1200/2000], Iter [1] Loss: 0.3885 Training Accuracy: 0.79259\n",
      "Epoch [1201/2000], Iter [1] Loss: 0.3776 Training Accuracy: 0.80741\n",
      "Epoch [1202/2000], Iter [1] Loss: 0.3756 Training Accuracy: 0.82222\n",
      "Epoch [1203/2000], Iter [1] Loss: 0.3740 Training Accuracy: 0.79259\n",
      "Epoch [1204/2000], Iter [1] Loss: 0.3672 Training Accuracy: 0.80000\n",
      "Epoch [1205/2000], Iter [1] Loss: 0.3865 Training Accuracy: 0.80000\n",
      "Epoch [1206/2000], Iter [1] Loss: 0.3765 Training Accuracy: 0.80741\n",
      "Epoch [1207/2000], Iter [1] Loss: 0.3818 Training Accuracy: 0.79259\n",
      "Epoch [1208/2000], Iter [1] Loss: 0.3734 Training Accuracy: 0.82963\n",
      "Epoch [1209/2000], Iter [1] Loss: 0.3778 Training Accuracy: 0.79259\n",
      "Epoch [1210/2000], Iter [1] Loss: 0.3661 Training Accuracy: 0.80741\n",
      "Epoch [1211/2000], Iter [1] Loss: 0.3758 Training Accuracy: 0.82222\n",
      "Epoch [1212/2000], Iter [1] Loss: 0.3756 Training Accuracy: 0.80741\n",
      "Epoch [1213/2000], Iter [1] Loss: 0.3850 Training Accuracy: 0.78519\n",
      "Epoch [1214/2000], Iter [1] Loss: 0.3706 Training Accuracy: 0.80000\n",
      "Epoch [1215/2000], Iter [1] Loss: 0.3604 Training Accuracy: 0.80000\n",
      "Epoch [1216/2000], Iter [1] Loss: 0.3687 Training Accuracy: 0.79259\n",
      "Epoch [1217/2000], Iter [1] Loss: 0.3544 Training Accuracy: 0.78519\n",
      "Epoch [1218/2000], Iter [1] Loss: 0.3776 Training Accuracy: 0.80000\n",
      "Epoch [1219/2000], Iter [1] Loss: 0.3651 Training Accuracy: 0.80741\n",
      "Epoch [1220/2000], Iter [1] Loss: 0.3644 Training Accuracy: 0.80000\n",
      "Epoch [1221/2000], Iter [1] Loss: 0.3787 Training Accuracy: 0.81481\n",
      "Epoch [1222/2000], Iter [1] Loss: 0.3790 Training Accuracy: 0.80741\n",
      "Epoch [1223/2000], Iter [1] Loss: 0.3684 Training Accuracy: 0.80000\n",
      "Epoch [1224/2000], Iter [1] Loss: 0.3665 Training Accuracy: 0.80741\n",
      "Epoch [1225/2000], Iter [1] Loss: 0.3726 Training Accuracy: 0.82963\n",
      "Epoch [1226/2000], Iter [1] Loss: 0.3733 Training Accuracy: 0.80000\n",
      "Epoch [1227/2000], Iter [1] Loss: 0.3751 Training Accuracy: 0.80000\n",
      "Epoch [1228/2000], Iter [1] Loss: 0.3700 Training Accuracy: 0.80000\n",
      "Epoch [1229/2000], Iter [1] Loss: 0.3765 Training Accuracy: 0.82222\n",
      "Epoch [1230/2000], Iter [1] Loss: 0.3665 Training Accuracy: 0.80741\n",
      "Epoch [1231/2000], Iter [1] Loss: 0.3839 Training Accuracy: 0.80741\n",
      "Epoch [1232/2000], Iter [1] Loss: 0.3738 Training Accuracy: 0.82222\n",
      "Epoch [1233/2000], Iter [1] Loss: 0.3769 Training Accuracy: 0.82222\n",
      "Epoch [1234/2000], Iter [1] Loss: 0.3756 Training Accuracy: 0.79259\n",
      "Epoch [1235/2000], Iter [1] Loss: 0.3637 Training Accuracy: 0.78519\n",
      "Epoch [1236/2000], Iter [1] Loss: 0.3792 Training Accuracy: 0.81481\n",
      "Epoch [1237/2000], Iter [1] Loss: 0.3804 Training Accuracy: 0.78519\n",
      "Epoch [1238/2000], Iter [1] Loss: 0.3825 Training Accuracy: 0.80741\n",
      "Epoch [1239/2000], Iter [1] Loss: 0.3822 Training Accuracy: 0.79259\n",
      "Epoch [1240/2000], Iter [1] Loss: 0.3792 Training Accuracy: 0.80741\n",
      "Epoch [1241/2000], Iter [1] Loss: 0.3689 Training Accuracy: 0.79259\n",
      "Epoch [1242/2000], Iter [1] Loss: 0.3565 Training Accuracy: 0.82222\n",
      "Epoch [1243/2000], Iter [1] Loss: 0.3800 Training Accuracy: 0.80000\n",
      "Epoch [1244/2000], Iter [1] Loss: 0.3792 Training Accuracy: 0.80741\n",
      "Epoch [1245/2000], Iter [1] Loss: 0.3740 Training Accuracy: 0.82222\n",
      "Epoch [1246/2000], Iter [1] Loss: 0.3913 Training Accuracy: 0.80741\n",
      "Epoch [1247/2000], Iter [1] Loss: 0.3802 Training Accuracy: 0.79259\n",
      "Epoch [1248/2000], Iter [1] Loss: 0.3663 Training Accuracy: 0.81481\n",
      "Epoch [1249/2000], Iter [1] Loss: 0.3673 Training Accuracy: 0.80000\n",
      "Epoch [1250/2000], Iter [1] Loss: 0.3692 Training Accuracy: 0.78519\n",
      "Epoch [1251/2000], Iter [1] Loss: 0.3491 Training Accuracy: 0.80000\n",
      "Epoch [1252/2000], Iter [1] Loss: 0.3805 Training Accuracy: 0.77778\n",
      "Epoch [1253/2000], Iter [1] Loss: 0.3673 Training Accuracy: 0.80000\n",
      "Epoch [1254/2000], Iter [1] Loss: 0.3788 Training Accuracy: 0.82222\n",
      "Epoch [1255/2000], Iter [1] Loss: 0.3654 Training Accuracy: 0.82222\n",
      "Epoch [1256/2000], Iter [1] Loss: 0.3715 Training Accuracy: 0.81481\n",
      "Epoch [1257/2000], Iter [1] Loss: 0.3767 Training Accuracy: 0.80000\n",
      "Epoch [1258/2000], Iter [1] Loss: 0.3749 Training Accuracy: 0.79259\n",
      "Epoch [1259/2000], Iter [1] Loss: 0.3723 Training Accuracy: 0.80000\n",
      "Epoch [1260/2000], Iter [1] Loss: 0.3918 Training Accuracy: 0.80000\n",
      "Epoch [1261/2000], Iter [1] Loss: 0.3578 Training Accuracy: 0.81481\n",
      "Epoch [1262/2000], Iter [1] Loss: 0.3714 Training Accuracy: 0.80000\n",
      "Epoch [1263/2000], Iter [1] Loss: 0.3745 Training Accuracy: 0.81481\n",
      "Epoch [1264/2000], Iter [1] Loss: 0.3790 Training Accuracy: 0.79259\n",
      "Epoch [1265/2000], Iter [1] Loss: 0.3607 Training Accuracy: 0.78519\n",
      "Epoch [1266/2000], Iter [1] Loss: 0.3633 Training Accuracy: 0.80741\n",
      "Epoch [1267/2000], Iter [1] Loss: 0.3706 Training Accuracy: 0.80000\n",
      "Epoch [1268/2000], Iter [1] Loss: 0.3706 Training Accuracy: 0.80741\n",
      "Epoch [1269/2000], Iter [1] Loss: 0.3710 Training Accuracy: 0.82222\n",
      "Epoch [1270/2000], Iter [1] Loss: 0.3650 Training Accuracy: 0.78519\n",
      "Epoch [1271/2000], Iter [1] Loss: 0.3765 Training Accuracy: 0.81481\n",
      "Epoch [1272/2000], Iter [1] Loss: 0.3452 Training Accuracy: 0.78519\n",
      "Epoch [1273/2000], Iter [1] Loss: 0.3732 Training Accuracy: 0.81481\n",
      "Epoch [1274/2000], Iter [1] Loss: 0.3827 Training Accuracy: 0.78519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1275/2000], Iter [1] Loss: 0.3658 Training Accuracy: 0.79259\n",
      "Epoch [1276/2000], Iter [1] Loss: 0.3627 Training Accuracy: 0.80000\n",
      "Epoch [1277/2000], Iter [1] Loss: 0.3669 Training Accuracy: 0.82222\n",
      "Epoch [1278/2000], Iter [1] Loss: 0.3732 Training Accuracy: 0.80741\n",
      "Epoch [1279/2000], Iter [1] Loss: 0.3569 Training Accuracy: 0.81481\n",
      "Epoch [1280/2000], Iter [1] Loss: 0.3645 Training Accuracy: 0.79259\n",
      "Epoch [1281/2000], Iter [1] Loss: 0.3602 Training Accuracy: 0.80000\n",
      "Epoch [1282/2000], Iter [1] Loss: 0.3633 Training Accuracy: 0.80000\n",
      "Epoch [1283/2000], Iter [1] Loss: 0.3576 Training Accuracy: 0.79259\n",
      "Epoch [1284/2000], Iter [1] Loss: 0.3666 Training Accuracy: 0.77778\n",
      "Epoch [1285/2000], Iter [1] Loss: 0.3714 Training Accuracy: 0.78519\n",
      "Epoch [1286/2000], Iter [1] Loss: 0.3665 Training Accuracy: 0.82222\n",
      "Epoch [1287/2000], Iter [1] Loss: 0.3716 Training Accuracy: 0.81481\n",
      "Epoch [1288/2000], Iter [1] Loss: 0.3452 Training Accuracy: 0.80741\n",
      "Epoch [1289/2000], Iter [1] Loss: 0.3543 Training Accuracy: 0.80741\n",
      "Epoch [1290/2000], Iter [1] Loss: 0.3605 Training Accuracy: 0.80000\n",
      "Epoch [1291/2000], Iter [1] Loss: 0.3623 Training Accuracy: 0.81481\n",
      "Epoch [1292/2000], Iter [1] Loss: 0.3764 Training Accuracy: 0.80741\n",
      "Epoch [1293/2000], Iter [1] Loss: 0.3825 Training Accuracy: 0.80741\n",
      "Epoch [1294/2000], Iter [1] Loss: 0.3529 Training Accuracy: 0.80000\n",
      "Epoch [1295/2000], Iter [1] Loss: 0.3661 Training Accuracy: 0.80741\n",
      "Epoch [1296/2000], Iter [1] Loss: 0.3626 Training Accuracy: 0.82222\n",
      "Epoch [1297/2000], Iter [1] Loss: 0.3612 Training Accuracy: 0.80741\n",
      "Epoch [1298/2000], Iter [1] Loss: 0.3698 Training Accuracy: 0.82963\n",
      "Epoch [1299/2000], Iter [1] Loss: 0.3558 Training Accuracy: 0.83704\n",
      "Epoch [1300/2000], Iter [1] Loss: 0.3609 Training Accuracy: 0.81481\n",
      "Epoch [1301/2000], Iter [1] Loss: 0.3522 Training Accuracy: 0.81481\n",
      "Epoch [1302/2000], Iter [1] Loss: 0.3434 Training Accuracy: 0.80000\n",
      "Epoch [1303/2000], Iter [1] Loss: 0.3631 Training Accuracy: 0.81481\n",
      "Epoch [1304/2000], Iter [1] Loss: 0.3577 Training Accuracy: 0.82222\n",
      "Epoch [1305/2000], Iter [1] Loss: 0.3510 Training Accuracy: 0.79259\n",
      "Epoch [1306/2000], Iter [1] Loss: 0.3718 Training Accuracy: 0.80741\n",
      "Epoch [1307/2000], Iter [1] Loss: 0.3580 Training Accuracy: 0.82222\n",
      "Epoch [1308/2000], Iter [1] Loss: 0.3505 Training Accuracy: 0.82222\n",
      "Epoch [1309/2000], Iter [1] Loss: 0.3682 Training Accuracy: 0.80741\n",
      "Epoch [1310/2000], Iter [1] Loss: 0.3707 Training Accuracy: 0.82963\n",
      "Epoch [1311/2000], Iter [1] Loss: 0.3658 Training Accuracy: 0.81481\n",
      "Epoch [1312/2000], Iter [1] Loss: 0.3517 Training Accuracy: 0.78519\n",
      "Epoch [1313/2000], Iter [1] Loss: 0.3600 Training Accuracy: 0.78519\n",
      "Epoch [1314/2000], Iter [1] Loss: 0.3554 Training Accuracy: 0.80000\n",
      "Epoch [1315/2000], Iter [1] Loss: 0.3679 Training Accuracy: 0.79259\n",
      "Epoch [1316/2000], Iter [1] Loss: 0.3542 Training Accuracy: 0.77037\n",
      "Epoch [1317/2000], Iter [1] Loss: 0.3705 Training Accuracy: 0.80741\n",
      "Epoch [1318/2000], Iter [1] Loss: 0.3581 Training Accuracy: 0.80741\n",
      "Epoch [1319/2000], Iter [1] Loss: 0.3634 Training Accuracy: 0.79259\n",
      "Epoch [1320/2000], Iter [1] Loss: 0.3718 Training Accuracy: 0.79259\n",
      "Epoch [1321/2000], Iter [1] Loss: 0.3591 Training Accuracy: 0.80000\n",
      "Epoch [1322/2000], Iter [1] Loss: 0.3775 Training Accuracy: 0.82222\n",
      "Epoch [1323/2000], Iter [1] Loss: 0.3578 Training Accuracy: 0.80000\n",
      "Epoch [1324/2000], Iter [1] Loss: 0.3545 Training Accuracy: 0.79259\n",
      "Epoch [1325/2000], Iter [1] Loss: 0.3511 Training Accuracy: 0.82222\n",
      "Epoch [1326/2000], Iter [1] Loss: 0.3601 Training Accuracy: 0.79259\n",
      "Epoch [1327/2000], Iter [1] Loss: 0.3549 Training Accuracy: 0.82963\n",
      "Epoch [1328/2000], Iter [1] Loss: 0.3691 Training Accuracy: 0.81481\n",
      "Epoch [1329/2000], Iter [1] Loss: 0.3693 Training Accuracy: 0.80000\n",
      "Epoch [1330/2000], Iter [1] Loss: 0.3477 Training Accuracy: 0.82963\n",
      "Epoch [1331/2000], Iter [1] Loss: 0.3585 Training Accuracy: 0.82222\n",
      "Epoch [1332/2000], Iter [1] Loss: 0.3401 Training Accuracy: 0.81481\n",
      "Epoch [1333/2000], Iter [1] Loss: 0.3537 Training Accuracy: 0.80000\n",
      "Epoch [1334/2000], Iter [1] Loss: 0.3657 Training Accuracy: 0.80000\n",
      "Epoch [1335/2000], Iter [1] Loss: 0.3540 Training Accuracy: 0.79259\n",
      "Epoch [1336/2000], Iter [1] Loss: 0.3711 Training Accuracy: 0.81481\n",
      "Epoch [1337/2000], Iter [1] Loss: 0.3730 Training Accuracy: 0.80741\n",
      "Epoch [1338/2000], Iter [1] Loss: 0.3560 Training Accuracy: 0.81481\n",
      "Epoch [1339/2000], Iter [1] Loss: 0.3727 Training Accuracy: 0.83704\n",
      "Epoch [1340/2000], Iter [1] Loss: 0.3515 Training Accuracy: 0.80000\n",
      "Epoch [1341/2000], Iter [1] Loss: 0.3574 Training Accuracy: 0.80741\n",
      "Epoch [1342/2000], Iter [1] Loss: 0.3512 Training Accuracy: 0.80000\n",
      "Epoch [1343/2000], Iter [1] Loss: 0.3741 Training Accuracy: 0.77778\n",
      "Epoch [1344/2000], Iter [1] Loss: 0.3624 Training Accuracy: 0.80000\n",
      "Epoch [1345/2000], Iter [1] Loss: 0.3510 Training Accuracy: 0.77778\n",
      "Epoch [1346/2000], Iter [1] Loss: 0.3609 Training Accuracy: 0.78519\n",
      "Epoch [1347/2000], Iter [1] Loss: 0.3510 Training Accuracy: 0.81481\n",
      "Epoch [1348/2000], Iter [1] Loss: 0.3491 Training Accuracy: 0.81481\n",
      "Epoch [1349/2000], Iter [1] Loss: 0.3543 Training Accuracy: 0.81481\n",
      "Epoch [1350/2000], Iter [1] Loss: 0.3593 Training Accuracy: 0.77778\n",
      "Epoch [1351/2000], Iter [1] Loss: 0.3604 Training Accuracy: 0.80000\n",
      "Epoch [1352/2000], Iter [1] Loss: 0.3664 Training Accuracy: 0.81481\n",
      "Epoch [1353/2000], Iter [1] Loss: 0.3449 Training Accuracy: 0.80741\n",
      "Epoch [1354/2000], Iter [1] Loss: 0.3445 Training Accuracy: 0.80741\n",
      "Epoch [1355/2000], Iter [1] Loss: 0.3531 Training Accuracy: 0.80741\n",
      "Epoch [1356/2000], Iter [1] Loss: 0.3504 Training Accuracy: 0.82963\n",
      "Epoch [1357/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.81481\n",
      "Epoch [1358/2000], Iter [1] Loss: 0.3723 Training Accuracy: 0.82222\n",
      "Epoch [1359/2000], Iter [1] Loss: 0.3519 Training Accuracy: 0.80741\n",
      "Epoch [1360/2000], Iter [1] Loss: 0.3733 Training Accuracy: 0.80741\n",
      "Epoch [1361/2000], Iter [1] Loss: 0.3510 Training Accuracy: 0.80741\n",
      "Epoch [1362/2000], Iter [1] Loss: 0.3500 Training Accuracy: 0.81481\n",
      "Epoch [1363/2000], Iter [1] Loss: 0.3690 Training Accuracy: 0.80741\n",
      "Epoch [1364/2000], Iter [1] Loss: 0.3617 Training Accuracy: 0.82963\n",
      "Epoch [1365/2000], Iter [1] Loss: 0.3468 Training Accuracy: 0.81481\n",
      "Epoch [1366/2000], Iter [1] Loss: 0.3492 Training Accuracy: 0.80741\n",
      "Epoch [1367/2000], Iter [1] Loss: 0.3658 Training Accuracy: 0.81481\n",
      "Epoch [1368/2000], Iter [1] Loss: 0.3503 Training Accuracy: 0.81481\n",
      "Epoch [1369/2000], Iter [1] Loss: 0.3553 Training Accuracy: 0.80741\n",
      "Epoch [1370/2000], Iter [1] Loss: 0.3604 Training Accuracy: 0.80741\n",
      "Epoch [1371/2000], Iter [1] Loss: 0.3520 Training Accuracy: 0.77778\n",
      "Epoch [1372/2000], Iter [1] Loss: 0.3602 Training Accuracy: 0.80741\n",
      "Epoch [1373/2000], Iter [1] Loss: 0.3592 Training Accuracy: 0.78519\n",
      "Epoch [1374/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.80000\n",
      "Epoch [1375/2000], Iter [1] Loss: 0.3577 Training Accuracy: 0.82963\n",
      "Epoch [1376/2000], Iter [1] Loss: 0.3624 Training Accuracy: 0.82222\n",
      "Epoch [1377/2000], Iter [1] Loss: 0.3493 Training Accuracy: 0.77037\n",
      "Epoch [1378/2000], Iter [1] Loss: 0.3523 Training Accuracy: 0.82222\n",
      "Epoch [1379/2000], Iter [1] Loss: 0.3595 Training Accuracy: 0.79259\n",
      "Epoch [1380/2000], Iter [1] Loss: 0.3488 Training Accuracy: 0.80741\n",
      "Epoch [1381/2000], Iter [1] Loss: 0.3433 Training Accuracy: 0.81481\n",
      "Epoch [1382/2000], Iter [1] Loss: 0.3495 Training Accuracy: 0.80741\n",
      "Epoch [1383/2000], Iter [1] Loss: 0.3623 Training Accuracy: 0.79259\n",
      "Epoch [1384/2000], Iter [1] Loss: 0.3613 Training Accuracy: 0.80000\n",
      "Epoch [1385/2000], Iter [1] Loss: 0.3533 Training Accuracy: 0.82222\n",
      "Epoch [1386/2000], Iter [1] Loss: 0.3599 Training Accuracy: 0.80000\n",
      "Epoch [1387/2000], Iter [1] Loss: 0.3547 Training Accuracy: 0.80741\n",
      "Epoch [1388/2000], Iter [1] Loss: 0.3534 Training Accuracy: 0.81481\n",
      "Epoch [1389/2000], Iter [1] Loss: 0.3555 Training Accuracy: 0.80741\n",
      "Epoch [1390/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.81481\n",
      "Epoch [1391/2000], Iter [1] Loss: 0.3603 Training Accuracy: 0.79259\n",
      "Epoch [1392/2000], Iter [1] Loss: 0.3540 Training Accuracy: 0.84444\n",
      "Epoch [1393/2000], Iter [1] Loss: 0.3718 Training Accuracy: 0.81481\n",
      "Epoch [1394/2000], Iter [1] Loss: 0.3528 Training Accuracy: 0.80741\n",
      "Epoch [1395/2000], Iter [1] Loss: 0.3470 Training Accuracy: 0.79259\n",
      "Epoch [1396/2000], Iter [1] Loss: 0.3338 Training Accuracy: 0.80000\n",
      "Epoch [1397/2000], Iter [1] Loss: 0.3557 Training Accuracy: 0.80000\n",
      "Epoch [1398/2000], Iter [1] Loss: 0.3472 Training Accuracy: 0.78519\n",
      "Epoch [1399/2000], Iter [1] Loss: 0.3459 Training Accuracy: 0.79259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1400/2000], Iter [1] Loss: 0.3515 Training Accuracy: 0.81481\n",
      "Epoch [1401/2000], Iter [1] Loss: 0.3646 Training Accuracy: 0.81481\n",
      "Epoch [1402/2000], Iter [1] Loss: 0.3668 Training Accuracy: 0.81481\n",
      "Epoch [1403/2000], Iter [1] Loss: 0.3390 Training Accuracy: 0.82963\n",
      "Epoch [1404/2000], Iter [1] Loss: 0.3393 Training Accuracy: 0.80000\n",
      "Epoch [1405/2000], Iter [1] Loss: 0.3445 Training Accuracy: 0.79259\n",
      "Epoch [1406/2000], Iter [1] Loss: 0.3618 Training Accuracy: 0.79259\n",
      "Epoch [1407/2000], Iter [1] Loss: 0.3613 Training Accuracy: 0.80000\n",
      "Epoch [1408/2000], Iter [1] Loss: 0.3471 Training Accuracy: 0.83704\n",
      "Epoch [1409/2000], Iter [1] Loss: 0.3585 Training Accuracy: 0.79259\n",
      "Epoch [1410/2000], Iter [1] Loss: 0.3508 Training Accuracy: 0.82222\n",
      "Epoch [1411/2000], Iter [1] Loss: 0.3504 Training Accuracy: 0.79259\n",
      "Epoch [1412/2000], Iter [1] Loss: 0.3432 Training Accuracy: 0.82963\n",
      "Epoch [1413/2000], Iter [1] Loss: 0.3466 Training Accuracy: 0.80741\n",
      "Epoch [1414/2000], Iter [1] Loss: 0.3477 Training Accuracy: 0.82963\n",
      "Epoch [1415/2000], Iter [1] Loss: 0.3369 Training Accuracy: 0.80000\n",
      "Epoch [1416/2000], Iter [1] Loss: 0.3525 Training Accuracy: 0.82963\n",
      "Epoch [1417/2000], Iter [1] Loss: 0.3346 Training Accuracy: 0.81481\n",
      "Epoch [1418/2000], Iter [1] Loss: 0.3618 Training Accuracy: 0.80000\n",
      "Epoch [1419/2000], Iter [1] Loss: 0.3436 Training Accuracy: 0.80000\n",
      "Epoch [1420/2000], Iter [1] Loss: 0.3542 Training Accuracy: 0.80741\n",
      "Epoch [1421/2000], Iter [1] Loss: 0.3548 Training Accuracy: 0.81481\n",
      "Epoch [1422/2000], Iter [1] Loss: 0.3498 Training Accuracy: 0.80741\n",
      "Epoch [1423/2000], Iter [1] Loss: 0.3529 Training Accuracy: 0.81481\n",
      "Epoch [1424/2000], Iter [1] Loss: 0.3568 Training Accuracy: 0.80741\n",
      "Epoch [1425/2000], Iter [1] Loss: 0.3439 Training Accuracy: 0.81481\n",
      "Epoch [1426/2000], Iter [1] Loss: 0.3317 Training Accuracy: 0.81481\n",
      "Epoch [1427/2000], Iter [1] Loss: 0.3499 Training Accuracy: 0.79259\n",
      "Epoch [1428/2000], Iter [1] Loss: 0.3567 Training Accuracy: 0.80000\n",
      "Epoch [1429/2000], Iter [1] Loss: 0.3490 Training Accuracy: 0.82963\n",
      "Epoch [1430/2000], Iter [1] Loss: 0.3682 Training Accuracy: 0.81481\n",
      "Epoch [1431/2000], Iter [1] Loss: 0.3607 Training Accuracy: 0.80000\n",
      "Epoch [1432/2000], Iter [1] Loss: 0.3472 Training Accuracy: 0.80000\n",
      "Epoch [1433/2000], Iter [1] Loss: 0.3488 Training Accuracy: 0.80741\n",
      "Epoch [1434/2000], Iter [1] Loss: 0.3500 Training Accuracy: 0.80741\n",
      "Epoch [1435/2000], Iter [1] Loss: 0.3644 Training Accuracy: 0.77778\n",
      "Epoch [1436/2000], Iter [1] Loss: 0.3483 Training Accuracy: 0.79259\n",
      "Epoch [1437/2000], Iter [1] Loss: 0.3511 Training Accuracy: 0.78519\n",
      "Epoch [1438/2000], Iter [1] Loss: 0.3477 Training Accuracy: 0.78519\n",
      "Epoch [1439/2000], Iter [1] Loss: 0.3525 Training Accuracy: 0.82222\n",
      "Epoch [1440/2000], Iter [1] Loss: 0.3506 Training Accuracy: 0.78519\n",
      "Epoch [1441/2000], Iter [1] Loss: 0.3559 Training Accuracy: 0.80741\n",
      "Epoch [1442/2000], Iter [1] Loss: 0.3551 Training Accuracy: 0.82963\n",
      "Epoch [1443/2000], Iter [1] Loss: 0.3556 Training Accuracy: 0.80741\n",
      "Epoch [1444/2000], Iter [1] Loss: 0.3499 Training Accuracy: 0.78519\n",
      "Epoch [1445/2000], Iter [1] Loss: 0.3500 Training Accuracy: 0.81481\n",
      "Epoch [1446/2000], Iter [1] Loss: 0.3607 Training Accuracy: 0.82222\n",
      "Epoch [1447/2000], Iter [1] Loss: 0.3549 Training Accuracy: 0.82963\n",
      "Epoch [1448/2000], Iter [1] Loss: 0.3550 Training Accuracy: 0.81481\n",
      "Epoch [1449/2000], Iter [1] Loss: 0.3484 Training Accuracy: 0.77778\n",
      "Epoch [1450/2000], Iter [1] Loss: 0.3515 Training Accuracy: 0.84444\n",
      "Epoch [1451/2000], Iter [1] Loss: 0.3417 Training Accuracy: 0.81481\n",
      "Epoch [1452/2000], Iter [1] Loss: 0.3593 Training Accuracy: 0.80741\n",
      "Epoch [1453/2000], Iter [1] Loss: 0.3572 Training Accuracy: 0.81481\n",
      "Epoch [1454/2000], Iter [1] Loss: 0.3478 Training Accuracy: 0.79259\n",
      "Epoch [1455/2000], Iter [1] Loss: 0.3500 Training Accuracy: 0.83704\n",
      "Epoch [1456/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.80000\n",
      "Epoch [1457/2000], Iter [1] Loss: 0.3515 Training Accuracy: 0.81481\n",
      "Epoch [1458/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.80000\n",
      "Epoch [1459/2000], Iter [1] Loss: 0.3405 Training Accuracy: 0.84444\n",
      "Epoch [1460/2000], Iter [1] Loss: 0.3442 Training Accuracy: 0.83704\n",
      "Epoch [1461/2000], Iter [1] Loss: 0.3482 Training Accuracy: 0.79259\n",
      "Epoch [1462/2000], Iter [1] Loss: 0.3521 Training Accuracy: 0.79259\n",
      "Epoch [1463/2000], Iter [1] Loss: 0.3518 Training Accuracy: 0.82222\n",
      "Epoch [1464/2000], Iter [1] Loss: 0.3389 Training Accuracy: 0.80000\n",
      "Epoch [1465/2000], Iter [1] Loss: 0.3385 Training Accuracy: 0.81481\n",
      "Epoch [1466/2000], Iter [1] Loss: 0.3532 Training Accuracy: 0.81481\n",
      "Epoch [1467/2000], Iter [1] Loss: 0.3458 Training Accuracy: 0.82222\n",
      "Epoch [1468/2000], Iter [1] Loss: 0.3514 Training Accuracy: 0.83704\n",
      "Epoch [1469/2000], Iter [1] Loss: 0.3551 Training Accuracy: 0.80741\n",
      "Epoch [1470/2000], Iter [1] Loss: 0.3394 Training Accuracy: 0.79259\n",
      "Epoch [1471/2000], Iter [1] Loss: 0.3501 Training Accuracy: 0.82963\n",
      "Epoch [1472/2000], Iter [1] Loss: 0.3494 Training Accuracy: 0.79259\n",
      "Epoch [1473/2000], Iter [1] Loss: 0.3378 Training Accuracy: 0.80000\n",
      "Epoch [1474/2000], Iter [1] Loss: 0.3442 Training Accuracy: 0.81481\n",
      "Epoch [1475/2000], Iter [1] Loss: 0.3296 Training Accuracy: 0.80741\n",
      "Epoch [1476/2000], Iter [1] Loss: 0.3433 Training Accuracy: 0.81481\n",
      "Epoch [1477/2000], Iter [1] Loss: 0.3395 Training Accuracy: 0.77778\n",
      "Epoch [1478/2000], Iter [1] Loss: 0.3451 Training Accuracy: 0.82963\n",
      "Epoch [1479/2000], Iter [1] Loss: 0.3485 Training Accuracy: 0.82222\n",
      "Epoch [1480/2000], Iter [1] Loss: 0.3549 Training Accuracy: 0.81481\n",
      "Epoch [1481/2000], Iter [1] Loss: 0.3360 Training Accuracy: 0.82963\n",
      "Epoch [1482/2000], Iter [1] Loss: 0.3367 Training Accuracy: 0.79259\n",
      "Epoch [1483/2000], Iter [1] Loss: 0.3489 Training Accuracy: 0.82963\n",
      "Epoch [1484/2000], Iter [1] Loss: 0.3561 Training Accuracy: 0.79259\n",
      "Epoch [1485/2000], Iter [1] Loss: 0.3455 Training Accuracy: 0.81481\n",
      "Epoch [1486/2000], Iter [1] Loss: 0.3615 Training Accuracy: 0.83704\n",
      "Epoch [1487/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.79259\n",
      "Epoch [1488/2000], Iter [1] Loss: 0.3320 Training Accuracy: 0.80741\n",
      "Epoch [1489/2000], Iter [1] Loss: 0.3467 Training Accuracy: 0.82222\n",
      "Epoch [1490/2000], Iter [1] Loss: 0.3419 Training Accuracy: 0.80000\n",
      "Epoch [1491/2000], Iter [1] Loss: 0.3458 Training Accuracy: 0.79259\n",
      "Epoch [1492/2000], Iter [1] Loss: 0.3387 Training Accuracy: 0.80000\n",
      "Epoch [1493/2000], Iter [1] Loss: 0.3329 Training Accuracy: 0.79259\n",
      "Epoch [1494/2000], Iter [1] Loss: 0.3458 Training Accuracy: 0.80741\n",
      "Epoch [1495/2000], Iter [1] Loss: 0.3337 Training Accuracy: 0.84444\n",
      "Epoch [1496/2000], Iter [1] Loss: 0.3277 Training Accuracy: 0.81481\n",
      "Epoch [1497/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.82222\n",
      "Epoch [1498/2000], Iter [1] Loss: 0.3305 Training Accuracy: 0.82963\n",
      "Epoch [1499/2000], Iter [1] Loss: 0.3490 Training Accuracy: 0.80741\n",
      "Epoch [1500/2000], Iter [1] Loss: 0.3327 Training Accuracy: 0.83704\n",
      "Epoch [1501/2000], Iter [1] Loss: 0.3380 Training Accuracy: 0.82222\n",
      "Epoch [1502/2000], Iter [1] Loss: 0.3478 Training Accuracy: 0.82222\n",
      "Epoch [1503/2000], Iter [1] Loss: 0.3414 Training Accuracy: 0.78519\n",
      "Epoch [1504/2000], Iter [1] Loss: 0.3440 Training Accuracy: 0.82963\n",
      "Epoch [1505/2000], Iter [1] Loss: 0.3431 Training Accuracy: 0.80000\n",
      "Epoch [1506/2000], Iter [1] Loss: 0.3376 Training Accuracy: 0.82222\n",
      "Epoch [1507/2000], Iter [1] Loss: 0.3475 Training Accuracy: 0.79259\n",
      "Epoch [1508/2000], Iter [1] Loss: 0.3450 Training Accuracy: 0.79259\n",
      "Epoch [1509/2000], Iter [1] Loss: 0.3365 Training Accuracy: 0.78519\n",
      "Epoch [1510/2000], Iter [1] Loss: 0.3453 Training Accuracy: 0.80000\n",
      "Epoch [1511/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.81481\n",
      "Epoch [1512/2000], Iter [1] Loss: 0.3330 Training Accuracy: 0.77778\n",
      "Epoch [1513/2000], Iter [1] Loss: 0.3374 Training Accuracy: 0.78519\n",
      "Epoch [1514/2000], Iter [1] Loss: 0.3353 Training Accuracy: 0.80741\n",
      "Epoch [1515/2000], Iter [1] Loss: 0.3297 Training Accuracy: 0.81481\n",
      "Epoch [1516/2000], Iter [1] Loss: 0.3516 Training Accuracy: 0.80000\n",
      "Epoch [1517/2000], Iter [1] Loss: 0.3335 Training Accuracy: 0.80741\n",
      "Epoch [1518/2000], Iter [1] Loss: 0.3523 Training Accuracy: 0.85185\n",
      "Epoch [1519/2000], Iter [1] Loss: 0.3388 Training Accuracy: 0.84444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1520/2000], Iter [1] Loss: 0.3451 Training Accuracy: 0.81481\n",
      "Epoch [1521/2000], Iter [1] Loss: 0.3397 Training Accuracy: 0.80000\n",
      "Epoch [1522/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.80000\n",
      "Epoch [1523/2000], Iter [1] Loss: 0.3373 Training Accuracy: 0.79259\n",
      "Epoch [1524/2000], Iter [1] Loss: 0.3361 Training Accuracy: 0.82222\n",
      "Epoch [1525/2000], Iter [1] Loss: 0.3357 Training Accuracy: 0.79259\n",
      "Epoch [1526/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.81481\n",
      "Epoch [1527/2000], Iter [1] Loss: 0.3315 Training Accuracy: 0.82222\n",
      "Epoch [1528/2000], Iter [1] Loss: 0.3349 Training Accuracy: 0.81481\n",
      "Epoch [1529/2000], Iter [1] Loss: 0.3390 Training Accuracy: 0.80000\n",
      "Epoch [1530/2000], Iter [1] Loss: 0.3426 Training Accuracy: 0.80741\n",
      "Epoch [1531/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.82963\n",
      "Epoch [1532/2000], Iter [1] Loss: 0.3198 Training Accuracy: 0.82222\n",
      "Epoch [1533/2000], Iter [1] Loss: 0.3356 Training Accuracy: 0.83704\n",
      "Epoch [1534/2000], Iter [1] Loss: 0.3321 Training Accuracy: 0.80741\n",
      "Epoch [1535/2000], Iter [1] Loss: 0.3422 Training Accuracy: 0.82222\n",
      "Epoch [1536/2000], Iter [1] Loss: 0.3329 Training Accuracy: 0.83704\n",
      "Epoch [1537/2000], Iter [1] Loss: 0.3413 Training Accuracy: 0.79259\n",
      "Epoch [1538/2000], Iter [1] Loss: 0.3273 Training Accuracy: 0.80741\n",
      "Epoch [1539/2000], Iter [1] Loss: 0.3397 Training Accuracy: 0.79259\n",
      "Epoch [1540/2000], Iter [1] Loss: 0.3373 Training Accuracy: 0.84444\n",
      "Epoch [1541/2000], Iter [1] Loss: 0.3424 Training Accuracy: 0.80000\n",
      "Epoch [1542/2000], Iter [1] Loss: 0.3427 Training Accuracy: 0.79259\n",
      "Epoch [1543/2000], Iter [1] Loss: 0.3454 Training Accuracy: 0.85926\n",
      "Epoch [1544/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.81481\n",
      "Epoch [1545/2000], Iter [1] Loss: 0.3433 Training Accuracy: 0.82222\n",
      "Epoch [1546/2000], Iter [1] Loss: 0.3332 Training Accuracy: 0.82963\n",
      "Epoch [1547/2000], Iter [1] Loss: 0.3351 Training Accuracy: 0.82222\n",
      "Epoch [1548/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.82222\n",
      "Epoch [1549/2000], Iter [1] Loss: 0.3297 Training Accuracy: 0.81481\n",
      "Epoch [1550/2000], Iter [1] Loss: 0.3349 Training Accuracy: 0.83704\n",
      "Epoch [1551/2000], Iter [1] Loss: 0.3346 Training Accuracy: 0.83704\n",
      "Epoch [1552/2000], Iter [1] Loss: 0.3513 Training Accuracy: 0.81481\n",
      "Epoch [1553/2000], Iter [1] Loss: 0.3318 Training Accuracy: 0.83704\n",
      "Epoch [1554/2000], Iter [1] Loss: 0.3366 Training Accuracy: 0.80741\n",
      "Epoch [1555/2000], Iter [1] Loss: 0.3327 Training Accuracy: 0.77778\n",
      "Epoch [1556/2000], Iter [1] Loss: 0.3414 Training Accuracy: 0.82222\n",
      "Epoch [1557/2000], Iter [1] Loss: 0.3442 Training Accuracy: 0.80000\n",
      "Epoch [1558/2000], Iter [1] Loss: 0.3331 Training Accuracy: 0.81481\n",
      "Epoch [1559/2000], Iter [1] Loss: 0.3289 Training Accuracy: 0.81481\n",
      "Epoch [1560/2000], Iter [1] Loss: 0.3407 Training Accuracy: 0.82963\n",
      "Epoch [1561/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.79259\n",
      "Epoch [1562/2000], Iter [1] Loss: 0.3379 Training Accuracy: 0.81481\n",
      "Epoch [1563/2000], Iter [1] Loss: 0.3288 Training Accuracy: 0.79259\n",
      "Epoch [1564/2000], Iter [1] Loss: 0.3377 Training Accuracy: 0.83704\n",
      "Epoch [1565/2000], Iter [1] Loss: 0.3410 Training Accuracy: 0.80000\n",
      "Epoch [1566/2000], Iter [1] Loss: 0.3304 Training Accuracy: 0.83704\n",
      "Epoch [1567/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.82222\n",
      "Epoch [1568/2000], Iter [1] Loss: 0.3284 Training Accuracy: 0.77037\n",
      "Epoch [1569/2000], Iter [1] Loss: 0.3175 Training Accuracy: 0.80741\n",
      "Epoch [1570/2000], Iter [1] Loss: 0.3282 Training Accuracy: 0.81481\n",
      "Epoch [1571/2000], Iter [1] Loss: 0.3308 Training Accuracy: 0.82222\n",
      "Epoch [1572/2000], Iter [1] Loss: 0.3315 Training Accuracy: 0.82222\n",
      "Epoch [1573/2000], Iter [1] Loss: 0.3412 Training Accuracy: 0.80000\n",
      "Epoch [1574/2000], Iter [1] Loss: 0.3129 Training Accuracy: 0.79259\n",
      "Epoch [1575/2000], Iter [1] Loss: 0.3377 Training Accuracy: 0.80741\n",
      "Epoch [1576/2000], Iter [1] Loss: 0.3298 Training Accuracy: 0.82222\n",
      "Epoch [1577/2000], Iter [1] Loss: 0.3293 Training Accuracy: 0.81481\n",
      "Epoch [1578/2000], Iter [1] Loss: 0.3324 Training Accuracy: 0.80000\n",
      "Epoch [1579/2000], Iter [1] Loss: 0.3350 Training Accuracy: 0.80000\n",
      "Epoch [1580/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.82222\n",
      "Epoch [1581/2000], Iter [1] Loss: 0.3354 Training Accuracy: 0.82963\n",
      "Epoch [1582/2000], Iter [1] Loss: 0.3392 Training Accuracy: 0.82963\n",
      "Epoch [1583/2000], Iter [1] Loss: 0.3423 Training Accuracy: 0.82222\n",
      "Epoch [1584/2000], Iter [1] Loss: 0.3396 Training Accuracy: 0.80741\n",
      "Epoch [1585/2000], Iter [1] Loss: 0.3439 Training Accuracy: 0.80741\n",
      "Epoch [1586/2000], Iter [1] Loss: 0.3370 Training Accuracy: 0.83704\n",
      "Epoch [1587/2000], Iter [1] Loss: 0.3144 Training Accuracy: 0.79259\n",
      "Epoch [1588/2000], Iter [1] Loss: 0.3423 Training Accuracy: 0.80000\n",
      "Epoch [1589/2000], Iter [1] Loss: 0.3340 Training Accuracy: 0.80741\n",
      "Epoch [1590/2000], Iter [1] Loss: 0.3192 Training Accuracy: 0.82963\n",
      "Epoch [1591/2000], Iter [1] Loss: 0.3353 Training Accuracy: 0.79259\n",
      "Epoch [1592/2000], Iter [1] Loss: 0.3422 Training Accuracy: 0.80741\n",
      "Epoch [1593/2000], Iter [1] Loss: 0.3227 Training Accuracy: 0.81481\n",
      "Epoch [1594/2000], Iter [1] Loss: 0.3327 Training Accuracy: 0.81481\n",
      "Epoch [1595/2000], Iter [1] Loss: 0.3284 Training Accuracy: 0.77037\n",
      "Epoch [1596/2000], Iter [1] Loss: 0.3233 Training Accuracy: 0.82222\n",
      "Epoch [1597/2000], Iter [1] Loss: 0.3384 Training Accuracy: 0.79259\n",
      "Epoch [1598/2000], Iter [1] Loss: 0.3440 Training Accuracy: 0.78519\n",
      "Epoch [1599/2000], Iter [1] Loss: 0.3393 Training Accuracy: 0.83704\n",
      "Epoch [1600/2000], Iter [1] Loss: 0.3311 Training Accuracy: 0.82222\n",
      "Epoch [1601/2000], Iter [1] Loss: 0.3362 Training Accuracy: 0.82222\n",
      "Epoch [1602/2000], Iter [1] Loss: 0.3307 Training Accuracy: 0.80000\n",
      "Epoch [1603/2000], Iter [1] Loss: 0.3477 Training Accuracy: 0.80000\n",
      "Epoch [1604/2000], Iter [1] Loss: 0.3473 Training Accuracy: 0.76296\n",
      "Epoch [1605/2000], Iter [1] Loss: 0.3452 Training Accuracy: 0.80741\n",
      "Epoch [1606/2000], Iter [1] Loss: 0.3291 Training Accuracy: 0.77778\n",
      "Epoch [1607/2000], Iter [1] Loss: 0.3304 Training Accuracy: 0.80000\n",
      "Epoch [1608/2000], Iter [1] Loss: 0.3330 Training Accuracy: 0.79259\n",
      "Epoch [1609/2000], Iter [1] Loss: 0.3307 Training Accuracy: 0.84444\n",
      "Epoch [1610/2000], Iter [1] Loss: 0.3345 Training Accuracy: 0.82222\n",
      "Epoch [1611/2000], Iter [1] Loss: 0.3432 Training Accuracy: 0.77778\n",
      "Epoch [1612/2000], Iter [1] Loss: 0.3259 Training Accuracy: 0.82222\n",
      "Epoch [1613/2000], Iter [1] Loss: 0.3319 Training Accuracy: 0.81481\n",
      "Epoch [1614/2000], Iter [1] Loss: 0.3244 Training Accuracy: 0.81481\n",
      "Epoch [1615/2000], Iter [1] Loss: 0.3309 Training Accuracy: 0.80741\n",
      "Epoch [1616/2000], Iter [1] Loss: 0.3456 Training Accuracy: 0.80741\n",
      "Epoch [1617/2000], Iter [1] Loss: 0.3291 Training Accuracy: 0.80741\n",
      "Epoch [1618/2000], Iter [1] Loss: 0.3250 Training Accuracy: 0.82222\n",
      "Epoch [1619/2000], Iter [1] Loss: 0.3242 Training Accuracy: 0.77778\n",
      "Epoch [1620/2000], Iter [1] Loss: 0.3322 Training Accuracy: 0.82963\n",
      "Epoch [1621/2000], Iter [1] Loss: 0.3309 Training Accuracy: 0.83704\n",
      "Epoch [1622/2000], Iter [1] Loss: 0.3279 Training Accuracy: 0.82222\n",
      "Epoch [1623/2000], Iter [1] Loss: 0.3401 Training Accuracy: 0.80000\n",
      "Epoch [1624/2000], Iter [1] Loss: 0.3423 Training Accuracy: 0.78519\n",
      "Epoch [1625/2000], Iter [1] Loss: 0.3371 Training Accuracy: 0.79259\n",
      "Epoch [1626/2000], Iter [1] Loss: 0.3352 Training Accuracy: 0.81481\n",
      "Epoch [1627/2000], Iter [1] Loss: 0.3320 Training Accuracy: 0.82963\n",
      "Epoch [1628/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.82963\n",
      "Epoch [1629/2000], Iter [1] Loss: 0.3276 Training Accuracy: 0.77037\n",
      "Epoch [1630/2000], Iter [1] Loss: 0.3298 Training Accuracy: 0.80000\n",
      "Epoch [1631/2000], Iter [1] Loss: 0.3339 Training Accuracy: 0.80000\n",
      "Epoch [1632/2000], Iter [1] Loss: 0.3302 Training Accuracy: 0.84444\n",
      "Epoch [1633/2000], Iter [1] Loss: 0.3292 Training Accuracy: 0.83704\n",
      "Epoch [1634/2000], Iter [1] Loss: 0.3278 Training Accuracy: 0.83704\n",
      "Epoch [1635/2000], Iter [1] Loss: 0.3355 Training Accuracy: 0.80741\n",
      "Epoch [1636/2000], Iter [1] Loss: 0.3436 Training Accuracy: 0.80000\n",
      "Epoch [1637/2000], Iter [1] Loss: 0.3234 Training Accuracy: 0.82963\n",
      "Epoch [1638/2000], Iter [1] Loss: 0.3287 Training Accuracy: 0.80741\n",
      "Epoch [1639/2000], Iter [1] Loss: 0.3240 Training Accuracy: 0.77037\n",
      "Epoch [1640/2000], Iter [1] Loss: 0.3166 Training Accuracy: 0.80741\n",
      "Epoch [1641/2000], Iter [1] Loss: 0.3219 Training Accuracy: 0.80741\n",
      "Epoch [1642/2000], Iter [1] Loss: 0.3364 Training Accuracy: 0.82222\n",
      "Epoch [1643/2000], Iter [1] Loss: 0.3315 Training Accuracy: 0.81481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1644/2000], Iter [1] Loss: 0.3158 Training Accuracy: 0.81481\n",
      "Epoch [1645/2000], Iter [1] Loss: 0.3273 Training Accuracy: 0.81481\n",
      "Epoch [1646/2000], Iter [1] Loss: 0.3429 Training Accuracy: 0.80000\n",
      "Epoch [1647/2000], Iter [1] Loss: 0.3205 Training Accuracy: 0.82222\n",
      "Epoch [1648/2000], Iter [1] Loss: 0.3300 Training Accuracy: 0.84444\n",
      "Epoch [1649/2000], Iter [1] Loss: 0.3240 Training Accuracy: 0.81481\n",
      "Epoch [1650/2000], Iter [1] Loss: 0.3255 Training Accuracy: 0.79259\n",
      "Epoch [1651/2000], Iter [1] Loss: 0.3335 Training Accuracy: 0.81481\n",
      "Epoch [1652/2000], Iter [1] Loss: 0.3446 Training Accuracy: 0.81481\n",
      "Epoch [1653/2000], Iter [1] Loss: 0.3481 Training Accuracy: 0.82963\n",
      "Epoch [1654/2000], Iter [1] Loss: 0.3317 Training Accuracy: 0.85185\n",
      "Epoch [1655/2000], Iter [1] Loss: 0.3454 Training Accuracy: 0.81481\n",
      "Epoch [1656/2000], Iter [1] Loss: 0.3240 Training Accuracy: 0.79259\n",
      "Epoch [1657/2000], Iter [1] Loss: 0.3360 Training Accuracy: 0.83704\n",
      "Epoch [1658/2000], Iter [1] Loss: 0.3243 Training Accuracy: 0.77778\n",
      "Epoch [1659/2000], Iter [1] Loss: 0.3462 Training Accuracy: 0.80741\n",
      "Epoch [1660/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.80741\n",
      "Epoch [1661/2000], Iter [1] Loss: 0.3234 Training Accuracy: 0.80741\n",
      "Epoch [1662/2000], Iter [1] Loss: 0.3155 Training Accuracy: 0.82963\n",
      "Epoch [1663/2000], Iter [1] Loss: 0.3266 Training Accuracy: 0.82963\n",
      "Epoch [1664/2000], Iter [1] Loss: 0.3273 Training Accuracy: 0.81481\n",
      "Epoch [1665/2000], Iter [1] Loss: 0.3359 Training Accuracy: 0.81481\n",
      "Epoch [1666/2000], Iter [1] Loss: 0.3398 Training Accuracy: 0.84444\n",
      "Epoch [1667/2000], Iter [1] Loss: 0.3363 Training Accuracy: 0.80741\n",
      "Epoch [1668/2000], Iter [1] Loss: 0.3297 Training Accuracy: 0.79259\n",
      "Epoch [1669/2000], Iter [1] Loss: 0.3346 Training Accuracy: 0.85185\n",
      "Epoch [1670/2000], Iter [1] Loss: 0.3212 Training Accuracy: 0.82963\n",
      "Epoch [1671/2000], Iter [1] Loss: 0.3292 Training Accuracy: 0.79259\n",
      "Epoch [1672/2000], Iter [1] Loss: 0.3168 Training Accuracy: 0.82222\n",
      "Epoch [1673/2000], Iter [1] Loss: 0.3281 Training Accuracy: 0.82963\n",
      "Epoch [1674/2000], Iter [1] Loss: 0.3258 Training Accuracy: 0.83704\n",
      "Epoch [1675/2000], Iter [1] Loss: 0.3263 Training Accuracy: 0.82222\n",
      "Epoch [1676/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.82222\n",
      "Epoch [1677/2000], Iter [1] Loss: 0.3244 Training Accuracy: 0.82963\n",
      "Epoch [1678/2000], Iter [1] Loss: 0.3270 Training Accuracy: 0.81481\n",
      "Epoch [1679/2000], Iter [1] Loss: 0.3284 Training Accuracy: 0.81481\n",
      "Epoch [1680/2000], Iter [1] Loss: 0.3227 Training Accuracy: 0.82222\n",
      "Epoch [1681/2000], Iter [1] Loss: 0.3220 Training Accuracy: 0.80741\n",
      "Epoch [1682/2000], Iter [1] Loss: 0.3329 Training Accuracy: 0.82963\n",
      "Epoch [1683/2000], Iter [1] Loss: 0.3308 Training Accuracy: 0.78519\n",
      "Epoch [1684/2000], Iter [1] Loss: 0.3277 Training Accuracy: 0.82963\n",
      "Epoch [1685/2000], Iter [1] Loss: 0.3336 Training Accuracy: 0.82222\n",
      "Epoch [1686/2000], Iter [1] Loss: 0.3291 Training Accuracy: 0.83704\n",
      "Epoch [1687/2000], Iter [1] Loss: 0.3224 Training Accuracy: 0.77778\n",
      "Epoch [1688/2000], Iter [1] Loss: 0.3244 Training Accuracy: 0.80000\n",
      "Epoch [1689/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.83704\n",
      "Epoch [1690/2000], Iter [1] Loss: 0.3314 Training Accuracy: 0.79259\n",
      "Epoch [1691/2000], Iter [1] Loss: 0.3294 Training Accuracy: 0.82963\n",
      "Epoch [1692/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.79259\n",
      "Epoch [1693/2000], Iter [1] Loss: 0.3219 Training Accuracy: 0.80000\n",
      "Epoch [1694/2000], Iter [1] Loss: 0.3251 Training Accuracy: 0.80741\n",
      "Epoch [1695/2000], Iter [1] Loss: 0.3343 Training Accuracy: 0.82963\n",
      "Epoch [1696/2000], Iter [1] Loss: 0.3256 Training Accuracy: 0.83704\n",
      "Epoch [1697/2000], Iter [1] Loss: 0.3141 Training Accuracy: 0.80741\n",
      "Epoch [1698/2000], Iter [1] Loss: 0.3373 Training Accuracy: 0.81481\n",
      "Epoch [1699/2000], Iter [1] Loss: 0.3330 Training Accuracy: 0.78519\n",
      "Epoch [1700/2000], Iter [1] Loss: 0.3329 Training Accuracy: 0.82222\n",
      "Epoch [1701/2000], Iter [1] Loss: 0.3286 Training Accuracy: 0.78519\n",
      "Epoch [1702/2000], Iter [1] Loss: 0.3251 Training Accuracy: 0.82963\n",
      "Epoch [1703/2000], Iter [1] Loss: 0.3259 Training Accuracy: 0.82222\n",
      "Epoch [1704/2000], Iter [1] Loss: 0.3223 Training Accuracy: 0.82222\n",
      "Epoch [1705/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.82222\n",
      "Epoch [1706/2000], Iter [1] Loss: 0.3161 Training Accuracy: 0.80000\n",
      "Epoch [1707/2000], Iter [1] Loss: 0.3222 Training Accuracy: 0.80000\n",
      "Epoch [1708/2000], Iter [1] Loss: 0.3330 Training Accuracy: 0.83704\n",
      "Epoch [1709/2000], Iter [1] Loss: 0.3390 Training Accuracy: 0.81481\n",
      "Epoch [1710/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.83704\n",
      "Epoch [1711/2000], Iter [1] Loss: 0.3234 Training Accuracy: 0.80000\n",
      "Epoch [1712/2000], Iter [1] Loss: 0.3155 Training Accuracy: 0.82963\n",
      "Epoch [1713/2000], Iter [1] Loss: 0.3239 Training Accuracy: 0.82222\n",
      "Epoch [1714/2000], Iter [1] Loss: 0.3147 Training Accuracy: 0.82222\n",
      "Epoch [1715/2000], Iter [1] Loss: 0.3156 Training Accuracy: 0.79259\n",
      "Epoch [1716/2000], Iter [1] Loss: 0.3335 Training Accuracy: 0.79259\n",
      "Epoch [1717/2000], Iter [1] Loss: 0.3348 Training Accuracy: 0.82222\n",
      "Epoch [1718/2000], Iter [1] Loss: 0.3343 Training Accuracy: 0.78519\n",
      "Epoch [1719/2000], Iter [1] Loss: 0.3290 Training Accuracy: 0.80741\n",
      "Epoch [1720/2000], Iter [1] Loss: 0.3176 Training Accuracy: 0.78519\n",
      "Epoch [1721/2000], Iter [1] Loss: 0.3347 Training Accuracy: 0.78519\n",
      "Epoch [1722/2000], Iter [1] Loss: 0.3300 Training Accuracy: 0.79259\n",
      "Epoch [1723/2000], Iter [1] Loss: 0.3166 Training Accuracy: 0.82222\n",
      "Epoch [1724/2000], Iter [1] Loss: 0.3283 Training Accuracy: 0.82222\n",
      "Epoch [1725/2000], Iter [1] Loss: 0.3187 Training Accuracy: 0.80000\n",
      "Epoch [1726/2000], Iter [1] Loss: 0.3214 Training Accuracy: 0.80000\n",
      "Epoch [1727/2000], Iter [1] Loss: 0.3236 Training Accuracy: 0.78519\n",
      "Epoch [1728/2000], Iter [1] Loss: 0.3176 Training Accuracy: 0.82222\n",
      "Epoch [1729/2000], Iter [1] Loss: 0.3368 Training Accuracy: 0.80741\n",
      "Epoch [1730/2000], Iter [1] Loss: 0.3182 Training Accuracy: 0.82222\n",
      "Epoch [1731/2000], Iter [1] Loss: 0.3232 Training Accuracy: 0.81481\n",
      "Epoch [1732/2000], Iter [1] Loss: 0.3274 Training Accuracy: 0.80000\n",
      "Epoch [1733/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.84444\n",
      "Epoch [1734/2000], Iter [1] Loss: 0.3129 Training Accuracy: 0.83704\n",
      "Epoch [1735/2000], Iter [1] Loss: 0.3215 Training Accuracy: 0.84444\n",
      "Epoch [1736/2000], Iter [1] Loss: 0.3257 Training Accuracy: 0.83704\n",
      "Epoch [1737/2000], Iter [1] Loss: 0.3268 Training Accuracy: 0.78519\n",
      "Epoch [1738/2000], Iter [1] Loss: 0.3196 Training Accuracy: 0.83704\n",
      "Epoch [1739/2000], Iter [1] Loss: 0.3280 Training Accuracy: 0.85185\n",
      "Epoch [1740/2000], Iter [1] Loss: 0.3242 Training Accuracy: 0.80000\n",
      "Epoch [1741/2000], Iter [1] Loss: 0.3278 Training Accuracy: 0.85926\n",
      "Epoch [1742/2000], Iter [1] Loss: 0.3217 Training Accuracy: 0.82963\n",
      "Epoch [1743/2000], Iter [1] Loss: 0.3197 Training Accuracy: 0.80741\n",
      "Epoch [1744/2000], Iter [1] Loss: 0.3355 Training Accuracy: 0.82222\n",
      "Epoch [1745/2000], Iter [1] Loss: 0.3261 Training Accuracy: 0.81481\n",
      "Epoch [1746/2000], Iter [1] Loss: 0.3105 Training Accuracy: 0.82963\n",
      "Epoch [1747/2000], Iter [1] Loss: 0.3177 Training Accuracy: 0.80741\n",
      "Epoch [1748/2000], Iter [1] Loss: 0.3244 Training Accuracy: 0.85185\n",
      "Epoch [1749/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.79259\n",
      "Epoch [1750/2000], Iter [1] Loss: 0.3190 Training Accuracy: 0.80000\n",
      "Epoch [1751/2000], Iter [1] Loss: 0.3170 Training Accuracy: 0.81481\n",
      "Epoch [1752/2000], Iter [1] Loss: 0.3443 Training Accuracy: 0.80741\n",
      "Epoch [1753/2000], Iter [1] Loss: 0.3186 Training Accuracy: 0.84444\n",
      "Epoch [1754/2000], Iter [1] Loss: 0.3241 Training Accuracy: 0.83704\n",
      "Epoch [1755/2000], Iter [1] Loss: 0.3083 Training Accuracy: 0.82963\n",
      "Epoch [1756/2000], Iter [1] Loss: 0.3146 Training Accuracy: 0.80741\n",
      "Epoch [1757/2000], Iter [1] Loss: 0.3196 Training Accuracy: 0.84444\n",
      "Epoch [1758/2000], Iter [1] Loss: 0.3216 Training Accuracy: 0.82222\n",
      "Epoch [1759/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.84444\n",
      "Epoch [1760/2000], Iter [1] Loss: 0.3246 Training Accuracy: 0.80000\n",
      "Epoch [1761/2000], Iter [1] Loss: 0.3305 Training Accuracy: 0.82963\n",
      "Epoch [1762/2000], Iter [1] Loss: 0.3229 Training Accuracy: 0.84444\n",
      "Epoch [1763/2000], Iter [1] Loss: 0.3287 Training Accuracy: 0.80741\n",
      "Epoch [1764/2000], Iter [1] Loss: 0.3174 Training Accuracy: 0.81481\n",
      "Epoch [1765/2000], Iter [1] Loss: 0.3142 Training Accuracy: 0.77778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1766/2000], Iter [1] Loss: 0.3338 Training Accuracy: 0.81481\n",
      "Epoch [1767/2000], Iter [1] Loss: 0.3137 Training Accuracy: 0.80000\n",
      "Epoch [1768/2000], Iter [1] Loss: 0.3207 Training Accuracy: 0.81481\n",
      "Epoch [1769/2000], Iter [1] Loss: 0.3282 Training Accuracy: 0.80000\n",
      "Epoch [1770/2000], Iter [1] Loss: 0.3354 Training Accuracy: 0.78519\n",
      "Epoch [1771/2000], Iter [1] Loss: 0.3115 Training Accuracy: 0.77778\n",
      "Epoch [1772/2000], Iter [1] Loss: 0.3215 Training Accuracy: 0.82963\n",
      "Epoch [1773/2000], Iter [1] Loss: 0.3263 Training Accuracy: 0.85185\n",
      "Epoch [1774/2000], Iter [1] Loss: 0.3211 Training Accuracy: 0.80741\n",
      "Epoch [1775/2000], Iter [1] Loss: 0.3224 Training Accuracy: 0.78519\n",
      "Epoch [1776/2000], Iter [1] Loss: 0.3213 Training Accuracy: 0.82222\n",
      "Epoch [1777/2000], Iter [1] Loss: 0.3317 Training Accuracy: 0.80741\n",
      "Epoch [1778/2000], Iter [1] Loss: 0.3040 Training Accuracy: 0.79259\n",
      "Epoch [1779/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.84444\n",
      "Epoch [1780/2000], Iter [1] Loss: 0.3197 Training Accuracy: 0.82963\n",
      "Epoch [1781/2000], Iter [1] Loss: 0.3220 Training Accuracy: 0.82222\n",
      "Epoch [1782/2000], Iter [1] Loss: 0.3148 Training Accuracy: 0.82222\n",
      "Epoch [1783/2000], Iter [1] Loss: 0.3231 Training Accuracy: 0.80741\n",
      "Epoch [1784/2000], Iter [1] Loss: 0.3234 Training Accuracy: 0.80000\n",
      "Epoch [1785/2000], Iter [1] Loss: 0.3251 Training Accuracy: 0.84444\n",
      "Epoch [1786/2000], Iter [1] Loss: 0.3305 Training Accuracy: 0.85926\n",
      "Epoch [1787/2000], Iter [1] Loss: 0.3251 Training Accuracy: 0.85185\n",
      "Epoch [1788/2000], Iter [1] Loss: 0.3199 Training Accuracy: 0.80741\n",
      "Epoch [1789/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.82222\n",
      "Epoch [1790/2000], Iter [1] Loss: 0.3136 Training Accuracy: 0.80000\n",
      "Epoch [1791/2000], Iter [1] Loss: 0.3341 Training Accuracy: 0.80741\n",
      "Epoch [1792/2000], Iter [1] Loss: 0.3275 Training Accuracy: 0.81481\n",
      "Epoch [1793/2000], Iter [1] Loss: 0.3210 Training Accuracy: 0.83704\n",
      "Epoch [1794/2000], Iter [1] Loss: 0.3209 Training Accuracy: 0.86667\n",
      "Epoch [1795/2000], Iter [1] Loss: 0.3153 Training Accuracy: 0.83704\n",
      "Epoch [1796/2000], Iter [1] Loss: 0.3359 Training Accuracy: 0.82222\n",
      "Epoch [1797/2000], Iter [1] Loss: 0.3113 Training Accuracy: 0.80741\n",
      "Epoch [1798/2000], Iter [1] Loss: 0.3032 Training Accuracy: 0.80741\n",
      "Epoch [1799/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.83704\n",
      "Epoch [1800/2000], Iter [1] Loss: 0.3147 Training Accuracy: 0.82222\n",
      "Epoch [1801/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.80000\n",
      "Epoch [1802/2000], Iter [1] Loss: 0.3146 Training Accuracy: 0.80000\n",
      "Epoch [1803/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.80741\n",
      "Epoch [1804/2000], Iter [1] Loss: 0.3289 Training Accuracy: 0.80741\n",
      "Epoch [1805/2000], Iter [1] Loss: 0.3182 Training Accuracy: 0.82222\n",
      "Epoch [1806/2000], Iter [1] Loss: 0.3004 Training Accuracy: 0.82963\n",
      "Epoch [1807/2000], Iter [1] Loss: 0.3073 Training Accuracy: 0.78519\n",
      "Epoch [1808/2000], Iter [1] Loss: 0.3195 Training Accuracy: 0.82222\n",
      "Epoch [1809/2000], Iter [1] Loss: 0.3240 Training Accuracy: 0.79259\n",
      "Epoch [1810/2000], Iter [1] Loss: 0.3180 Training Accuracy: 0.77778\n",
      "Epoch [1811/2000], Iter [1] Loss: 0.3296 Training Accuracy: 0.85185\n",
      "Epoch [1812/2000], Iter [1] Loss: 0.3127 Training Accuracy: 0.81481\n",
      "Epoch [1813/2000], Iter [1] Loss: 0.3188 Training Accuracy: 0.82963\n",
      "Epoch [1814/2000], Iter [1] Loss: 0.3100 Training Accuracy: 0.80741\n",
      "Epoch [1815/2000], Iter [1] Loss: 0.3324 Training Accuracy: 0.82222\n",
      "Epoch [1816/2000], Iter [1] Loss: 0.3115 Training Accuracy: 0.80000\n",
      "Epoch [1817/2000], Iter [1] Loss: 0.3151 Training Accuracy: 0.78519\n",
      "Epoch [1818/2000], Iter [1] Loss: 0.3285 Training Accuracy: 0.82222\n",
      "Epoch [1819/2000], Iter [1] Loss: 0.3163 Training Accuracy: 0.82222\n",
      "Epoch [1820/2000], Iter [1] Loss: 0.3171 Training Accuracy: 0.79259\n",
      "Epoch [1821/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.83704\n",
      "Epoch [1822/2000], Iter [1] Loss: 0.3080 Training Accuracy: 0.83704\n",
      "Epoch [1823/2000], Iter [1] Loss: 0.3217 Training Accuracy: 0.80000\n",
      "Epoch [1824/2000], Iter [1] Loss: 0.3248 Training Accuracy: 0.78519\n",
      "Epoch [1825/2000], Iter [1] Loss: 0.3120 Training Accuracy: 0.81481\n",
      "Epoch [1826/2000], Iter [1] Loss: 0.3336 Training Accuracy: 0.81481\n",
      "Epoch [1827/2000], Iter [1] Loss: 0.3277 Training Accuracy: 0.80000\n",
      "Epoch [1828/2000], Iter [1] Loss: 0.3136 Training Accuracy: 0.78519\n",
      "Epoch [1829/2000], Iter [1] Loss: 0.3212 Training Accuracy: 0.82222\n",
      "Epoch [1830/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.77037\n",
      "Epoch [1831/2000], Iter [1] Loss: 0.3241 Training Accuracy: 0.85926\n",
      "Epoch [1832/2000], Iter [1] Loss: 0.3213 Training Accuracy: 0.81481\n",
      "Epoch [1833/2000], Iter [1] Loss: 0.3272 Training Accuracy: 0.84444\n",
      "Epoch [1834/2000], Iter [1] Loss: 0.3192 Training Accuracy: 0.79259\n",
      "Epoch [1835/2000], Iter [1] Loss: 0.3226 Training Accuracy: 0.82222\n",
      "Epoch [1836/2000], Iter [1] Loss: 0.3141 Training Accuracy: 0.80741\n",
      "Epoch [1837/2000], Iter [1] Loss: 0.3261 Training Accuracy: 0.85185\n",
      "Epoch [1838/2000], Iter [1] Loss: 0.3238 Training Accuracy: 0.83704\n",
      "Epoch [1839/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.80000\n",
      "Epoch [1840/2000], Iter [1] Loss: 0.3054 Training Accuracy: 0.82222\n",
      "Epoch [1841/2000], Iter [1] Loss: 0.3301 Training Accuracy: 0.80741\n",
      "Epoch [1842/2000], Iter [1] Loss: 0.3241 Training Accuracy: 0.82222\n",
      "Epoch [1843/2000], Iter [1] Loss: 0.3175 Training Accuracy: 0.85185\n",
      "Epoch [1844/2000], Iter [1] Loss: 0.3239 Training Accuracy: 0.80000\n",
      "Epoch [1845/2000], Iter [1] Loss: 0.3123 Training Accuracy: 0.82963\n",
      "Epoch [1846/2000], Iter [1] Loss: 0.3332 Training Accuracy: 0.79259\n",
      "Epoch [1847/2000], Iter [1] Loss: 0.3162 Training Accuracy: 0.78519\n",
      "Epoch [1848/2000], Iter [1] Loss: 0.3194 Training Accuracy: 0.80741\n",
      "Epoch [1849/2000], Iter [1] Loss: 0.3111 Training Accuracy: 0.81481\n",
      "Epoch [1850/2000], Iter [1] Loss: 0.3155 Training Accuracy: 0.80000\n",
      "Epoch [1851/2000], Iter [1] Loss: 0.3199 Training Accuracy: 0.82222\n",
      "Epoch [1852/2000], Iter [1] Loss: 0.3064 Training Accuracy: 0.82963\n",
      "Epoch [1853/2000], Iter [1] Loss: 0.3236 Training Accuracy: 0.82222\n",
      "Epoch [1854/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.82963\n",
      "Epoch [1855/2000], Iter [1] Loss: 0.3049 Training Accuracy: 0.80741\n",
      "Epoch [1856/2000], Iter [1] Loss: 0.3068 Training Accuracy: 0.82222\n",
      "Epoch [1857/2000], Iter [1] Loss: 0.3050 Training Accuracy: 0.82963\n",
      "Epoch [1858/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.82222\n",
      "Epoch [1859/2000], Iter [1] Loss: 0.3261 Training Accuracy: 0.80741\n",
      "Epoch [1860/2000], Iter [1] Loss: 0.3134 Training Accuracy: 0.82963\n",
      "Epoch [1861/2000], Iter [1] Loss: 0.3104 Training Accuracy: 0.84444\n",
      "Epoch [1862/2000], Iter [1] Loss: 0.3249 Training Accuracy: 0.79259\n",
      "Epoch [1863/2000], Iter [1] Loss: 0.3158 Training Accuracy: 0.86667\n",
      "Epoch [1864/2000], Iter [1] Loss: 0.3177 Training Accuracy: 0.80000\n",
      "Epoch [1865/2000], Iter [1] Loss: 0.3179 Training Accuracy: 0.80000\n",
      "Epoch [1866/2000], Iter [1] Loss: 0.3147 Training Accuracy: 0.81481\n",
      "Epoch [1867/2000], Iter [1] Loss: 0.3227 Training Accuracy: 0.82222\n",
      "Epoch [1868/2000], Iter [1] Loss: 0.3172 Training Accuracy: 0.80741\n",
      "Epoch [1869/2000], Iter [1] Loss: 0.3086 Training Accuracy: 0.78519\n",
      "Epoch [1870/2000], Iter [1] Loss: 0.3271 Training Accuracy: 0.82222\n",
      "Epoch [1871/2000], Iter [1] Loss: 0.3119 Training Accuracy: 0.82222\n",
      "Epoch [1872/2000], Iter [1] Loss: 0.3184 Training Accuracy: 0.82963\n",
      "Epoch [1873/2000], Iter [1] Loss: 0.3142 Training Accuracy: 0.80741\n",
      "Epoch [1874/2000], Iter [1] Loss: 0.3049 Training Accuracy: 0.82963\n",
      "Epoch [1875/2000], Iter [1] Loss: 0.3229 Training Accuracy: 0.83704\n",
      "Epoch [1876/2000], Iter [1] Loss: 0.3061 Training Accuracy: 0.82222\n",
      "Epoch [1877/2000], Iter [1] Loss: 0.3169 Training Accuracy: 0.82222\n",
      "Epoch [1878/2000], Iter [1] Loss: 0.3131 Training Accuracy: 0.82222\n",
      "Epoch [1879/2000], Iter [1] Loss: 0.3145 Training Accuracy: 0.82963\n",
      "Epoch [1880/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.83704\n",
      "Epoch [1881/2000], Iter [1] Loss: 0.3121 Training Accuracy: 0.81481\n",
      "Epoch [1882/2000], Iter [1] Loss: 0.3029 Training Accuracy: 0.82222\n",
      "Epoch [1883/2000], Iter [1] Loss: 0.3005 Training Accuracy: 0.84444\n",
      "Epoch [1884/2000], Iter [1] Loss: 0.3176 Training Accuracy: 0.83704\n",
      "Epoch [1885/2000], Iter [1] Loss: 0.3241 Training Accuracy: 0.82963\n",
      "Epoch [1886/2000], Iter [1] Loss: 0.3191 Training Accuracy: 0.86667\n",
      "Epoch [1887/2000], Iter [1] Loss: 0.3172 Training Accuracy: 0.82222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1888/2000], Iter [1] Loss: 0.3259 Training Accuracy: 0.80741\n",
      "Epoch [1889/2000], Iter [1] Loss: 0.3075 Training Accuracy: 0.83704\n",
      "Epoch [1890/2000], Iter [1] Loss: 0.3048 Training Accuracy: 0.83704\n",
      "Epoch [1891/2000], Iter [1] Loss: 0.3157 Training Accuracy: 0.79259\n",
      "Epoch [1892/2000], Iter [1] Loss: 0.3083 Training Accuracy: 0.80741\n",
      "Epoch [1893/2000], Iter [1] Loss: 0.3228 Training Accuracy: 0.81481\n",
      "Epoch [1894/2000], Iter [1] Loss: 0.3051 Training Accuracy: 0.82963\n",
      "Epoch [1895/2000], Iter [1] Loss: 0.3204 Training Accuracy: 0.80741\n",
      "Epoch [1896/2000], Iter [1] Loss: 0.3124 Training Accuracy: 0.78519\n",
      "Epoch [1897/2000], Iter [1] Loss: 0.3086 Training Accuracy: 0.82963\n",
      "Epoch [1898/2000], Iter [1] Loss: 0.3235 Training Accuracy: 0.84444\n",
      "Epoch [1899/2000], Iter [1] Loss: 0.3201 Training Accuracy: 0.82222\n",
      "Epoch [1900/2000], Iter [1] Loss: 0.2965 Training Accuracy: 0.82222\n",
      "Epoch [1901/2000], Iter [1] Loss: 0.3203 Training Accuracy: 0.83704\n",
      "Epoch [1902/2000], Iter [1] Loss: 0.3195 Training Accuracy: 0.80741\n",
      "Epoch [1903/2000], Iter [1] Loss: 0.3092 Training Accuracy: 0.80741\n",
      "Epoch [1904/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.84444\n",
      "Epoch [1905/2000], Iter [1] Loss: 0.3127 Training Accuracy: 0.82222\n",
      "Epoch [1906/2000], Iter [1] Loss: 0.3151 Training Accuracy: 0.80741\n",
      "Epoch [1907/2000], Iter [1] Loss: 0.3190 Training Accuracy: 0.81481\n",
      "Epoch [1908/2000], Iter [1] Loss: 0.3218 Training Accuracy: 0.80000\n",
      "Epoch [1909/2000], Iter [1] Loss: 0.3175 Training Accuracy: 0.82963\n",
      "Epoch [1910/2000], Iter [1] Loss: 0.3081 Training Accuracy: 0.84444\n",
      "Epoch [1911/2000], Iter [1] Loss: 0.3095 Training Accuracy: 0.84444\n",
      "Epoch [1912/2000], Iter [1] Loss: 0.3174 Training Accuracy: 0.80741\n",
      "Epoch [1913/2000], Iter [1] Loss: 0.3140 Training Accuracy: 0.82963\n",
      "Epoch [1914/2000], Iter [1] Loss: 0.3025 Training Accuracy: 0.80741\n",
      "Epoch [1915/2000], Iter [1] Loss: 0.3080 Training Accuracy: 0.81481\n",
      "Epoch [1916/2000], Iter [1] Loss: 0.3102 Training Accuracy: 0.82222\n",
      "Epoch [1917/2000], Iter [1] Loss: 0.2913 Training Accuracy: 0.77778\n",
      "Epoch [1918/2000], Iter [1] Loss: 0.3071 Training Accuracy: 0.80000\n",
      "Epoch [1919/2000], Iter [1] Loss: 0.3067 Training Accuracy: 0.84444\n",
      "Epoch [1920/2000], Iter [1] Loss: 0.3074 Training Accuracy: 0.83704\n",
      "Epoch [1921/2000], Iter [1] Loss: 0.3086 Training Accuracy: 0.80741\n",
      "Epoch [1922/2000], Iter [1] Loss: 0.3160 Training Accuracy: 0.83704\n",
      "Epoch [1923/2000], Iter [1] Loss: 0.3035 Training Accuracy: 0.85926\n",
      "Epoch [1924/2000], Iter [1] Loss: 0.3192 Training Accuracy: 0.84444\n",
      "Epoch [1925/2000], Iter [1] Loss: 0.3069 Training Accuracy: 0.79259\n",
      "Epoch [1926/2000], Iter [1] Loss: 0.3027 Training Accuracy: 0.85926\n",
      "Epoch [1927/2000], Iter [1] Loss: 0.3130 Training Accuracy: 0.78519\n",
      "Epoch [1928/2000], Iter [1] Loss: 0.3072 Training Accuracy: 0.82963\n",
      "Epoch [1929/2000], Iter [1] Loss: 0.3057 Training Accuracy: 0.84444\n",
      "Epoch [1930/2000], Iter [1] Loss: 0.3269 Training Accuracy: 0.79259\n",
      "Epoch [1931/2000], Iter [1] Loss: 0.3196 Training Accuracy: 0.85185\n",
      "Epoch [1932/2000], Iter [1] Loss: 0.3145 Training Accuracy: 0.80000\n",
      "Epoch [1933/2000], Iter [1] Loss: 0.3041 Training Accuracy: 0.81481\n",
      "Epoch [1934/2000], Iter [1] Loss: 0.3171 Training Accuracy: 0.87407\n",
      "Epoch [1935/2000], Iter [1] Loss: 0.3140 Training Accuracy: 0.83704\n",
      "Epoch [1936/2000], Iter [1] Loss: 0.3117 Training Accuracy: 0.80000\n",
      "Epoch [1937/2000], Iter [1] Loss: 0.3088 Training Accuracy: 0.80741\n",
      "Epoch [1938/2000], Iter [1] Loss: 0.2986 Training Accuracy: 0.85185\n",
      "Epoch [1939/2000], Iter [1] Loss: 0.3171 Training Accuracy: 0.80741\n",
      "Epoch [1940/2000], Iter [1] Loss: 0.3097 Training Accuracy: 0.82222\n",
      "Epoch [1941/2000], Iter [1] Loss: 0.3265 Training Accuracy: 0.81481\n",
      "Epoch [1942/2000], Iter [1] Loss: 0.3129 Training Accuracy: 0.82222\n",
      "Epoch [1943/2000], Iter [1] Loss: 0.3120 Training Accuracy: 0.87407\n",
      "Epoch [1944/2000], Iter [1] Loss: 0.3112 Training Accuracy: 0.88148\n",
      "Epoch [1945/2000], Iter [1] Loss: 0.3242 Training Accuracy: 0.85185\n",
      "Epoch [1946/2000], Iter [1] Loss: 0.3231 Training Accuracy: 0.81481\n",
      "Epoch [1947/2000], Iter [1] Loss: 0.3065 Training Accuracy: 0.82222\n",
      "Epoch [1948/2000], Iter [1] Loss: 0.3108 Training Accuracy: 0.85185\n",
      "Epoch [1949/2000], Iter [1] Loss: 0.3188 Training Accuracy: 0.85185\n",
      "Epoch [1950/2000], Iter [1] Loss: 0.3262 Training Accuracy: 0.83704\n",
      "Epoch [1951/2000], Iter [1] Loss: 0.3107 Training Accuracy: 0.86667\n",
      "Epoch [1952/2000], Iter [1] Loss: 0.3188 Training Accuracy: 0.84444\n",
      "Epoch [1953/2000], Iter [1] Loss: 0.3012 Training Accuracy: 0.82222\n",
      "Epoch [1954/2000], Iter [1] Loss: 0.3156 Training Accuracy: 0.79259\n",
      "Epoch [1955/2000], Iter [1] Loss: 0.3147 Training Accuracy: 0.82222\n",
      "Epoch [1956/2000], Iter [1] Loss: 0.3150 Training Accuracy: 0.81481\n",
      "Epoch [1957/2000], Iter [1] Loss: 0.3094 Training Accuracy: 0.80000\n",
      "Epoch [1958/2000], Iter [1] Loss: 0.3103 Training Accuracy: 0.80000\n",
      "Epoch [1959/2000], Iter [1] Loss: 0.3087 Training Accuracy: 0.84444\n",
      "Epoch [1960/2000], Iter [1] Loss: 0.3075 Training Accuracy: 0.82963\n",
      "Epoch [1961/2000], Iter [1] Loss: 0.3087 Training Accuracy: 0.83704\n",
      "Epoch [1962/2000], Iter [1] Loss: 0.3171 Training Accuracy: 0.80741\n",
      "Epoch [1963/2000], Iter [1] Loss: 0.3030 Training Accuracy: 0.80741\n",
      "Epoch [1964/2000], Iter [1] Loss: 0.3072 Training Accuracy: 0.82963\n",
      "Epoch [1965/2000], Iter [1] Loss: 0.3045 Training Accuracy: 0.83704\n",
      "Epoch [1966/2000], Iter [1] Loss: 0.3021 Training Accuracy: 0.82222\n",
      "Epoch [1967/2000], Iter [1] Loss: 0.3165 Training Accuracy: 0.80741\n",
      "Epoch [1968/2000], Iter [1] Loss: 0.3200 Training Accuracy: 0.85185\n",
      "Epoch [1969/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.83704\n",
      "Epoch [1970/2000], Iter [1] Loss: 0.3094 Training Accuracy: 0.81481\n",
      "Epoch [1971/2000], Iter [1] Loss: 0.3029 Training Accuracy: 0.84444\n",
      "Epoch [1972/2000], Iter [1] Loss: 0.3106 Training Accuracy: 0.82963\n",
      "Epoch [1973/2000], Iter [1] Loss: 0.3019 Training Accuracy: 0.83704\n",
      "Epoch [1974/2000], Iter [1] Loss: 0.3041 Training Accuracy: 0.82963\n",
      "Epoch [1975/2000], Iter [1] Loss: 0.3039 Training Accuracy: 0.83704\n",
      "Epoch [1976/2000], Iter [1] Loss: 0.3056 Training Accuracy: 0.80000\n",
      "Epoch [1977/2000], Iter [1] Loss: 0.3014 Training Accuracy: 0.78519\n",
      "Epoch [1978/2000], Iter [1] Loss: 0.3150 Training Accuracy: 0.80000\n",
      "Epoch [1979/2000], Iter [1] Loss: 0.3099 Training Accuracy: 0.85185\n",
      "Epoch [1980/2000], Iter [1] Loss: 0.3034 Training Accuracy: 0.82222\n",
      "Epoch [1981/2000], Iter [1] Loss: 0.3117 Training Accuracy: 0.84444\n",
      "Epoch [1982/2000], Iter [1] Loss: 0.3067 Training Accuracy: 0.80000\n",
      "Epoch [1983/2000], Iter [1] Loss: 0.3085 Training Accuracy: 0.82222\n",
      "Epoch [1984/2000], Iter [1] Loss: 0.3112 Training Accuracy: 0.77037\n",
      "Epoch [1985/2000], Iter [1] Loss: 0.3090 Training Accuracy: 0.85185\n",
      "Epoch [1986/2000], Iter [1] Loss: 0.3076 Training Accuracy: 0.82963\n",
      "Epoch [1987/2000], Iter [1] Loss: 0.3092 Training Accuracy: 0.80741\n",
      "Epoch [1988/2000], Iter [1] Loss: 0.2982 Training Accuracy: 0.80741\n",
      "Epoch [1989/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.81481\n",
      "Epoch [1990/2000], Iter [1] Loss: 0.3185 Training Accuracy: 0.80741\n",
      "Epoch [1991/2000], Iter [1] Loss: 0.3024 Training Accuracy: 0.82963\n",
      "Epoch [1992/2000], Iter [1] Loss: 0.3208 Training Accuracy: 0.85185\n",
      "Epoch [1993/2000], Iter [1] Loss: 0.3051 Training Accuracy: 0.82222\n",
      "Epoch [1994/2000], Iter [1] Loss: 0.3184 Training Accuracy: 0.82963\n",
      "Epoch [1995/2000], Iter [1] Loss: 0.3088 Training Accuracy: 0.82963\n",
      "Epoch [1996/2000], Iter [1] Loss: 0.2947 Training Accuracy: 0.82963\n",
      "Epoch [1997/2000], Iter [1] Loss: 0.3051 Training Accuracy: 0.84444\n",
      "Epoch [1998/2000], Iter [1] Loss: 0.3154 Training Accuracy: 0.80000\n",
      "Epoch [1999/2000], Iter [1] Loss: 0.2967 Training Accuracy: 0.81481\n",
      "Epoch [2000/2000], Iter [1] Loss: 0.3143 Training Accuracy: 0.83704\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "loss1 = []\n",
    "train_acc = []\n",
    "\n",
    "Epoch = 2000\n",
    "\n",
    "for epoch in range(Epoch):\n",
    "    acc = 0\n",
    "    \n",
    "    for i, (features, labels) in enumerate(trainloader):\n",
    "        features = Variable(features)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        features = features.float()\n",
    "        \n",
    "        outputs = model(features)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % len(trainloader) == 0:\n",
    "            Ypred = predict(model, torch.from_numpy(X_train).float())\n",
    "            acc = np.mean(Y_train == Ypred)\n",
    "            \n",
    "            train_acc1 = acc/len(trainloader)\n",
    "            train_acc.append(train_acc1)\n",
    "            loss1.append(loss.data)\n",
    "            \n",
    "            print ('Epoch [%d/%d], Iter [%d] Loss: %.4f Training Accuracy: %.5f' %(epoch+1, 2000, i+1, loss.data, train_acc1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the accuracies and loss functions below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_loss = loss1[0].numpy()\n",
    "\n",
    "for i in range(len(loss1)):\n",
    "    np_loss = np.append(np_loss, loss1[i])\n",
    "\n",
    "np_acc = train_acc[0]\n",
    "\n",
    "for i in range(len(train_acc)):\n",
    "    np_acc = np.append(np_acc, train_acc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de7xUdb3/8ddHEG+AiIAScrPwgubxsiPxXpmBBShaQcpRUzl5RE2rX15+2kmzi/7SY6l5y8wrlh6SCiUzL2mSbLwjoQgmiBdQUkBhs+Hz++O75syamTWzZ/aey94z7+fjMY+11nd911qfWbPns7/zXTdzd0REpOvbrNYBiIhIeSihi4jUCSV0EZE6oYQuIlInlNBFROqEErqISJ1QQpdOw8y6mdkaMxtSzroijUIJXdotSqip1yYz+yg2fVyp63P3je7e091fL2fd9jKzU8zMzWxipbYhUk6mC4ukHMzsNeAUd/9zgTrd3b21elF1jJn9FRgJPO7uE6q87W7uvrGa25SuTy10qRgz+4GZ3W1md5nZauB4MxttZnPM7F9m9qaZ/czMNo/qd49axMOi6duj+feb2Woze9LMhpdaN5o/1sxeNrP3zeznZvaEmZ1YIPadgQOB/wDGmln/rPkTzexZM/vAzBaZ2RFR+fZmdkv03laZ2b1R+Slm9khs+aT4rzGzB8xsLXCwmY2PtrHazF43swuzYjgk2pfvm9lSM5sS7d/lZrZZrN5Xzay5hI9OuigldKm0o4E7gW2Bu4FW4CygHyFhjiEkzXy+BlwI9AVeBy4pta6ZDQB+A3wn2u4SYFQbcZ8AzHH3e4BXgcmpGWZ2AHAz8C2gD/AZ4J/R7DuBHoSW/Q7AVW1sJzv+7wO9gCeBNcDxhH03DjjLzL4UxTAc+CNwBbA9sA/wgrs/CawGPhdb7/HAbSXEIV2UErpU2uPu/nt33+TuH7n7XHf/u7u3uvti4Abg0ALL3+Puze6+AbgD2Lsddb8EPOvu90XzrgRW5luJmRkwhZCciYYnxKqcDNzo7g9F72upuy80s8GERHqau69y9xZ3f6xAvNlmuPuT0TrXu/tf3P3FaPo5YDrpfXU88IC7/ybalyvd/dlo3q3RfMysXxTTXSXEIV2UErpU2tL4hJntZmZ/NLO3zOwD4GJCqzmft2LjHwI921H3Y/E4PBw4WlZgPYcAgwmteggJfV8z2zOaHkxotWcbDKx09/cLrLuQ7H012sweMbMVZvY+cArpfZUvBgit8aPMbGtgEvCwu7/TzpikC1FCl0rLPup+PfAi8Al37w1cBFiFY3gT2Ck1EbXABxWofwLhu/G8mb0FPEF4H/8ezV8KfDxhuaVAPzPrnTBvLbB1bHrHhDrZ+2o6cC8w2N23BW4iva/yxUB05k8zMIHwS0PdLQ1CCV2qrRfwPrDWzHancP95ufyB0MIeZ2bdCX34/ZMqRq3aYwndKnvHXmcTDup2A34JnGJmnzGzzcxsJzPb1d2XAn8GrjGzPma2uZkdEq36OWAvM/ukmW0FfK+IuHsB77n7OjPbn9DaTrkdGGNmx0QHWPuZ2b/F5t8KnAfsBtxXxLakDiihS7V9i9ACXk1ord9d6Q26+9vAVwkHEN8ltGyfAdYnVJ8YxXa7u7+VegE3AlsBn3f3vwGnAj8j/HN6mNAFAlHfNfAy8DZwRhTDS8APgUeAhUAxfeunAT+KzhA6n3QXEO6+hHCg9LvAe8DTwCdjy94L7Ew4rvBREduSOqDz0KXhRK3s5cCx7v7XWsdTCVG30hLgRHd/pMbhSJWohS4NwczGmNm2ZrYF4dTGVuCpGodVSV8h/AJ5tNaBSPV0r3UAIlVyEOFUxh7AfOAod0/qcunyzOxxYARwnOsneENRl4uISJ1Ql4uISJ2oWZdLv379fNiwYbXavIhIlzRv3ryV7p542m3NEvqwYcNobtb9gkRESmFm/8w3r6gul+gMgYXRXeXOTZg/1MweMrPno0uVd0paj4iIVE6bCT06Z/caYCzhDnKTzWxkVrX/B9zq7nsR7s3xo3IHKiIihRXTQh8FLHL3xe7eQri/RPbN/kcCD0XjDyfMFxGRCismoQ8i8y5wy8i9sdFzwDHR+NFALzPbPntFZjbVzJrNrHnFihXtiVdERPIoJqEn3Qkv++T1bwOHmtkzhPs1v0G4Ei9zIfcb3L3J3Zv69088SCsiIu1UzFkuy0jfeAjCbUiXxyu4+3LCTY0ws57AMR24J7SIiLRDMS30ucAIMxtuZj0It/CcGa8Q3bozta7zCI/nEhGRKmozoUdPaZ8GzAYWAL9x9/lmdrGZjY+qHQYsNLOXCc9RvLRC8YqIdFovvwwPPdR2vUqp2b1cmpqaXBcWiUg9seiIYyXTqpnNc/empHm6l4uISBn8/ve1jkAJXUSkw+bOhfHj265XaUroItJQ1qyBv/2tuLrPPAMrV6aXe/LJ5HqrVuUuVwtK6CLSUKZMgQMPTCfqQvbdF5qi3urjj4cDDoCkayI32yx3uVpQQheRili5EpYsqf5216yBBQuS561YAb/7XRhfuzZz3uuvw9tv5y7zz+jehvPmheFHWY/cXrIE/vSn9sdbTnoEnYhUxJAhIflV+0S6cePgkUdg06b0WScphR7BMHRoGOaLN98ZLDvvnFzfPXf7laYWuoiURUtLZos8uyXbHu++G17r16dbym155JEwfPXVdNkHH8Bbb8GHH6bLWltD2QcfZC6/YEFy7KlulU2bioujtRWWLg3b/Mc/YNEieOWV4pZtLyV0ESmLb34ztFbffbd86+zXL7xOPjm0ruMJuS0jRqQPVu62GwwcmDm/pSWUfeITmeUjR8LEibnrS7W2i03oGzaEXynbbAO77x7i2WWX/AdWy0EJXaSBrVsH//pX4TqrV4fXO+/kznvvvdB6Bnj00TB8883SY4ifJfL22yFpxpP3nXeGYVsJPbu1/cQToVWcFFNLSxgmHeR84IHcstR+WrMGli/P3+ee8vrryeV//3v+ZTpKCV2kgY0aBdttV7hO797htcMOuUl9++3T51/37h2G778fEl6xDj0U+vYN42+/DTvuCBddFFq1Kal+640bC69r220zp8eNC63iJKl/RPncf3/mdCqh77cfDBoU+tx33DH/8vH4484+u/B2O0IJXaQOrF+fbnHGbdhQOHG98EJ6PPusjyTz56e7HFL1U2d4bL55GLa2Zp5l8vrrYZkVK9KJee3aML52LTz1VPo9pLprfv3r5BbuunWhf/uDD8KyGze2v4tn9er0eFIr/Ykn0uPxfdPWP5VivF+he9EqoYvUgd6902dpxI0cCVtu2fbyzz8PPXvC3XcXrvfZz4bW8/TpoX7K9OnQrVsYnz8fDj88PW/o0DBvwAC44orQCu/ZM7RU4+vYdlvYY48wvmxZ8vaHDYOttw51e/aE7t1DH3tSF0lb4jEOGJA7P/4PMh5nOaS6kMpNCV2kDrS0hDM24tzDmRVJsk+9e/rpMJw1q+1t3XZb7n1LZs5MJ/RUizvJjTemzz656qrMeW11gRTyl7+0f9kkm23WsXjaUsw/2fZQQhepETM45ZTMsi99KZSvXx+G3/9+6eu89lo47bTMqxdXrgzzzMKZKJttltlHfNJJYXjrrbD//qHemWcmn0f9+uu5Lcy77kon9F//On98LS3hKs1yu/zy8q5v0yb42c/Ku864LbaozHqV0EVq6Je/zJz+4x/DMHW2xlVXZbami+m/veEGuO66zLKlsacCp84Vv+uu5OVTZ2H8/OdtbyuuexGXKZbj3PR8ytG3XS2p4w3lpoQuUmYXXJBu2c6YEcZTLe9DD01u9Y4alVme6vpYtSq0plOt6x12gPvuS0+bhT7luOeey11/qvUcd9tt7Xt/+bz0Utt1sruFyqmYfyidhRK6SBfxwx+G4caN8KMfhfFUy/uxx9Lz4ubOzZy+/vrkdb/7Llya9TywYlq91bgE/bXXKr+NeqGELhKZPDmcbdERp54a7pyX7e67Q/IbMgTOOy+Uvf9+KLvvvtBdEW8dx1/Zt2RduzZ/so23JpOSbaGrCbOTfzH22qv0ZaRyKvVrQgldupzp0+Hhhzu2jptuSk6aF1wQhkuXwo9/HMZT51RfeinMnp1/nddemzm9Zk1pl6pL55bvAqVsxRzwrGkL3czGmNlCM1tkZucmzB9iZg+b2TNm9ryZHVn+UKXejB1b2lkcr70WrkzMluqnNss8c+PEEzNb0AcdFFref/5zus7HPx7Oad5/f7j33swbOkFYbvToMD53bjh7JJ877shsbQ8aBIsXF//+pHP7P/+nuHqpbrZCKtbf7+4FX0A34FVgZ6AH8BwwMqvODcBp0fhI4LW21rvffvu5NLZw/kbx9S+9NL1MfLk+fdJlu++eu/7s1yc/mVw+YED+ZfTSa9Om4uo99VR6/OCDk+s88URHvjc0uyfn1WJa6KOARe6+2N1bgOnAhOz/C0B0Jwe2BUq4k4PUozffhL33DucHH3NMaF1/8pPpmxl9+cvpurfcApMmhVP4pkwJXRxm4SfuYYfBUUeFu+WlukNSzMKVhfGbS6WWLXQQMH65e1zSzafqjXutI+iahg4t/sByvN5DD2XOSx23qdSFRYlZPv4CjgVuik1PAa7OqjMQeAFYBqwC9suzrqlAM9A8ZMiQ9v+Lkk4vuzV91llheMUVYX6hFs5Xv1r71lhXe519dnH12tr39fY6/PDcsuOPd1+woO1lJ09Ojw8alLvvfvCD3GUefti9uTn//l6yxP2SS0Jrv70o0EJPLMyoAF9OSOg/z6pzDvCtaHw08BKwWaH1qsul89iwIfzxPvOM+6OPup98cviD+/BD96OOcn/hBfeJE90XL04v8+GH6a6L3XbLXN9VV+X+offtW/svd1d7Je3HpNfHPhb2ezF1i6n37W/X/r2X8jrooPzzrrwyeR8Usx+uuy49vuOOucskrcM9fI/AfdSo3DobNnT8+9rRhD4amB2bPg84L6vOfGBwbHoxMKDQepXQO49Ua2WXXdJ/eC0t7jNnZv4xjh+fXubee5O/JO61/4J3ltdXvpJcPnBgGN5+u/uhh7r/x38k17vssuK2s912Yb/HExC4T5rkfthhuZ9TfPqb38ycvvBC9/fec//CF3K3M3Gi+5FHFo5l113DP+/Zs8P0HXfk1nn8cffvfKe497bVVunxzTdPTt5z5uSW7b67+7RpoeERLz/99Lb/TvfYIwznzXM/8cQw3q9fWOYPf8i/L3v3DmUbN4b3t2RJbp2NGzv+fe1oQu8eJejhpA+K7pFV537gxGh8d0IfuhVarxJ69bzwQvhJnu9n3oMP5v5R33Zb8pf6jTfyfxH23LO4L2mjvJ57LnM6lbivuy73M0hKEj/8YXHb2Wab3PWkEtdLL2XWjdfJno5bsiR3O0mxZq/vmmvyv7dCyyW9+vVz//Ofw/hnPxuWmTgxt15LS27ZgQcmb+eee9re/gEHhOHf/hb+uUH6n2a+zwrcp07Nfe/ZdTrS1ZJeXwcSelieI4GXCWe7XBCVXQyMj8ZHAk9Eyf5Z4Ii21qmEXj2DB4dP+o03kuf37198kjr99I4luUq/Roxw32+/tuv98pf5582Zk26ZpV4HHxy6lrLrHn105vSMGenE0drqfsop6Xlvv+0+ZYr7mjW5n8EDD4TjDu7u55/v3quX+6pV6X+2r76aXs+3vhWGZmHYs2d6PSNGhLLTTgvTxST0O+7ITcSlJPRbb02X/epXue+trfXke22/vfuf/hTGDz88LLNsWXr+IYeEz2XTplAX3H/847DPFy1K3s66denyxx5Ll3/qU+nxV191//rXwz+KTZvC8Z9589LLff/74XOJr3vKlPD5Jkn9Wom/947ocEKvxEsJvXpSCf3MMzPLly9PJ4eu/or/lE4p1Af90UehTr5kkz3v9ttDV0F2/UcfdZ8wIYz/z/8k7/9yfZmz15P6tTRgQLrs+utDWaq1GE/o3bvnvq98Xn45zP/EJ3LrJi07fnz+fdDehN63r/v994fxI44ovP1C8r3fdetCWY8e7VtvKctUK6F3odvZSHulrkr72c8y70F9yinF3f+6Mxs4MJwieW7O5W5w/PFw1lmw666wcGEomzEjXGmauprv2mvD49IGDMh8riWEe35ffnm4sdW4cck3vRo1Ci67LNwd8fOfT47xjDPgU59q/3tMmToVjjgiPb3DDuH0z/gjzbKfTB9/AHLqIRDnn5++30w+w4eHR8tdcAH84hcwIXai8tVX5z6j8/LLw61x4/GlnH8+fOxjMG1a4W2OHp159a47HHIIfO5zcOWV6fKf/rS0K3DPOSc8WON3v8ss79EDjj0WvvGN4teV7YorwhXBnUa+TF/pl1ro5ffb37rPmuV+8cXh537KrrumWwhTp1a/9dze17hxme8vqU41Pfpo5raT+sJr7eabQ2wnnVS4Xi32X6EW+gMP5Jb16VPb+DrruunghUXSRXz5y3DkkeERYfEnysTvG3HDDeXZ1te+1vF1ZN81EEJr+EtfCuOXXJI5b8qU0LL8ylfC9C23dDyGUuy7b2jt33JLaPkefXR1t1+McePCAyy+/e226119dXViSpkyJfMzj19K7x6Gv/oVfOYz4UHMt95a3fgOOaRy27z9djj44MqsO848tSerrKmpyZubm2uy7Xrzr3+FqyrjD7rdeutwv4jUgxLKzb3wlXPu4WfxNtuky047LXRxjB4Nc+aEuxNedFG4t8rs2ck/16W+jRkTPvtZs8K9faRtZjbP3ZuS5qmFXgfGj899avmHH1YumU+ZEoannppZftRRYXjmmWG45Zah7/TSS2GrrUIfMIQbcm29dbhs/3vfC0l/v/0qE6t0buefH/qyR42qdST1QS30Lup3v4N168IXIvVIsY54+unwU71Pn+T5qYOPEB4IPG4cvPhiuD8LwMSJ4W6FIlJZhVroOsulC1q7tvz9t1tuGVrR+fToEW49+/rr0BT9KQ0dmp7/n/9Z3nhEpHRK6J3YmjWhBf6jH4XW8fz54RFkq1eXZ/0nnRQOQkE4ja/QTfd79ICXX84s69UrfTBLRGpPCb0TGz06dGvMm5f7eLNymDw5ndC33DJ9kHPffUMXzJgx8OCD4fmX55xT/u2LSHkpoXcyixfDypWha+PFF0NZR5L57bfDcceFpGyWfvr7AQeEC2H69QvbSz1BRS1uka5LZ7l0Mh//OHz605kPgChGvisRP/3pMOzWLVxFmJo+7rgwPP30MOzVq/RYRaRzUULvJNatC63p9pozJ1y6fscdYfqrXw2XYscv/QZ4/PHwFPvUszG/9z3YsKHwAVER6RrU5dJJHHwwtPcszgMOCK3vPn1gr71C2dixyQc5u3eH3r3T02YVfGCtiFSVvso1tHIlvPEGtLa2L5kvXx66Snr0SJftuWdoqec7n1xE6pcSeg3179/+ZQ8+OFzsk0TJXKQxqQ+9yjZsCOeSl2rnndPjo0aF+1+IiMQpoVfZpEnhVMFSnXhienz8eB3EFJFcupdLFa1dCz17hvGNG9PnhCdJnR+e0toartTctAl23z39IAMRaSy6l0sn8OyzsM8+6ek99ihcPzvZd+sWErmISD5FtfPMbIyZLTSzRWaW87AvM7vSzJ6NXi+b2b/KH2rXlv1j5B//SK63xRawYEE4hxzCY+NeeaWysYlIfWizhW5m3YBrgM8Dy4C5ZjbT3V9K1XH3s2P1zwD2yVmRFOX222G33UJr/i9/ga9/PfMhESIi+RTTQh8FLHL3xe7eAkwHJhSoPxm4qxzBNYIvfjE9PmsWHHNMGL/3XnjsMSVzESleMQl9ELA0Nr0sKsthZkOB4cBf8syfambNZta8IvsRO3VkxoxwBWb8lf10n5Q//CE9PnZs+o6HffpU5xmEIlI/iknoSU+OzHdqzCTgHnffmDTT3W9w9yZ3b+rfkatqOrlrry2u3vHHVzYOEWksxST0ZcDg2PROwPI8dSeh7paCpyOmnlgP6WdwioiUQzGnLc4FRpjZcOANQtL+WnYlM9sV2A54sqwRdkGFEvpNN4UHOO+yS/oxcuorF5FyaDOhu3urmU0DZgPdgJvdfb6ZXQw0u/vMqOpkYLrX6kqlTmTWrPzzevWC3/8+s0x95SJSDkVdWOTus4BZWWUXZU3/V/nCqk/XX1/rCESknukC8jL5wQ/gO98J3Sn5HHpo9eIRkcajS//L5MILw/Dkk5PnT54c+s1FRCpFCb0M7rknPZ7vfit33lmdWESkcanLpQzyPdB59OjqxiEijU0JvQOuvDL/6YYPPghPPFHdeESksanLpQPOOSf/vKamcBn/hRfqYKiIVIcSeoWknut58cW1jUNEGoe6XCrgjjtqHYGINCIl9AJuuin3QRR/+xscckj+lvegQfC1nBsjiIhUnrpcCjj11PAEoXXr0mUHHhiGf/1r8jK/+EXl4xIRSaIWehvWr0+PF3OXmvgDK0REqkkJPY+k5P3SS7llAN1jv3M20x4VkRpRl0seGxMe0ZEvWW/YUNlYRESKofZkHkkJPanVftJJlY9FRKQYaqHnsWlTevyRR2D16uQHV0yZUrWQREQKUkLPI95C/8xn8tfbbbfKxyIiUoy6T+jz50O/frDDDqUtl9Tlkk3PZhKRzqTuE/qee0LPnqHLpBTFJHQRkc6kqIOiZjbGzBaa2SIzOzdPna+Y2UtmNt/MOtXdv9esKX2ZeB96trlzdWaLiHQ+bbbQzawbcA3weWAZMNfMZrr7S7E6I4DzgAPdfZWZDahUwNVSqIU+ZEjmueciIp1BMS30UcAid1/s7i3AdGBCVp1TgWvcfRWAu79T3jAr46238rfely9PLn/vPRjQ5f9diUg9KiahDwKWxqaXRWVxuwC7mNkTZjbHzMYkrcjMpppZs5k1r1ixon0Rl9HAgeG+5Un23Te5fLvtKhePiEhHFJPQLaEs+/yO7sAI4DBgMnCTmfXJWcj9Bndvcvem/v37lxprSTZtgkWLMstaWmDVqsyyhQvbXtcZZ5QvLhGRSikmoS8DBsemdwKyOySWAfe5+wZ3XwIsJCT4mrnoIhiRFcGXvwx9+5a+rgceKE9MIiKVVExCnwuMMLPhZtYDmATMzKrzO+AzAGbWj9AFs7icgZZqxozcsplR1O7w0Ue581tawtkrLS2Z5a+8Am++Ce90iSMDItKo2jxXw91bzWwaMBvoBtzs7vPN7GKg2d1nRvOOMLOXgI3Ad9z93UoG3hEtLbD11rnlvXuHA55Ll2aW/9u/wY47Vic2EZH2KurkO3efBczKKrsoNu7AOdGrUyh0FeeHHyaXr1+fm8zPOgt++MPyxSUiUikNczZ1PMFnd7dY0mHfyNlnJ7fmRUQ6m4a5fW78QqH4U4ja0qNH+WMREamEukzora2wYEFm2eabp8cPOqj4dW2xRXliEhGptLpM6G21wPNdBZpELXQR6SrqMqGX87a2Sugi0lXUZUJvbS3fuuJdNSIinVldJvRLLinfugqdASMi0pnUZUK/4oqOLX/++eWJQ0SkmuoyoXfEqFFw6aW1jkJEpHQNc2FRse65Jwyfegqee662sYiIlEIJPcvg6L6Sn/pUeImIdBUN2+UycGC44daoUemyq6+uXTwiIh1Vdwn9ySeTy489NnP6+uvDLXGPOy5Mn3EGnH56ZWMTEamkuutyOeCA5PItt8yc7tUrDL/+9fBP4MILKxuXiEil1V1Cz2errTKne/ZMD++6q/rxiIiUW911ueSTndB32aU2cYiIVErDJPTsuyb27l2bOEREKqVhErpusiUi9a6uEnqhuyzqJlsiUu+KSuhmNsbMFprZIjM7N2H+iWa2wsyejV6nlD/Uts2enX9ePKGfcELlYxERqbY2z3Ixs27ANcDngWXAXDOb6e4vZVW9292nVSDGoq1cmX9eKqFPnAi33FKVcEREqqqYFvooYJG7L3b3FmA6MKGyYbVPofugH310GE6dWp1YRESqrZjz0AcBS2PTy4BPJ9Q7xswOAV4Gznb3pdkVzGwqMBVgyJAhpUfbhqTnf8b71cv5JCMRkc6mmBZ60iMeslPj74Fh7r4X8Gfg10krcvcb3L3J3Zv69+9fWqRF0JksItLIiknoy4DBsemdgIzHLLv7u+6eejTzjcB+5QlPRESKVUxCnwuMMLPhZtYDmATMjFcws4GxyfHAgvKFWDx1qYhII2uzD93dW81sGjAb6Abc7O7zzexioNndZwJnmtl4oBV4DzixgjEXiLUWWxUR6RyKujmXu88CZmWVXRQbPw84r7yhlU4JXUQaWV1dKbppU60jEBGpnbpK6Gqhi0gjU0IXEakTSugiInVCCV1EpE4ooYuI1Im6Sug6y0VEGlldJXS10EWkkRV1YVFXkUrov/kNfOxj0LdvbeMREammukzoo0bB0KG1jUVEpNrqssvFkm74KyJS5+oqoX/3u2G4WV29KxGR4tRV6nv33TBUC11EGlFdJfQUJXQRaURK6CIidUIJXUSkTtRlQtdBURFpRHWZ+tRCF5FGpIQuIlInikroZjbGzBaa2SIzO7dAvWPNzM2sqXwhlk4JXUQaUZsJ3cy6AdcAY4GRwGQzG5lQrxdwJvD3cgdZKiV0EWlExbTQRwGL3H2xu7cA04EJCfUuAS4D1pUxvnbRQVERaUTFpL5BwNLY9LKo7H+Z2T7AYHf/Q6EVmdlUM2s2s+YVK1aUHKyIiORXTEJP6sD43zuPm9lmwJXAt9pakbvf4O5N7t7Uv3//4qMsQvzhFrovuog0omIS+jJgcGx6J2B5bLoXsCfwiJm9BuwPzKz2gdF4QteTi0SkERWT0OcCI8xsuJn1ACYBM1Mz3f19d+/n7sPcfRgwBxjv7s0ViTgPtdBFpNG1mdDdvRWYBswGFgC/cff5ZnaxmY2vdIDF2rgxPb7ttrWLQ0SkVop6YpG7zwJmZZVdlKfuYR0Pq3SpFvpll8Hmm9ciAhGR2qqbE/xSCV2nLIpIo6qb9JfqclFCF5FGVTfpr6UlDHv0qG0cIiK1UjcJ/Y03wrBv39rGISJSK3WT0OfNC0MldBFpVHWT0FP22KPWEYiI1EbdJPTW1jDsXtSJmCIi9aduEvqGDWGohL9l1k4AAAmlSURBVC4ijapuEnqqha6LikSkUdVNQn/rrTBUC11EGlVdJPQZM8Il/6CELiKNqy4S+qOPpseV0EWkUdVFQr/qqvR4t261i0NEpJbqIqGLiIgSuohI3VBCFxGpE0roIiJ1QgldRKROKKGLiNSJohK6mY0xs4VmtsjMzk2Y/w0ze8HMnjWzx81sZPlDFRGRQtpM6GbWDbgGGAuMBCYnJOw73f2T7r43cBlwRdkjFRGRgoppoY8CFrn7YndvAaYDE+IV3P2D2OQ2gJcvRBERKUYxF8oPApbGppcBn86uZGanA+cAPYDPJq3IzKYCUwGGDBlSaqwiIlJAMS10SyjLaYG7+zXu/nHgu8D/TVqRu9/g7k3u3tS/f//SIi3CT39a9lWKiHQZxST0ZcDg2PROwPIC9acDR3UkqPaaNKkWWxUR6RyKSehzgRFmNtzMegCTgJnxCmY2Ijb5ReCV8oVYvI0ba7FVEZHOoc0+dHdvNbNpwGygG3Czu883s4uBZnefCUwzs8OBDcAq4IRKBh339NPpcSV0EWlk5l6bE1Kampq8ubm5w+uxWA//xo2wmS6VEpE6Zmbz3L0paV7dpL9p05TMRaSx1U0KvPrqWkcgIlJbdZPQRUQaXd0k9JNPrnUEIiK1VTcJ/YtfrHUEIiK1VTcJ3ZKuZxURaSB1k9BFRBqdErqISJ2om4SuLhcRaXR1k9BFRBpdl07oNbprgYhIp9SlE/r69bWOQESk8+jSCX3NmlpHICLSeXTphP7MM+lxHRQVkUbXpRP62rW1jkBEpPNQQhcRqRNdOqHH+9DV5SIija5LJ/R4C32nnWoXh4hIZ1AXCf3pp2GffWobi4hIrRWV0M1sjJktNLNFZnZuwvxzzOwlM3vezB4ys6HlDzXXmjWwxRZK5iIiUERCN7NuwDXAWGAkMNnMRmZVewZocve9gHuAy8odaJJVq2CbbaqxJRGRzq+YFvooYJG7L3b3FmA6MCFewd0fdvcPo8k5QMV7tJcvhxtvhPfeq/SWRES6hmIS+iBgaWx6WVSWz8nA/UkzzGyqmTWbWfOKFSuKjzLBW291aHERkbpTTEJPOiEw8bZYZnY80ARcnjTf3W9w9yZ3b+rfv3/xUSbYb78OLS4iUne6F1FnGTA4Nr0TsDy7kpkdDlwAHOruum2WiEiVFdNCnwuMMLPhZtYDmATMjFcws32A64Hx7v5O+cNM++//1kVEIiJJ2kzo7t4KTANmAwuA37j7fDO72MzGR9UuB3oCvzWzZ81sZp7Vddjee2dOX57YuSMi0niK6XLB3WcBs7LKLoqNH17muPI67LDM6d12q9aWRUQ6ty55peiLL6bHN2yoXRwiIp1Jl0zoe+wBJ5xQ6yhERDqXorpcOqMrroAddoBx42odiYhI59BlE3rfvvCTn9Q6ChGRzqNLdrmIiEguJXQRkTqhhC4iUieU0EVE6oQSuohInVBCFxGpE0roIiJ1QgldRKROmHvisyoqv2GzFcA/27l4P2BlGcMpF8VVGsVVus4am+IqTUfiGuruiU8IqllC7wgza3b3plrHkU1xlUZxla6zxqa4SlOpuNTlIiJSJ5TQRUTqRFdN6DfUOoA8FFdpFFfpOmtsiqs0FYmrS/ahi4hIrq7aQhcRkSxK6CIidaLLJXQzG2NmC81skZmdW+VtDzazh81sgZnNN7OzovL/MrM3zOzZ6HVkbJnzolgXmtkXKhjba2b2QrT95qisr5k9aGavRMPtonIzs59FcT1vZvtWKKZdY/vkWTP7wMy+WYv9ZWY3m9k7ZvZirKzk/WNmJ0T1XzGzDj8IMU9cl5vZP6JtzzCzPlH5MDP7KLbfrosts1/0+S+KYrcKxFXy51bu72ueuO6OxfSamT0blVdzf+XLDdX9G3P3LvMCugGvAjsDPYDngJFV3P5AYN9ovBfwMjAS+C/g2wn1R0YxbgEMj2LvVqHYXgP6ZZVdBpwbjZ8L/CQaPxK4HzBgf+DvVfrs3gKG1mJ/AYcA+wIvtnf/AH2BxdFwu2h8uwrEdQTQPRr/SSyuYfF6Wet5ChgdxXw/MLYCcZX0uVXi+5oUV9b8nwIX1WB/5csNVf0b62ot9FHAIndf7O4twHRgQrU27u5vuvvT0fhqYAEwqMAiE4Dp7r7e3ZcAiwjvoVomAL+Oxn8NHBUrv9WDOUAfMxtY4Vg+B7zq7oWuDq7Y/nL3x4D3ErZXyv75AvCgu7/n7quAB4Ex5Y7L3f/k7q3R5Bxgp0LriGLr7e5PesgKt8beS9niKiDf51b272uhuKJW9leAuwqto0L7K19uqOrfWFdL6IOApbHpZRROqBVjZsOAfYC/R0XTop9ON6d+VlHdeB34k5nNM7OpUdkO7v4mhD84YEAN4kqZROYXrdb7C0rfP7XYb18ntORShpvZM2b2qJkdHJUNimKpRlylfG7V3l8HA2+7+yuxsqrvr6zcUNW/sa6W0JP6uap+3qWZ9QTuBb7p7h8AvwA+DuwNvEn42QfVjfdAd98XGAucbmaHFKhb1f1oZj2A8cBvo6LOsL8KyRdHtffbBUArcEdU9CYwxN33Ac4B7jSz3lWMq9TPrdqf52QyGw1V318JuSFv1TwxdCi2rpbQlwGDY9M7AcurGYCZbU74wO5w9/8BcPe33X2ju28CbiTdTVC1eN19eTR8B5gRxfB2qislGr5T7bgiY4Gn3f3tKMaa769IqfunavFFB8O+BBwXdQsQdWm8G43PI/RP7xLFFe+WqUhc7fjcqrm/ugMTgbtj8VZ1fyXlBqr8N9bVEvpcYISZDY9afZOAmdXaeNRH90tggbtfESuP9z8fDaSOwM8EJpnZFmY2HBhBOBhT7ri2MbNeqXHCQbUXo+2njpKfANwXi+vfoyPt+wPvp34WVkhGy6nW+yum1P0zGzjCzLaLuhuOiMrKyszGAN8Fxrv7h7Hy/mbWLRrfmbB/FkexrTaz/aO/0X+PvZdyxlXq51bN7+vhwD/c/X+7Uqq5v/LlBqr9N9aRI7u1eBGODr9M+G97QZW3fRDh58/zwLPR60jgNuCFqHwmMDC2zAVRrAvp4JH0AnHtTDiD4Dlgfmq/ANsDDwGvRMO+UbkB10RxvQA0VXCfbQ28C2wbK6v6/iL8Q3kT2EBoBZ3cnv1D6NNeFL1OqlBciwj9qKm/seuiusdEn+9zwNPAuNh6mggJ9lXgaqKrwMscV8mfW7m/r0lxReW3AN/IqlvN/ZUvN1T1b0yX/ouI1Imu1uUiIiJ5KKGLiNQJJXQRkTqhhC4iUieU0EVE6oQSuohInVBCFxGpE/8fyeg+1/zh46cAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_acc, color=\"blue\")\n",
    "plt.title(\"Training Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_loss, color=\"red\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.plot(np_acc, color=\"blue\", label=\"Training Accuracy\")\n",
    "plt.plot(np_loss, color=\"red\", label=\"Training Loss\")\n",
    "plt.title(\"Training Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.7333333333333333\n"
     ]
    }
   ],
   "source": [
    "Ypred = predict(model, torch.from_numpy(X_test).float())\n",
    "acc = np.mean(Y_test == Ypred)\n",
    "print('Test accuracy: ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     precision    recall  f1-score   support\n",
      "\n",
      "    data_set-setosa       1.00      1.00      1.00         5\n",
      "data_set-versicolor       0.60      1.00      0.75         6\n",
      " data_set-virginica       0.00      0.00      0.00         4\n",
      "\n",
      "           accuracy                           0.73        15\n",
      "          macro avg       0.53      0.67      0.58        15\n",
      "       weighted avg       0.57      0.73      0.63        15\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aland\\.conda\\envs\\study\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "target_names = ['data_set-setosa', 'data_set-versicolor', 'data_set-virginica']\n",
    "print(classification_report(Y_test, Ypred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-study]",
   "language": "python",
   "name": "conda-env-.conda-study-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
